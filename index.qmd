---
title: "Discovering Data-Driven Nudges to Help Students Learn More Math"
author:
  - name: Marcos Gallo
    orcid: 0000-0002-8227-2661
  - name: Sean Hu
  - name: Ben Manning
  - name: Angela Duckworth
  - name: Colin Camerer
keywords: 
  - Pedagogical Decision-Making
  - Digital Education Platforms
  - Empirical Field Data
  - Instructional Adaptation
abstract: |
  The COVID-19 pandemic has exacerbated learning deficits, particularly in mathematics [@dipietro2023]. Adaptive online learning platforms offer a promising strategy to mitigate these adverse effects [@meeter2021], but designing effective interventions to improve student outcomes via such platforms remains a challenge. Here, we present a two-phase study that uses granular teacher and student engagement data from a large online Math platform in the United States. First, using unsupervised learning techniques (Independent Component Analysis, ICA; and a modified Predicting Context Sensitivity (PCS) approach, drawn from @buyalskaya2023, we identified critical teacher behaviors associated with improved student performance, namely empathy-focused engagement and strategic weekly login patterns. Building on these insights and in consultation with instructors, we developed two behaviorally-informed interventions: an empathy intervention encouraging teachers to view math problems from the student's perspective, and a habit-building intervention emphasizing the importance of Friday logins for reflective analysis and proactive planning. In a large-scale randomized controlled trial (N = 140,461 teachers across 22,281 schools), we demonstrated that the empathy intervention significantly increased student lesson completion by 4.09%. The habit condition also showed promise, increasing lesson completion by 1.81%, although this effect was not significantly different from the active control. Our approach demonstrates the potential of data-driven behavioral interventions to inform the development of more effective digital learning interventions, ultimately improving math education outcomes for students across diverse educational settings. Furthermore, we provide a generalizable framework for designing targeted interventions in various contexts where granular behavioral data are available.
plain-language-summary: |
  The COVID-19 pandemic has widened the gap in math achievements in mathematics among elementary students. Our research offers a novel, data-driven approach to designing effective interventions to address learning deficits. We identified key teacher behaviors associated with student success and developed targeted, low-cost interventions to encourage these behaviors. By demonstrating the effectiveness of these interventions in a large-scale randomized controlled trial, our study provides compelling evidence for the value of data-driven, behaviorally informed solutions to enhance teaching practices and improve student achievement. These findings can potentially guide educators, policymakers, and educational technology developers in creating more impactful digital learning solutions. They also pave the way for evidence-based interventions across diverse domains.
bibliography: megastudyrefs.bib
csl: pnas.csl
---

```{r import}

library(data.table)
library(dtplyr)
library(tidyverse)
# library(zoo)
library(ggrepel)
library(gt)
library(caret)
library(Statamarkdown)
set.seed(794563797)
# Timestamp: 2023-12-21 21:42:25 UTC
# Â© 1998-2023 RANDOM.ORG

```

```{r preprocessing}
#| eval: false

teacher_usage <- read.csv("Data/raw/Teacher Usage - Time Series 2020-10-09T1635.csv")
classroom_student_usage <- read.csv("Data/raw/Classroom Student Usage - Time Series 2020-10-09T1616.csv")
classroom_info <- read.csv("Data/raw/Classroom Info 2020-10-09T1617.csv")
classroom_teacher_lookup <- read.csv("Data/raw/Classroom-Teacher Lookup 2020-10-09T1618.csv")
school_info <- read.csv("Data/raw/School Info 2020-10-09T1619.csv")
la_usage_types <- read.csv("Data/raw/la_usage_types.csv")

teacher_usage <- teacher_usage %>%
  mutate(
    # Noting week, and year to summarize teacher behavior:
    week = lubridate::isoweek(Usage.Time),
    year = lubridate::year(Usage.Time),
    # Transforming Event.Type for "Resource Downloaded"
    Event.Type = ifelse(Event.Type == "Resource Downloaded",
                        paste0("RD.", Curriculum.Resource.Category),
                        Event.Type)) %>%
  mutate(year = ifelse(week == 1 & year == 2019, 2020, year))

classroom_student_usage <- classroom_student_usage %>% 
  mutate(Minutes.per.Active.User = as.numeric(Minutes.per.Active.User))


# Teacher activity
teacher_usage_total <- teacher_usage %>%
  # Summarize teacher behavior
  count(Adult.User.ID, Event.Type, week, year) %>%
  pivot_wider(names_from = Event.Type, values_from = n,
              values_fill = list(n = 0)) %>%
  full_join(teacher_usage %>%
              group_by(Adult.User.ID, week, year) %>%
              # Summarize teacher minutes
              summarize(Minutes.on.Zearn...Total = sum(Minutes.on.Zearn...Total)) %>%
              ungroup(),
            by = c("Adult.User.ID", "week", "year"))

# Classroom activity
# Filter out classrooms
classroom_info <- classroom_info %>%
  group_by(Classroom.ID) %>%
  # At least 5 students enrolled
  filter(Students...Total > 5) %>%
  # Remove classrooms with multiple schools
  filter(n_distinct(MDR.School.ID) == 1) %>%
  ungroup()
# Merge with classroom information
classroom_student_usage_total <- classroom_teacher_lookup %>%
  inner_join(classroom_info,
             by = "Classroom.ID",
             relationship = "many-to-many") %>%
  ungroup() %>%
  # Merge with classroom activity
  inner_join(classroom_student_usage %>%
               # # Remove empty weeks (not present in original code)
               # filter(rowSums(select(., -c(1:2)), na.rm = TRUE) > 0) %>%
               group_by(Classroom.ID) %>%
               # At least 5 active students per week on average
               filter(mean(Active.Users...Total, na.rm = T) > 5) %>%
               ungroup() %>%
               # Fill in NAs with 0
               mutate(across(-c(1:2), ~ replace_na(., 0))),
             by = "Classroom.ID", relationship = "many-to-many") %>%
  # Use only mergeable teachers
  inner_join(teacher_usage_total %>% count(Adult.User.ID, name = "teacher_weeks"),
             by = c("Teacher.User.ID" = "Adult.User.ID"),
             relationship = "many-to-many") %>%
  mutate(week = lubridate::isoweek(Usage.Week),
         year = lubridate::year(Usage.Week)) %>%
  mutate(year = ifelse(week == 1 & year == 2019, 2020, year))

# Merging and Filtering active teacher-classrooms
teacher_student_usage_subset <- classroom_teacher_lookup %>%
  # Merge with Teacher activity
  inner_join(teacher_usage_total,
             by = c("Teacher.User.ID" = "Adult.User.ID"),
             relationship = "many-to-many") %>%
  # Merge with classroom information
  inner_join(classroom_info %>% select(Classroom.ID, MDR.School.ID),
             by = "Classroom.ID", relationship = "many-to-many") %>%
  # Notice inner_join: we only match weeks for which both teachers and 
  # classrooms have data, making it fit for fixed effects
  inner_join(classroom_student_usage_total,
             by = c("Classroom.ID", "Teacher.User.ID", "MDR.School.ID",
                    "week", "year"),
             relationship = "many-to-many") %>%
  group_by(Classroom.ID) %>%
  # Remove teachers or classes inactive for at least 5 months
  filter(n() > 20) %>%
  # Remove June, July, and August
  filter(!week %in% c(23:35)) %>%
  # Calculate the number of classes per teacher
  ungroup() %>% group_by(Teacher.User.ID) %>%
  mutate(teacher_number_classes = n_distinct(Classroom.ID)) %>%
  ungroup() %>% group_by(Classroom.ID) %>%
  # Remove duplicate (classroom, week) pairs
  mutate(unique_id = paste(Classroom.ID, Usage.Week)) %>%
  # Remove those most likely to be "supervisors"
  arrange(teacher_number_classes, -teacher_weeks) %>%
  filter(!duplicated(unique_id)) %>% select(-unique_id) %>%
  ungroup() %>% group_by(Teacher.User.ID) %>%
  # Adjust the number of weeks for each teacher
  mutate(week = week - lubridate::isoweek(min(Usage.Week)) + 1) %>%
  mutate(week = ifelse(week < 1 | year == 2020,
                       week + 52 * (2020 - min(year)),
                       week)) %>%
  ungroup() %>% arrange(Classroom.ID, Teacher.User.ID, Usage.Week)


# Merge with school types
teacher_student_usage_subset <- teacher_student_usage_subset %>%
  left_join(la_usage_types %>% mutate(
    curriculum = as.integer(`Schools.and.Districts.Usage.Type` %in%
                              c("20-21 Curriculum", "Core Complement")),
    MDR.School.ID = as.integer(`Schools.and.Districts.MDR.School.ID`)) %>%
      select(MDR.School.ID, curriculum), by = c("MDR.School.ID")) %>%
  left_join(school_info %>% 
              select(MDR.School.ID, District.Rollup.ID,
                     Demographics...Zipcode.Median.Income,
                     Demographics...Poverty.Level,
                     Is.Charter..Yes...No.,
                     MDR.School.has.School.Account..Yes...No.,
                     School.Address...Zipcode),
            by = "MDR.School.ID") %>%
  rename(medianIncome = Demographics...Zipcode.Median.Income,
         povertyLevel = Demographics...Poverty.Level,
         charterSchool = Is.Charter..Yes...No.,
         schoolAccount = MDR.School.has.School.Account..Yes...No.,
         zipcode = School.Address...Zipcode) %>%
  mutate(charterSchool = case_when(charterSchool == "No" ~ 0,
                                   charterSchool == "Yes" ~ 1,
                                   .default = NA),
         schoolAccount = case_when(schoolAccount == "No" ~ 0,
                                   schoolAccount == "Yes" ~ 1,
                                   .default = NA)) %>%
  rename(Adult.User.ID = Teacher.User.ID)


# Write to csv
write.csv(teacher_student_usage_subset, "./Data/df.csv")

```

```{r preprocessing-habits}
#| eval: false

# Prepare data
setDT(teacher_usage)
teacher_usage[, Date := as.Date(Usage.Time)]

# Sequence of dates for the overall grid expand
date_seq <- seq.Date(min(teacher_usage$Date),
                     max(teacher_usage$Date),
                     by = "day")

# Logged indicator
teacher_usage[, logged := 1]

# Create combinations of Adult.User.ID and Date, and merge with logged data
df <- CJ(Adult.User.ID = unique(teacher_usage$Adult.User.ID), Date = date_seq)
df <- df[teacher_usage, on = .(Adult.User.ID, Date),
         `:=`(logged = i.logged,
              Minutes.on.Zearn...Total = i.Minutes.on.Zearn...Total)]
df[is.na(logged), logged := 0]
df[is.na(Minutes.on.Zearn...Total), Minutes.on.Zearn...Total := 0]

# Code datetime variables and compute additional metrics
setorder(df, Adult.User.ID, Date)
df[, `:=`(
  Year = year(Date),
  Month = month(Date),
  Week = isoweek(Date),
  dayofweek = wday(Date, label = TRUE, week_start = 1),
  prev_visits = cumsum(logged) - logged
  ), by = Adult.User.ID]
df[, `:=`(
  prev_dow_visits = cumsum(logged) - logged
  ), by = .(Adult.User.ID, dayofweek)]

# Streak, time lag, and other computations
df[, `:=`(
  lag.logged = shift(logged, 1, type = "lag", fill = 0)
  ), by = Adult.User.ID]
df[, start_streak:=as.integer(logged == 1 & lag.logged == 0)]
df[, start_streak:=cumsum(start_streak), by = Adult.User.ID]
df[lag.logged==0, streak:=as.double(0)]
df[is.na(streak), streak:=as.double(1:.N), by = .(Adult.User.ID, start_streak)]

# Time lag calculation
df[, `:=`(
  start_streak_lag = shift(start_streak, 1, fill = 0)
), by = Adult.User.ID]

# Streak calculation by day of week
df[,attended_lag_dow:=shift(logged, 1, fill = 0),
   by = .(Adult.User.ID, dayofweek)]
df[, `:=`(
  streak_dow = fcase(
    attended_lag_dow == 0, as.double(0),
    default = as.double(NA)
  ),
  start_streak_dow = as.numeric(logged == 1 & attended_lag_dow == 0)
  ), by = .(Adult.User.ID, dayofweek)]
df[, start_streak_dow := cumsum(start_streak_dow),
   by = .(Adult.User.ID, dayofweek)]
df[is.na(streak_dow),
   streak_dow := 1:.N,
   by = .(Adult.User.ID, start_streak_dow, dayofweek)]

# Previous week visits calculations
## Calculate weekly visits
weekly_logged <- df[, .(week_visits = sum(logged)),
                    by = .(Adult.User.ID, Week, Year)]
## Calculate previous week visits
setorder(weekly_logged, Adult.User.ID, Year, Week)
weekly_logged[, prev_week_visits := shift(week_visits, 1, fill = 0),
               by = Adult.User.ID]
df <- merge(df, weekly_logged,
            by = c("Adult.User.ID", "Week", "Year"), all.x = TRUE)
setorder(df, Adult.User.ID, Date)

# Calculate previous day of week visit
df[, prev_dow_visit := shift(logged, 1, fill = 0),
   by = .(Adult.User.ID, dayofweek)]
# Previous minutes spent
df[, prev_min_zearn := shift(Minutes.on.Zearn...Total, 1, fill = 0),
   by = Adult.User.ID]

# Add a time column, a sequence for each Adult.User.ID
df[, time := seq_len(.N), by = Adult.User.ID]
# Calculate the first time point for each Adult.User.ID where logged == 1
first_time_index <- df[logged == 1,
                       .(first_time = min(time)),
                       by = Adult.User.ID]
df[first_time_index,
   on = .(Adult.User.ID),
   `:=`(first_time = i.first_time)]
df[, first_time := time - first_time]
# Calculate the last time point for each Adult.User.ID where logged == 1
last_time_index <- df[logged == 1,
                      .(last_time = max(time)),
                      by = Adult.User.ID]
df[last_time_index,
   on = .(Adult.User.ID),
   `:=`(last_time = i.last_time)]
df[, last_time := last_time - time]
# Filter out rows outside the first and last logged==1
df <- df[first_time >= 0 & last_time >= 0]

df[, `:=`(
  first_week = isoweek(min(Date)),
  first_year = year(min(Date)),
  time = 1:.N
  ), by = Adult.User.ID]
df[, Week := Week - first_week + 1]
df[, Week := fifelse(Week < 1 | Year == 2020,
                     Week + 52 * (2020 - first_year),
                     Week), by = Adult.User.ID]

df[, `:=`(
  mean_visits = mean(logged),
  total_visits = sum(logged),
  prev_freq = prev_visits / time,
  prev_dow_freq = prev_dow_visits / Week
  ), by = Adult.User.ID]
  
df[lag.logged==1, time_lag:=1]
df[lag.logged==0, time_lag:=as.double(1:.N)+1,
   by = .(Adult.User.ID, start_streak_lag)]
df[time == 1, time_lag:=0, by = Adult.User.ID]

# Finalize the dataset
# Cleanup: Remove temporary columns
df[, c("first_time", "last_time", "first_week", "first_year") := NULL]
setorder(df, Adult.User.ID, Date)

# Merge with classroom data
df_habits <- df %>%
  lazy_dt() %>%
  filter(Adult.User.ID %in%
           unique(classroom_student_usage_total$Teacher.User.ID)) %>%
  mutate(dayofweek = factor(dayofweek, ordered = FALSE,
                            levels = c("Mon", "Tue", "Wed", "Thu",
                                       "Fri", "Sat", "Sun")),
         Month = factor(Month, ordered = FALSE),
         week_label = isoweek(Date)) %>%
  pivot_wider(names_from = dayofweek, values_from = logged, 
              values_fill = list(logged = 0)) %>%
  group_by(Adult.User.ID, Week) %>%
  summarize(across(c("Mon", "Tue", "Wed", "Thu", "Fri", "Sat", "Sun"),
                   sum, na.rm = TRUE),
            Total_Minutes_on_Zearn = sum(Minutes.on.Zearn...Total, na.rm = TRUE),
            mean_streak = mean(streak, na.rm = TRUE),
            mean_streak_dow = mean(streak_dow, na.rm = TRUE),
            mean_visits = mean(mean_visits, na.rm = TRUE),
            total_visits = mean(total_visits, na.rm = TRUE),
            mean_time_lag = mean(time_lag, na.rm = TRUE),
            Month = first(Month),
            Year = first(Year),
            week_label = first(week_label),
            .groups = 'drop')

# Pivot longer to aggregate by month
df_monthly <- df_habits %>%
  pivot_longer(cols = Mon:Sun, names_to = "dayofweek", values_to = "logged") %>%
  group_by(Adult.User.ID, Month) %>%
  summarize(total_logged_per_month = as.double(sum(logged, na.rm = TRUE))) %>%
  # Pivot wider to create a column for each month
  pivot_wider(names_from = Month, values_from = total_logged_per_month,
              names_prefix = "month_",
              values_fill = list(total_logged_per_month = 0)) %>%
  mutate(row_total = rowSums(select(., starts_with("month_")))) %>%
  mutate(across(starts_with("month_"), ~ ./row_total)) %>%
  # mutate(across(starts_with("month_"), ~ ./row_total * 100)) %>%
  select(-row_total)

df_habits <- df_habits %>%
  group_by(Adult.User.ID) %>%
  summarize(across(c(Mon:Sun), sum, na.rm = TRUE),
            across(c(Total_Minutes_on_Zearn:mean_time_lag), mean, na.rm = TRUE),
            total_weeks = n_distinct(Week),
            .groups = 'drop') %>%
  mutate(row_total = rowSums(select(., c(Mon:Sun)))) %>%
  mutate(across(c(Mon:Sun), ~ ./row_total)) %>%
  # mutate(across(c(Mon:Sun), ~ ./row_total * 100)) %>%
  select(-row_total) %>%
  inner_join(df_monthly, by = "Adult.User.ID") %>%
  inner_join(classroom_teacher_lookup %>%
               inner_join(classroom_info, by = "Classroom.ID",
                           relationship = "many-to-many") %>%
               # Merge with classroom activity
               inner_join(classroom_student_usage, by = "Classroom.ID",
                          relationship = "many-to-many") %>%
               # Use only mergeable teachers
               inner_join(teacher_usage_total %>%
                            count(Adult.User.ID, name = "teacher_weeks"),
                          by = c("Teacher.User.ID" = "Adult.User.ID"),
                          relationship = "many-to-many") %>%
               mutate(week = lubridate::isoweek(Usage.Week),
                      year = lubridate::year(Usage.Week)) %>%
               mutate(year = ifelse(week == 1 & year == 2019, 2020, year)) %>%
               group_by(Teacher.User.ID, week, year, MDR.School.ID) %>%
               summarize(Badges.per.Active.User = sum(Badges.per.Active.User,
                                                      na.rm = T),
                         Year = first(year),
                         .groups = 'drop') %>%
               group_by(Teacher.User.ID, MDR.School.ID) %>%
               summarize(Badges.per.Active.User = mean(Badges.per.Active.User,
                                                       na.rm = T),
                         .groups = 'drop'),
             by = c("Adult.User.ID" = "Teacher.User.ID")) %>%
  as_tibble()

badges <- classroom_teacher_lookup %>%
  inner_join(classroom_info, by = "Classroom.ID",
             relationship = "many-to-many") %>%
  # Merge with classroom activity
  inner_join(classroom_student_usage, by = "Classroom.ID",
             relationship = "many-to-many") %>%
  # Use only mergeable teachers
  inner_join(teacher_usage_total %>% count(Adult.User.ID, name = "teacher_weeks"),
             by = c("Teacher.User.ID" = "Adult.User.ID"),
             relationship = "many-to-many") %>%
  mutate(week = lubridate::isoweek(Usage.Week),
         year = lubridate::year(Usage.Week)) %>%
  mutate(year = ifelse(week == 1 & year == 2019, 2020, year)) %>%
  group_by(Teacher.User.ID, week, year, MDR.School.ID) %>%
  summarize(Badges.per.Active.User = sum(Badges.per.Active.User, na.rm = T),
            .groups = 'drop') %>%
  group_by(Teacher.User.ID) %>%
  summarize(Badges.per.Active.User = mean(Badges.per.Active.User, na.rm = T),
            .groups = 'drop')

# Write to csv
fwrite(df, "./Data/df_habits_long.csv")
fwrite(badges, "./Data/df_badges.csv")
fwrite(df_habits, "./Data/df_habits.csv")

```

# Introduction

The decline in math performance among American students has been a critical issue exacerbated by the COVID-19 pandemic, with some reporting an alarming half-year lag in math achievement among U.S. public school students in grades 3-8 [@fahle2023]. Learning losses and disparities, particularly among younger students, have been significant due to reduced instructional time and remote learning challenges [@zierer2021; @dipietro2023]. Moreover, @ewing2021 note that repeated school closures compounded these issues, emphasizing that the pandemic is not the only problem affecting math performance.

Emphasizing quality math instruction, particularly in pedagogical strategies, guidance, and teacher-student relations, is crucial for improving math performance, especially for students who have lost interest in mathematics or lack a sufficient foundation [@wang2023; @battey2016]. Furthermore, teachersâ deep mathematical knowledge is also important, enabling them to understand students' challenges better and provide adequate support [@hill2008; @battey2016]. This combination of empathy and subject expertise highlights the importance of specialized training focusing on pedagogical skills and content knowledge.

The pandemic also accelerated the adoption of digital platforms for math education. @meeter2021 found that adaptive practice software effectively mitigated the adverse effects of school closures. This result is consistent with the qualitative evidence from @alabdulaziz2021, who reported that teachers found digital platforms beneficial in addressing the challenges of remote learning during the pandemic. Recent meta-analyses demonstrate that integrating digital tools and blended learning approaches improves student outcomes significantly [@ran2021; @ran2020; @sadaf2021]. Leveraging these tools and insights gained during the pandemic should help address longstanding educational challenges [@ewing2021]. In particular, integrating technology, pedagogy, and content knowledge is essential, and professional development programs are most effective when they focus on using technology to foster a more engaging and effective learning environment [@young2016; @blanchard2016].

The increased use of digital platforms has provided more data to support the importance of student engagement in online learning. Blending online and in-person teaching methods can effectively enhance engagement and understanding, depending on the implementation [@chiang2016; @li2010; @li2022; @mawardi2023; @sadaf2021; @yu2023]. Teachers play a crucial role in helping students develop meta-cognitive skills to foster student engagement [@haleva2020]. Furthermore, teachers' beliefs and self-efficacy toward technology integration influence their willingness to adopt innovative teaching practices [@ertmer2012; @liljedahl2020]. This relationship between student engagement and teacher attitudes suggests the importance of creating solutions that can be implemented across diverse educational settings. Our current study aims to address this need by developing and evaluating cost-effective, high-quality interventions designed to improve student outcomes through enhanced teacher engagement with the Zearn platform.

In this study, we partner with Zearn to address these topics. This math education platform reaches approximately 25% of elementary schools and over a million middle-school students in the United States. Zearn's approach integrates interactive digital lessons with hands-on teaching, aligning with the Common Core State Standards and providing a comprehensive educational experience [@zearnma2023].

Our study leverages this rich resource to offer an innovative approach to educational interventions using behavioral science principles. Focusing on quality of teaching, we align with @hanushek2020, who maintains that the efficacy of resource utilization supersedes quantity. We also follow current trends in providing cost-effective, easy-to-implement interventions (i.e., "nudges") that offer engagement incentives for both teachers and students [@lynch2019; @lavecchia2016; @koch2015].

Our two-step approach initially employs unsupervised learning techniques to analyze behavioral patterns in teacher activity on Zearn, aligning with the data mining value in educational research [@salazar2007; @hershcovits2020; @qiu2022; @al-shabandar2018; @shin2020]. Subsequently, we aim to establish the causality of our interventions through a large-scale field experiment guided by recommendations from @greene2022 for holistic, transparent, and reproducible research. Our unique integration of behavioral science with digital education seeks to provide impactful insights into math education and offer a blueprint for similar studies in other fields.

# Results

## Study 1: Data-Driven Nudge Design

Our study used a comprehensive dataset from Zearn's educational platform, encompassing the 2019-2020 academic year and spanning multiple schools in Louisiana. Zearn's content is designed to promote intuitive understanding by progressing from concrete to pictorial to abstract examples. The platform offers a personalized experience, providing targeted remediation when a student encounters difficulties. Teachers can monitor individual student progress by tracking activities such as "Badges" and "Tower of Power." Badges track student lesson completion, and the Tower of Power is an assessment that presents students with challenging problems at the end of each lesson. When students struggle or fail to answer correctly during a Tower of Power, the platform provides "Boosts" (i.e., hints or further explanations) and notifies teachers with a "Tower Alert." These features aim to motivate students and provide valuable information for educators to support learning.

Other important variables from the dataset included teacher logins, file downloads, and specific interactions with educational content. Additionally, we accessed student data at the classroom-week level, encompassing metrics such as lesson completion (i.e., Badges) and instances of learning difficulties (i.e., Tower Alerts). This granularity allowed for an in-depth analysis of both teacher behaviors and student performance.

The data revealed diverse engagement patterns. Teachers logged in multiple times per week, exhibiting variations in the frequency and duration of their interactions. The student data, aggregated at the classroom-week level, showed a wide range of performance levels across various classrooms and schools. The standard deviations of key variables underscored this diversity, as detailed in the summary statistics of @tbl-summary. This rich combination of teacher behaviors and student performance metrics, carefully matched with a weekly frequency for each classroom, allowed for an in-depth analysis while upholding privacy standards. Note that the analyses in Study 1 underwent post hoc modifications to rectify some subsequently identified errors. These changes, while adjusting the coefficients slightly, did not significantly alter the overall results or patterns observed (refer to the SI for the original analyses).

```{r}
#| label: tbl-summary
#| tbl-cap: "Study 1 Data. The table summarizes key educational metrics for Zearn teachers. The data was collected from July 2019 to June 2020."

teacher_student_usage_subset <- read_csv(file = "./Data/df.csv")
teacher_usage <- read.csv("Data/raw/Teacher Usage - Time Series 2020-10-09T1635.csv")

# Pre-calculate unique teachers and classrooms
unique_teachers <- unique(teacher_student_usage_subset$Adult.User.ID)
unique_classrooms <- unique(
  teacher_student_usage_subset[, c('Adult.User.ID', 'Classroom.ID')]
  )

# Statistics for "# of classes"
n_teachers <- length(unique_teachers)
n_classes <- nrow(unique_classrooms)
mean_classes <- n_classes / n_teachers
classes_per_teacher <- unique_classrooms %>% count(Adult.User.ID)

# Statistics for "# of badges"
teacher_student_usage_subset$Total.Badges <- 
  teacher_student_usage_subset$Active.Users...Total *
  teacher_student_usage_subset$Badges.per.Active.User
badges_per_teacher <- teacher_student_usage_subset %>% 
  group_by(Adult.User.ID) %>% 
  summarize(Total.Badges = sum(Total.Badges))

# Statistics for "Total minutes on Zearn"
minutes_per_teacher <- teacher_usage %>%
  filter(!Adult.User.ID %in% unique_teachers) %>%
  group_by(Adult.User.ID) %>%
  summarize(Total.Minutes = sum(Minutes.on.Zearn...Total))

# Creating a function for common statistics
calculate_stats <- function(data, value) {
  list(
    N = sum(data[[value]]),
    Mean = mean(data[[value]]),
    SD = sd(data[[value]]),
    Quantiles = quantile(data[[value]], probs = c(0.25, 0.5, 0.75))
  )
}

# Calculating statistics
stats_classes <- calculate_stats(classes_per_teacher, "n")
stats_badges <- calculate_stats(badges_per_teacher, "Total.Badges")
stats_minutes <- calculate_stats(minutes_per_teacher, "Total.Minutes")

# Formatting the table with gt
gt_table <- tibble(
  Category = c("No. of teachers", "No. of classes", "No. of badges", "Total minutes \n on Zearn"),
  N = c(n_teachers, stats_classes$N, stats_badges$N, stats_minutes$N),
  Mean = c(NA, stats_classes$Mean, stats_badges$Mean, stats_minutes$Mean),
  `Standard Deviation` = c(NA, stats_classes$SD, stats_badges$SD, stats_minutes$SD),
  `1st Quartile` = c(NA, stats_classes$Quantiles[1], stats_badges$Quantiles[1], stats_minutes$Quantiles[1]),
  `2nd Quartile` = c(NA, stats_classes$Quantiles[2], stats_badges$Quantiles[2], stats_minutes$Quantiles[2]),
  `3rd Quartile` = c(NA, stats_classes$Quantiles[3], stats_badges$Quantiles[3], stats_minutes$Quantiles[3])
) %>%
  gt() %>%
  fmt_markdown() %>%
  tab_spanner(
    label = "Aggregated Statistics per Teacher",
    columns = c("Mean", "Standard Deviation", "1st Quartile", "2nd Quartile", "3rd Quartile")
  ) %>%
  cols_label(
    Category = "",
    N = "N",
    Mean = "Mean",
    `Standard Deviation` = "Standard \n Deviation",
    `1st Quartile` = "1st \n Quartile",
    `2nd Quartile` = "Median",
    `3rd Quartile` = "3rd \n Quartile"
  ) %>% 
  fmt_number(
    columns = c("N"),
    decimals = 0
  ) %>%
  fmt_number(
    columns = c("Mean", "Standard Deviation"),
    decimals = 2,
    use_seps = TRUE
  ) %>%
  fmt_number(
    columns = c("1st Quartile", "2nd Quartile", "3rd Quartile"),
    decimals = 1
  ) %>%
  sub_missing(
    columns = c("Mean", "Standard Deviation", "1st Quartile", "2nd Quartile", "3rd Quartile"),
    missing_text = "-"
  ) %>%
  as_raw_html()

# Display the table
gt_table

```

```{r clean-environment}
#| include: false

rm(list = setdiff(ls(), c("teacher_student_usage_subset", "teacher_usage")))
gc(verbose = FALSE)

```

### Study 1a: Identifying Key Teacher Behaviors Associated with Student Performance Using Independent Component Analysis

Zearn offers a variety of activities for students and teachers, resulting in a large number of usage variables. We chose to summarize this large set of online activities into more easily interpretable variables through a dimensionality reduction technique. We employed Independent Component Analysis (ICA) on these teacher behavioral variables, given the non-Gaussian nature of the data [@hyvarinen2000]. This statistical approach allowed us to uncover latent variables that might have been obscured with traditional methods. The resulting components were weighted vectors of specific teacher activities and were estimated to maximize statistical independence in the data-generating process [@hyvarinen2000].

By examining the explained variance of each added component, we concluded that three independent components best portray the underlying dimensions of teacher behavior. (For more details, please refer to the SI section and @fig-scree). It is important to note that the results presented here reflect a revised version of our pre-experimental analysis. Although we maintained the same analytical framework, we addressed several inaccuracies identified in a post hoc review (for detailed information, please refer to the Supplementary Information). These corrections yielded more precise and robust results.

```{r ICA}

library(ica)

# Perform ICA to choose number of components
# Removing columns with zero variance

ICA_cols <- teacher_student_usage_subset %>%
  select(`RD.elementary_schedule`:RD.grade_level_teacher_materials) %>%
  select(where(~ sd(.x, na.rm = TRUE) != 0)) %>%
  mutate_all(scale)

imod_fast <- ica(ICA_cols, nc = 10)

# Create Elbow Plot
## SEAN: Fix plot to show integers in x-axis and % in y-axis
elbow_plot <- ggplot(data.frame(Component = 1:10, Variance = imod_fast$vafs),
                     aes(x = Component, y = Variance)) +
  geom_line(color = "gray") +
  geom_point() +
  labs(x = "Component", y = "Proportion of Variance Explained",
       title = "Elbow Plot for Determining Number of Components") +
  theme_minimal() + 
  scale_x_continuous(breaks=c(2,4,6,8,10)) +
  scale_y_continuous(labels = scales::percent)


# ICA via different algorithms

## SEAN: Check for the 3 methods:
## 1) http://research.ics.aalto.fi/ica/icasso/
## 2) https://www.cs.helsinki.fi/u/ahyvarin/code/isctest/
## We will choose the best method accordingly (most stable and interpretable)

imod_fast <- ica(ICA_cols, nc = 3)
# imod.imax <- ica(ICA_cols, nc = 3, method = "imax")
# imod.jade <- ica(ICA_cols, nc = 3, method = "jade")


# Iterate through each principal component
for (i in 1:ncol(imod_fast$M)) {
    # Find the index of the variable with the largest absolute coefficient
    largest_coef_index <- which.max(abs(imod_fast$M[, i]))

    # Check if the largest coefficient is negative
    if (imod_fast$M[largest_coef_index, i] < 0) {
        # Multiply all coefficients in this principal component by -1
        imod_fast$M[, i] <- -1 * imod_fast$M[, i]
        imod_fast$S[, i] <- -1 * imod_fast$S[, i]
    }
}

# Add ICA components to the dataset
teacher_student_usage_subset <- teacher_student_usage_subset %>%
  mutate(
    ic1 = imod_fast$S[, 1],
    ic2 = imod_fast$S[, 2],
    ic3 = imod_fast$S[, 3]
  )

```

The significant finding from the ICA was the prominence of the first Independent Component (IC1), accounting for `r round(imod_fast$vafs[1]*100, 2)`% of the variance in teacher Zearn activity, as indicated in @fig-ica-heatmap. This result is significant in nudge engineering, highlighting the need to focus interventions on elements encapsulated by IC1. Conversely, IC2 and IC3, with `r round(imod_fast$vafs[2]*100, 2)`% and `r round(imod_fast$vafs[3]*100, 2)`% variance explained, play lesser but still noteworthy roles.

```{r}
#| label: fig-ica-heatmap
#| fig-cap: "Independent Component Analysis (ICA) Results. The heatmap displays teacher actions in each row, while the columns represent the three independent components (IC1, IC2, IC3) that explained the most variance in the teacher behavioral data, with their respective percentage of variance explained in parentheses. The color gradient on the heatmap indicates the relative importance of each activity within these components. Note that these metrics pertain to teacher activity on the platform, not student actions. RD = Resource Download."

library(pheatmap)

# Add a row for the percentage of variance explained
ica_weights <- imod_fast$M

# Define row names
variable_names <- c(
  "User Session" = "User Session",
  "RD.optional_problem_sets" = "Optional Problem Sets RD",
  "RD.student_notes_and_exit_tickets" = "Student Notes and Exit Tickets RD",
  "RD.mission_overview" = "Mission Overview RD",
  "RD.pd_course_notes" = "PD Course Notes RD",
  "RD.elementary_schedule" = "Elementary Schedule RD",
  "RD.whole_group_fluency" = "Whole Group Fluency RD",
  "Guided Practice Completed" = "Guided Practice Completed",
  "RD.small_group_lessons" = "Small Group Lesson RD",
  "Tower Completed" = "Tower Completed",
  "Fluency Completed" = "Fluency Completed",
  "Number Gym Activity Completed" = "Number Gym Activity Completed",
  "RD.grade_level_overview" = "Grade Level Overview RD",
  "Tower Stage Failed" = "Tower Stage Failed",
  "Kindergarten Activity Completed" = "Kindergarten Activity Completed",
  "Tower Struggled" = "Tower Struggled",
  "RD.k_mission" = "Kindergarten Mission RD",
  "RD.whole_group_word_problems" = "Whole Group Word Problems RD",
  "RD.assessments" = "Assessments RD",
  "RD.teaching_and_learning_approach" = "Teaching and Learning Approach RD",
  "RD.optional_homework" = "Optional Homework RD",
  "RD.k_schedule" = "Kindergarten Schedule RD",
  "RD.curriculum_map" = "Curriculum Map RD",
  "RD.assessments_answer_key" = "Assessments Answer Key RD",
  "RD.pd_course_guide" = "PD Course Guide RD",
  "RD.grade_level_teacher_materials" = "Grade Level Teacher Materials RD"
)
rownames(ica_weights) <- variable_names[names(ICA_cols)]

# Order rows by descending IC1
ordered <- as.data.frame(ica_weights) %>%
  mutate(across(everything(), ~ if_else(. <= quantile(., 0.75), 0, .))) %>%
  arrange(desc(V1), desc(V2), desc(V3))
ica_weights <- ica_weights[rownames(ordered), ]

# Create the heatmap
pheatmap(ica_weights,
         cluster_rows = FALSE,
         cluster_cols = FALSE,
         color = colorRampPalette(c("white", "red"))(10),
         breaks = seq(-0.1, 1, length.out = 10),
         show_colnames = TRUE,
         show_rownames = TRUE,
         fontsize_row = 10,
         fontsize_col = 12,
         labels_col = c(
           sprintf("IC1 \n (%.1f%%)", imod_fast$vafs[1] * 100),
           sprintf("IC2 \n (%.1f%%)", imod_fast$vafs[2] * 100),
           sprintf("IC3 \n (%.1f%%)", imod_fast$vafs[3] * 100)
           ),
         angle_col = 0,
         cell_fun = function(j, i, x, y, width, height) {
             if (ica_weights[i, j] > 0.6) {
                 text(x, y, round(ica_weights[i, j], 2), col = "black", cex = 0.8)
             }
         })

```

The activities with the highest weighting in IC1 included metrics associated with the "Tower of Power" (Struggled, Failed, and Completed) and other problem-solving exercises (i.e., Fluency exercises, Guided Practice, and Number Gym Activity). Notably, the top two variables in this component involved an interactive feature of the Tower of Power when students struggle to understand a concept (i.e., Tower Struggled or Failed). The platform offers tailored support through "Boosts," which break down questions into smaller steps, helping students understand and correct their mistakes. Given that the metrics in the ICs pertain to teacher activity, IC1 suggests that teachers proactively engage with these problem-solving activities, sometimes deliberately making mistakes, to better understand the student experience. This interaction may help teachers devise strategies to break down complex problems into simpler steps, known as instructional scaffolding [@cai2022; @beed1991].

After consulting with Zearn educators and administrators (see SI for details), it was that teachers with high levels of IC1 apply an empathy-driven pedagogy. In this context, "empathy" refers to teachers' ability to comprehend and engage with students' challenges, as demonstrated by their focused attention on areas where students struggled or succeeded. The weightings in IC1 for activities like Tower Struggles (`r round(ica_weights["Tower Struggled",1], 3)`), Fluency Completed (`r round(ica_weights["Fluency Completed",1], 3)`), Guided Practice Completed (`r round(ica_weights["Guided Practice Completed",1], 3)`), and Number Gym Activity Completed (`r round(ica_weights["Number Gym Activity Completed",1], 3)`) were significantly higher compared to other activities. This pattern underlines this empathy-driven process in teaching, where teachers' engagement with the platform is focused on understanding student challenges.

The second component, IC2, showed substantial weightings on a variety of Resource Downloads (RD), particularly Small Group Lessons (`r round(ica_weights["Small Group Lesson RD",2], 3)`) and Whole Group (i.e., entire class) Word Problems (`r round(ica_weights["Whole Group Word Problems RD",2], 3)`). We labeled this component as "Classroom Activities." IC3, with strong weights on Resource Download (RD) for the Professional Development (PD) course guide (`r round(ica_weights["PD Course Guide RD",3], 3)`) and course notes (`r round(ica_weights["PD Course Notes RD",3], 3)`), was labeled "Professional Development." This component, accounting for a smaller variance (`r round(imod_fast$vafs[3]*100, 3)`%), was harder to interpret due to more diverse activities.

```{r plm-regression}

library(plm)

# Define the columns to be used in the formula
columns_used <- colnames(teacher_student_usage_subset %>%
                           select(ic1:ic3,
                                  teacher_number_classes,
                                  Students...Total,
                                  Grade.Level
                                  # MDR.School.ID,
                                  ))

# Create the formula
fmla_str <- sprintf("logBadges ~ %s",
                    paste(sprintf("`%s`", columns_used), collapse = " + "))
fmla <- as.formula(fmla_str)

# Define the panel model index
pUsage <- pdata.frame(
  teacher_student_usage_subset %>%
    arrange(Classroom.ID, Adult.User.ID, year, week) %>%
    mutate(Classroom.ID = factor(Classroom.ID, ordered = FALSE),
           Adult.User.ID = factor(Adult.User.ID, ordered = FALSE),
           Usage.Week = as_date(Usage.Week),
           # MDR.School.ID = factor(MDR.School.ID, ordered = FALSE),
           # District.Rollup.ID = factor(District.Rollup.ID, ordered = FALSE),
           Grade.Level = factor(Grade.Level, ordered = TRUE),
           medianIncome = factor(medianIncome, ordered = TRUE, exclude = ""),
           povertyLevel = factor(povertyLevel, ordered = TRUE, exclude = ""),
           zipcode = factor(zipcode, ordered = FALSE),
           logBadges = log1p(Badges.per.Active.User)),
  index = c("Classroom.ID", "Usage.Week", "Adult.User.ID"))

# 'Within' model
ica_fe_model <- plm(fmla, data = pUsage, model = "within")

# Hausman Test supports Fixed Effects
# ica_re_model <- plm(fmla, data = pUsage, model = "random")
# phtest(ica_fe_model, ica_re_model)
# chisq = 81.31, df = 4, p-value < 2.2e-16
# alternative hypothesis: one model is inconsistent


# Subsample model
pUsage <- pUsage %>% filter(schoolAccount == 1, curriculum == 1)

subsample_model <- colnames(teacher_student_usage_subset %>%
                           select(ic1:ic3,
                                  teacher_number_classes,
                                  Students...Total,
                                  Grade.Level,
                                  medianIncome
                                  # MDR.School.ID,
                                  ))
fmla_str <- sprintf("logBadges ~ %s",
                    paste(sprintf("`%s`", subsample_model), collapse = " + "))
fmla <- as.formula(fmla_str)

ica_fe_subsample <- plm(fmla, data = pUsage, model = "within")

```

```{r}
#| label: tbl-plm
#| tbl-cap: "Regression of Log Badges on the Independent Components. The table displays the results of a panel regression model with clustered standard errors at the teacher level in parentheses."

## Graphing Helper Functions
# Function to format coefficients and standard errors
format_coef_se <- function(coef, stars, se) {
  paste0(sprintf("%.3f", coef),
         stars,
         "<br>",
         "(",
         sprintf("%.4f", se),
         ")")
}

# Function to add significance asterisks
add_significance <- function(p_value) {
  if (p_value < 0.001) {
    return("***")
  } else if (p_value < 0.01) {
    return("**")
  } else if (p_value < 0.05) {
    return("*")
  } else {
    return("")
  }
}

# Improve formatting for the Average Intercept, R-squared, and N
format_value <- function(value, digits = 3) {
  round(value, digits = digits)
}


# Extracting model summaries
ica_fe_summary <- broom::tidy(ica_fe_model)
ica_fe_summary[, c(3:5)] <-
  summary(ica_fe_model,
          vcov = vcovHC(ica_fe_model,
                        type = "HC3"))$coef[, c(2:4)]
ica_fe_subsample_summary <- broom::tidy(ica_fe_subsample)
ica_fe_subsample_summary[, c(3:5)] <-
  summary(ica_fe_subsample,
          vcov = vcovHC(ica_fe_subsample,
                        type = "HC3"))$coef[, c(2:4)]

# Merging summaries into one data frame
merged_summaries <- merge(
  ica_fe_summary,
  ica_fe_subsample_summary,
  by = "term",
  suffixes = c("_fe", "_fe_subsample")
)

# Apply formatting to coefficients, standard errors, and adding significance
merged_summaries <- merged_summaries %>%
  mutate(
    significance_fe = sapply(p.value_fe, add_significance),
    significance_fe_subsample = sapply(p.value_fe_subsample, add_significance),
    coef_se_fe = format_coef_se(estimate_fe,
                                significance_fe,
                                std.error_fe),
    coef_se_fe_subsample = format_coef_se(
      estimate_fe_subsample,
      significance_fe_subsample,
      std.error_fe_subsample
    )
  ) %>%
  select(term, coef_se_fe, coef_se_fe_subsample) %>%
  mutate(term = case_when(
    term == "ic1" ~ "IC 1",
    term == "ic2" ~ "IC 2",
    term == "ic3" ~ "IC 3",
    term == "teacher_number_classes" ~ "No. of Classes",
    .default = term  # Keep all other terms unchanged
  ))

gt_table <- gt(merged_summaries) %>%
  cols_label(
    term = "",
    coef_se_fe = "All Schools",
    coef_se_fe_subsample = "Zearn Curriculum"
  ) %>%
  tab_spanner(
    label = "ln(Badges + 1)",
    columns = c(coef_se_fe, coef_se_fe_subsample)
  ) %>%
  tab_footnote(
    footnote = "Note: * p<0.05; ** p<0.01; *** p<0.001"
  ) %>%
  rows_add(
    term = c(
      "Average Intercept",
      "R-squared",
      "N"
    ),
    coef_se_fe = c(
      format_value(mean(fixef(ica_fe_model))),
      format_value(summary(ica_fe_model)$r.squared[1]),
      paste(
        "Teachers: ", format_value(n_distinct(index(ica_fe_model$model)$Adult.User.ID)),
        "<br>Classes: ", format_value(pdim(ica_fe_model)$nT$n),
        "<br>Weeks: ", format_value(pdim(ica_fe_model)$nT$T),
        "<br>Total: ", format_value(pdim(ica_fe_model)$nT$N),
        sep = ""
      )
    ),
    coef_se_fe_subsample = c(
      format_value(mean(fixef(ica_fe_subsample))),
      format_value(summary(ica_fe_subsample)$r.squared[1]),
      paste(
        "Teachers: ", format_value(n_distinct(index(ica_fe_subsample$model)$Adult.User.ID)),
        "<br>Classes: ", format_value(pdim(ica_fe_subsample)$nT$n),
        "<br>Weeks: ", format_value(pdim(ica_fe_subsample)$nT$T),
        "<br>Total: ", format_value(pdim(ica_fe_subsample)$nT$N),
        sep = ""
      )
    )
  ) %>%
  tab_style(
    style = cell_text(weight = "bold", align = "center"),
    locations = cells_column_labels(columns = everything())
  ) %>%
  tab_style(
    style = cell_text(align = "center"),
    locations = cells_body(columns = c(coef_se_fe, coef_se_fe_subsample))
  ) %>%
  tab_style(
    style = cell_text(align = "left"),
    locations = cells_stub()
  ) %>%
  # Use tab_style to add double lines for model fit statistics
  tab_style(
    style = list(
      cell_borders(
        sides = "top",
        style = "double",
        color = "lightgray",
        weight = px(4)
      )),
    locations = cells_body(
      columns = everything(),
      rows = term %in% c("R-squared")
    )
  ) %>%
  fmt_markdown() %>%
  as_raw_html()


# Print the table
gt_table

```

Building upon these findings, we advanced to a fixed-effects panel regression model that accounted for various temporal and subject-specific variables and the potential impact of teachers handling multiple classes. The regression formula incorporated changes in independent components (IC1, IC2, IC3) as predictors for the change in the logarithm of badges (+1), accounting for individual teachers, classes, and weeks. As presented in @tbl-plm, the regression highlighted a strong positive contemporaneous correlation between IC1 and badges, with a coefficient of `r round(as.numeric(ica_fe_summary[ica_fe_summary$term == "ic1","estimate"]), 4)` (p = `r round(as.numeric(ica_fe_summary[ica_fe_summary$term == "ic1","p.value"]), 4)`), suggesting a significant impact on student achievement. Upon the recommendation of Zearn administrators, a supplementary analysis was conducted, focused exclusively on schools using Zearn either as their core curriculum or a key supplementary resource, effectively excluding teachers who independently chose Zearn despite the absence of school-wide implementation. This subset analysis revealed an effect of `r round(as.numeric(ica_fe_subsample_summary[ica_fe_subsample_summary$term == "ic1","estimate"]), 4)` (p = `r round(as.numeric(ica_fe_subsample_summary[ica_fe_subsample_summary$term == "ic1","p.value"]), 4)`), indicating that IC1 is especially significant for those schools.

This correlation also has practical significance. It indicates that an increase in activities associated with IC1, such as increasing the encounters of 'Tower Struggle' by one standard deviation, is associated with an approximate `r round(summary(ica_fe_model)$coefficients[c("ic1","ic2","ic3"),"Estimate"] %*% ica_weights["Tower Struggled",1:3] / sd(unlist(teacher_student_usage_subset[,"Tower Struggled"])) *100, 2)` percent increase in student badges. Although this increase may seem small, it may be substantial in the context of nudges, where even modest changes often lead to far-reaching effects.

### Study 1b: Identifying Key Teacher Habits Associated with Student Performance

Prompted by @buyalskaya2023, we sought to uncover the subtleties of habitual behaviors within an educational setting. Our primary goal was to understand how regular teacher interactions with the Zearn platform impacted student learning outcomes. To measure this, we used the log-transformed average weekly badges per student over the entire school year as our dependent variable. We constructed our explanatory variables with careful consideration of the patterns that could identify habitual engagement and their relationship to student performance:

1.  Login Percentages across months and days of the week: The frequency of logins across different time periods. For example, among all the logins for a teacher, we assess how many are from January or Tuesdays, compared to all other months and days of the week, respectively.

2.  Average Minutes: Teachers' weekly average time spent on the platform, measured in minutes.

3.  Average Streak: Average number of consecutive days in which the teacher logs in, excluding weekends.

4.  Average Weekday Streak: Average number of consecutive weekdays in which the teacher logs in (e.g., three Tuesday in a row).

5.  Average Days Between Logins: Average number of days between two login instances.

6.  Total Logins: Total number of teacher logins over the entire school year.

To account for any school-specific factors that may have influenced the relationship between teacher behavior and student achievement, we estimated a linear regression model. Unlike Study 1, this regression did not follow each teacher or class across weeks, as our unit of analysis was a teacher summed across classrooms and averaged across all weeks. Note that our regression omitted the login percentages from July and Sunday, periods with low login incidence, to avoid multicollinearity.

The regression results, as detailed in @tbl-habit-regression, revealed that all weekday login percentages positively affected student badges. However, Friday logins stood out significantly, suggesting that specific days of the week have more influence on habitual engagement.

```{r habit-regression}

# Read in the data
df <- fread(file = "./Data/df_habits.csv")

df <- df %>%
  mutate(Adult.User.ID = factor(Adult.User.ID, ordered = FALSE),
         MDR.School.ID = factor(MDR.School.ID, ordered = FALSE),
         logBadges = log1p(Badges.per.Active.User))

habits_model <- lm(Badges.per.Active.User ~
                     Mon + Tue + Wed + Thu + Fri + Sat +
                     month_1 + month_2 + month_3 +
                     month_4 + month_5 + month_6 +
                     month_8 + month_9 +
                     month_10 + month_11 + month_12 +
                     Total_Minutes_on_Zearn + mean_streak +
                     mean_time_lag + mean_streak_dow + total_visits,
                     data = df)
```

```{r}
#| label: tbl-habit-regression
#| tbl-cap: "Regression of Log Average Badges on Habit Variables. The table displays the results of the regression model with standard errors clustered at the school level in parentheses. Coefficients on months and days measure the difference between the effects of login percentage relative to the July percentage (for months) and the Sunday percentage (for days)."

habits_results <- broom::tidy(
  lmtest::coeftest(
    habits_model,
    vcov = sandwich::vcovCL(habits_model,
                            cluster = ~ MDR.School.ID,
                            type = "HC3")
    )
  )
# Calculate values for additional rows
r_squared <- summary(habits_model)$r.squared
n_total <- nrow(df)

format_coef <- function(coef, se) {
  paste0(sprintf("%.3f", coef), " (", sprintf("%.3f", se), ")")
}

# Convert to a gt table
gt_table <- habits_results %>%
  as.data.frame() %>%
  select(-c("statistic")) %>%
  mutate(
    Coefficient = mapply(format_coef, estimate, `std.error`),
    p_value = mapply(format.pval, `p.value`, digits = 3, eps = .001),
    term = case_when(
      term == "Mon" ~ "`%`Monday",
      term == "Tue" ~ "`%`Tuesday",
      term == "Wed" ~ "`%`Wednesday",
      term == "Thu" ~ "`%`Thursday",
      term == "Fri" ~ "`%`Friday",
      term == "Sat" ~ "`%`Saturday",
      term == "month_1" ~ "`%`January",
      term == "month_2" ~ "`%`February",
      term == "month_3" ~ "`%`March",
      term == "month_4" ~ "`%`April",
      term == "month_5" ~ "`%`May",
      term == "month_6" ~ "`%`June",
      term == "month_8" ~ "`%`August",
      term == "month_9" ~ "`%`September",
      term == "month_10" ~ "`%`October",
      term == "month_11" ~ "`%`November",
      term == "month_12" ~ "`%`December",
      term == "Total_Minutes_on_Zearn" ~ "Avg. Minutes",
      term == "mean_streak" ~ "Avg. Streak",
      term == "mean_time_lag" ~ "Avg. Days Between Logins",
      term == "mean_streak_dow" ~ "Avg. Weekday Streak",
      term == "total_visits" ~ "Total Logins",
      TRUE ~ term  # Default case
      )) %>%
  mutate(p_value = sub("0.", ".", p_value)) %>%
  select(term, Coefficient, p_value) %>%
  gt() %>%
  tab_spanner(
    label = "Avg. Weekly Badges",
    columns = c(Coefficient, p_value)
  ) %>%
  cols_label(
    term = "",
    Coefficient = "Estimate (SE)",
    p_value = "P-value"
  ) %>%
  cols_align(
    align = "center",
    columns = c(Coefficient, p_value)
  ) %>%
  # Add rows for R-squared and N
  rows_add(
    term = c("R-squared", "N"),
    Coefficient = c(sprintf("%.3f", r_squared), as.character(n_total)),
    p_value = c("", "")
  ) %>%
  # Use tab_style to add double lines for model fit statistics
  tab_style(
    style = list(
      cell_borders(
        sides = "top",
        style = "double",
        color = "lightgray",
        weight = px(4)
      )),
    locations = cells_body(
      columns = everything(),
      rows = term %in% c("R-squared")
    )
  ) %>%
  fmt_markdown() %>%
  as_raw_html()

# Print the table
gt_table

```

```{r friday-example}
#| eval: false

mean(unlist(df[,13])) *                         # Total Teacher Logins 
  (colMeans(as.matrix(df[,c(2:6)])) -           # Times weekday percentages
     0.02481 * c(-1/4, -1/4, -1/4, -1/4, 1)) /  # Adjustment so Fri = 1/4
  mean(unlist(df[,15]))                         # Normalized by Total Weeks
#       Mon       Tue       Wed       Thu       Fri 
# 0.3285072 0.3481013 0.3478826 0.3405730 0.2753985 

mean(unlist(df[,13])) *
  (colMeans(as.matrix(df[,c(2:6)])) +
     0.1176 * c(-1/4, -1/4, -1/4, -1/4, 1)) /     # Adjustment so Fri = 1/2
  mean(unlist(df[,15]))
#       Mon       Tue       Wed       Thu       Fri 
# 0.2723426 0.2919367 0.2917181 0.2844085 0.5000568 

# 0.1176 - (-0.02481) = 0.1424

```

In particular, our analysis indicates that shifting 10% of logins from other weekdays to Fridays, without increasing overall platform usage, could boost student lesson completion by around `r round(10 * c(-1/4, -1/4, -1/4, -1/4, 1) %*% habits_model$coefficients[2:6], 2)`%. For the typical teacher, this means switching from one Friday login per month to two while reducing one login from another weekday during that month, resulting in an increase of `r round(14.24 * c(-1/4, -1/4, -1/4, -1/4, 1) %*% habits_model$coefficients[2:6], 2)`% in average weekly lesson completion. This outcome underscores the importance of strategic engagement rather than just login frequency.

Based on our data analysis and focus group discussions with teachers and administrators, we hypothesized that this pronounced effect may be due to two key factors. First, Friday logins could facilitate "Reflective Catch-Up," enabling teachers to review and analyze the previous week's activities and make necessary adjustments. Second, "Foresighted Planning" may occur on Fridays as teachers proactively plan for the upcoming week, a practice less common on weekends.

## Study 2: Nudge EngineeringâFrom Data to Intervention Design

In Study 1, we discovered that specific psychopedagogical strategies, habits, and timing of teacher interactions were associated with promoting student success on digital platforms. With this foundation, we could craft effective educational interventions and strategies. By leveraging the insights from the first study, we aimed to develop targeted interventions to increase student learning outcomes.

### Study 2a: Using ICA Insights to Design an Empathy-Driven Teacher Intervention

We used the first component from the ICA in Study 1a to design an "empathy" nudge. Our analysis in Study 1a indicated that this process was linked to the highest weighted behaviors in IC1 (note that the original analysis yielded an "empathy" coefficient of 0.0255 for the whole sample and 0.1434 for the curriculum-only subsample, versus `r round(as.numeric(ica_fe_summary[ica_fe_summary$term == "ic1","estimate"]), 4)` and `r round(as.numeric(ica_fe_subsample_summary[ica_fe_subsample_summary$term == "ic1","estimate"]), 4)`, respectively, in the revised analysis; refer to SI for details). This intervention involved sending emails to teachers, encouraging them to adopt a more student-centered teaching approach by viewing math problems from their students' perspective. The emails contained key related messages that mentioned the general findings of a "recent analysis" (i.e., Study 1a), emphasized the importance of empathy in teaching math, and included advice from other teachers and helpful tips for assisting students who struggle with a lesson. The emails also suggested specific actions, including using Zearn's features to view lessons from a student's perspective and checking the Tower Alerts Report (see SI for the complete emails). We hypothesized that this empathy approach would significantly enhance student performance.

### Study 2b: Using Habit Analysis Insights to Design a Friday Login Intervention

In Study 2b, we aimed to test whether nudging teachers to log in on Fridays, as opposed to an unspecified day, would improve student performance. Our approach involved sending emails to teachers, reminding them to log in on Fridays and suggesting reflective and planning behaviors to engage in during those sessions. These emails included testimonials from other teachers, research insights, and motivational messages to encourage habit formation. Teachers were also encouraged to review student progress and identify areas where students needed additional support (see SI for the complete emails). Our rationale was that Friday logins would aid in reflecting on the week's activities and proactive planning for the following week.

In addition, we designed a control email tailored to this treatment. These emails were sent every Wednesday to remind Zearn teachers to review their Zearn Pace Report without the personalized behavioral prompts or motivational materials in the Friday treatment group emails. Although the control emails prompted participation with Zearn, they did not provide information on Friday logins, introspection, or strategic preparation.

We hypothesized that the Friday approach to teacher engagement with the platform would yield a greater impact on student achievement than the Wednesday control or our study-wide control.

### Intervention Impact Evaluation

In collaboration with Zearn, Study 2 became part of a large multi-arm âmegastudy,â set in motion during a critical four-week period in 2021, involving over 140,000 teachers and nearly 3 million students. Teachers in our study taught a median of 20 students (mean = 21.30, SD = 15.31) in a median of 1 classroom (mean = 1.15, SD = 0.59). Before the intervention, teachers in our study had, on average, logged into Zearn a total of 3.62 times between July 1, 2021, and September 14, 2021 (SD = 8.49) (see SI for a complete description of the sample).

```{stata}
#| include: false

*********************************************************
* Adapted from:                       							    *
* Title: Zearn Main and Per Protocol Analyses				    *
* Author: Ramon Silvera Zumaran		      						    *
* Date: 13 Jul 2023										   	              *
*********************************************************

clear all
set more off
set type double
* ssc install blindschemes
set scheme plotplain

*************************
**# Defining Programs #**
*************************

* Data Transformation and BH Adjustment Program
/* This program performs a Benjamini-Hochberg Adjustment for multiple hypothesis
   testing and summarizes the original results alongside the adjusted p-values */

cap drop bhadjustment

program define bhadjustment
	args cond_num
	
	matrix results = r(table)
	local colnames : colfullnames results
	local colnum : colsof results
	local cond_num = `cond_num'
	
	gen category_num = .
	gen category = ""
	gen coef = .
	gen standard_error = .
	gen p_val = .
	gen p_val_BH = .
	
	forvalues j = 1/`cond_num' {
		levelsof condition_cat if condition == `j', local(level) clean
		replace category_num = `j' in `j'
		replace category = "`level'" in `j'
		replace coef = results[1,`j'] in `j'
		replace standard_error = results[2,`j'] in `j'
		replace p_val = results[4,`j'] in `j'
	}
	
	keep category_num-p_val
	
	drop if mi(category)
	
	sort category_num
	drop category_num
	
	sort p_val
	gen rank = _n

	gen p_val_BH = p_val*(`cond_num'/rank)

	forvalues i = 1/`cond_num' {
		local num = `cond_num'+1
		local ind = `num'-`i'
		if p_val_BH[`ind'+1] < p_val_BH[`ind'] {
			replace p_val_BH = p_val_BH[`ind'+1] in `ind'
		}
	}
	
	replace p_val_BH = round(p_val_BH, 0.001)

	drop rank
	
	list

end

***************************************
**# Defining globals for covariates #**
***************************************
/* These are our pre-registered covariates for our primary DV regressions. */

* Main Analysis: Badges
global badges_covs perc_G1 perc_G2 perc_G4 perc_G5 perc_G6 perc_G7 perc_G8 is_free teacher_badges_pre pre_intv_logins total_associated_students n_classes acct_age missing_classroom_grade opened_weekneg1 opened_week0

********************************************
**# Primary and Secondary DV Regressions #**
********************************************

* Student Math Performance
use "Data/megastudy_clean.dta", clear
// Pre-registered regression
quietly areg teacher_badges_during ib16.condition $badges_covs [aweight=total_associated_students], absorb(combined_school_id)

quietly putexcel set study2regress.xlsx, replace

matrix coef = r(table)'
putexcel A1 = matrix(coef), names
quietly putexcel save

file open myfile using rsquare.txt, write replace
file write myfile `"`e(r2)'"'
file close myfile
file open myfile using ntotal.txt, write replace
file write myfile `"`e(N)'"'
file close myfile

```

```{stata}
#| eval: false

quietly areg teacher_badges_during ib16.condition $badges_covs [aweight=total_associated_students], absorb(combined_school_id)
predict xb

# Regression-estimated average number of lessons completed by students in the control condition:
sum xb if 16.condition == 1
quietly sum xb if condition == 7 | condition == 8 | condition == 8
gen diff = r(mean) - 1.76114

// Wald test for checking whether friday and weekly treatment are equal 
test 9.condition-8.condition=0
local sign_car = sign(_b[8.condition]-_b[7.condition])
display "H_1: Wednesday >= Friday p-value = " normal(`sign_car'*sqrt(r(F)))

// Cohen's d
esize twosample xb if condition == 8  |  condition == 16, by(condition)
esize twosample xb if condition == 9  |  condition == 16, by(condition)
esize twosample xb if condition == 8  |  condition == 9, by(condition)

```

```{r}
#| label: tbl-megastudy-regression
#| tbl-cap: "Efficacy of Different Teacher Engagement Interventions on Student Learning Outcomes. The table showcases the marginal effects of the 'Empathy' and 'Friday Login' interventions compared to the study-wide control as a baseline. It also includes the results from the Friday-specific control group. We measure student achievement by the number of badges students earned."

megastudy_model <- readxl::read_xlsx("study2regress.xlsx")
names(megastudy_model)[1:5] <- c("Treatment", "Coefficient",
                                 "Std. Error", "t_value", "p_value")
megastudy_model[c(7:9,33),1] <- c("Empathy", "Friday",
                                  "Friday-control", "Intercept")
unlink("regress.xlsx")

megastudy_model[c(7:9,33),1:5] %>%
  mutate(
    Coefficient = mapply(format_coef, Coefficient, `Std. Error`),
    p_value = mapply(format.pval, p_value, digits = 3, eps = .001),
    Treatment = case_when(
      Treatment == "Intercept" ~ "Study Control",
      Treatment == "Empathy" ~ "Empathy",
      Treatment == "Friday" ~ "Friday",
      Treatment == "Friday-control" ~ "Friday Control",
      .default = Treatment  # Default case
    )
  ) %>%
  mutate(p_value = sub("0.", ".", p_value)) %>%
  select(Treatment, Coefficient, p_value) %>%
  gt() %>%
  tab_spanner(
    label = "Total Badges",
    columns = c(Coefficient, p_value)
  ) %>%
  cols_label(
    Treatment = "Treatment",
    Coefficient = "Estimate (SE)",
    p_value = "P-value"
  ) %>%
  cols_align(
    align = "center",
    columns = c(Coefficient, p_value)
  ) %>%
  # Add rows for R-squared and N
  rows_add(
    Treatment = c("R-squared", "N"),
    Coefficient = c(
      sprintf("%.3f", as.numeric(readLines("rsquare.txt", warn=FALSE))),
      readLines("ntotal.txt", warn=FALSE)
    ),
    p_value = c("", "")
  ) %>%
  # Use tab_style to add double lines for model fit statistics
  tab_style(
    style = list(
      cell_borders(
        sides = "top",
        style = "double",
        color = "lightgray",
        weight = px(4)
      )),
    locations = cells_body(
      columns = everything(),
      rows = Treatment %in% c("R-squared")
    )
  ) %>%
  fmt_markdown() %>%
  as_raw_html()
unlink("rsquare.txt")
unlink("ntotal.txt")

```

As stated in our pre-registration, we evaluated the impact of intervention messages on the number of math lessons completed by students during the four-week intervention period [@gallo2022a; @gallo2022b]. Students in the megastudy control condition completed a regression-estimated 1.761 lessons during the 4-week intervention period. @tbl-megastudy-regression shows that our interventions increased the number of math lessons completed by students during the intervention period by a regression-estimated average of 0.0487 lessons, which is a 2.77% increase over the megastudy control condition. Specifically, the empathy treatment increased the number of lessons completed by students by 0.0721 lessons, or a 4.09% increase over the megastudy control condition (d = 0.0188, p = .018). The Friday treatment increased the number of lessons completed by students by 0.0550 lessons, or a 1.81% increase over the megastudy control condition (d = 0.0192, p = .068). The Friday treatment was not significantly different from the Friday-specific control (F(1,118137) = 0.85, p = .357).

Table S3 presents unstandardized coefficients from our primary regression analysis and unadjusted, robust SEs and CIs. Additionally, we used the Benjamini-Hochberg (BH) procedure to compute adjusted p-values, which help to control for false discovery rates when conducting multiple comparisons [@benjamini1995]. Before adjusting for multiple hypothesis testing, all treatments exhibited significant benefits. However, only our treatment-specific control had a BH-adjusted p-value of less than 0.05. This intervention involved encouraging teachers to log in to Zearn weekly to receive updated student performance reports. Although reliable, the effect of this intervention was small, resulting in an estimated 0.0898 extra lessons completed in four weeks, or a 5.10% increase over the megastudy control (d = 0.0235, p = .003). Even after applying the James-Stein shrinkage procedure to adjust for the winner's curse (i.e., the maximum of 15 estimated effects is upward biased), we estimated that this intervention still produced 0.061 extra lessons completed, or a 3.46% increase over the control condition [@james1992].

# Discussion

This study aimed to improve student math learning on the Zearn platform by integrating data analysis into educational intervention. We identified critical teacher behaviors influencing student performance and evaluated two novel interventions: empathy and Friday logins.

Our first study revealed subtle but significant patterns in teacher engagement that traditional analyses might overlook. Study 1a identified a significant independent component strongly associated with metrics related to struggles and achievements, suggesting teachers' empathy-driven engagement, focusing on areas where students faced challenges or succeeded. This pattern also correlated with a significant increase in student achievement, aligning with findings emphasizing the importance of teacher empathy in educational outcomes [@hill2008; @battey2016]. In Study 1b, we discovered that teachers who logged into Zearn on Fridays had a notable impact on student math performance. This behavior suggested a commitment to continuous planning and support, echoing the findings of @blanchard2016 that technology integration does not need large-scale changes in practices to enhance student learning. Overall, our research highlights the importance of teacher engagement and personalized instruction and feedback in improving student performance on the Zearn platform. As @ertmer2012 have emphasized, aligning student-centered beliefs and practices is vital to success, regardless of technological, administrative, or assessment barriers.

In Study 2a, the success of the empathy intervention can be attributed to its alignment with psychological principles that emphasize the importance of emotional connectivity between teachers and students. This intervention appears to have fostered a more engaging and supportive learning environment, a feature essential for the success of digital platforms [@lavecchia2016; @koch2015]. In contrast, Study 2b's unexpected results highlight the complexities of behavior change in educational settings, suggesting that repetitive routines without meaningful engagement or context may not enhance learning outcomes. Notably, our study-specific control outperformed all other megastudy interventions. Initial analyses from Duckworth et al. (2024) suggest that this effect is due to the higher salience of personalization present, such as the suggestion of classroom-specific actions (e.g., "CLICK HERE to see which of your students are struggling"). The lack of additional content in this control highlighted actionable steps to engage with students, an effect which, in retrospect, aligns with previous literature.

Our study's insights transcend the immediate context. It showcases the potential of data-driven interventions to create strategies that cater to the unique needs of teachers and students. This approach paves the way for personalized and responsive pedagogy. In the realm of digital learning, our findings underscore the pivotal role of teacher engagement and tailored content, which are crucial for replicating and enhancing the benefits of traditional classrooms. In essence, our research provides a roadmap for designing online educational tools that are more effective and engaging.

While yielding several insights, our study is limited by its focus on a specific demographic and educational context within Zearn. The data provide teacher and classroom interactions only within the online platform, which may reflect or influence in-class dynamics but does not directly measure in-class interactions. Consequently, generalizing our results to other educational settings, cultures, or age groups may be challenging. Future research could explore the relationship between online engagement patterns and in-class teaching practices. Further, we could not examine variation in performance among individual students within each classroom because of data aggregation, and future research could examine the variation in treatment effects among students within each classroom. Additionally, while our approach was more cost-effective and less time-intensive, it achieved a more modest impact than the substantial effects seen in more intensive programs [@banerjee2007; @dipietro2023]. The simplicity of our email interventions and the short duration of the study likely contributed to these results, although their magnitude aligns with other reports from educational technology applications [@cheung2013]. Future research could explore more engaging and intensive intervention methods over extended periods to potentially yield greater impacts on learning outcomes.

While rooted in education data, our research introduces a paradigm with significant implications beyond its primary focus. By combining data-driven analysis with targeted behavioral interventions, our approach offers a flexible framework that can be adapted to various fields. Whether in healthcare, environmental behavior, or organizational management, our methodology demonstrates the potential to harness data insights for effective behavioral change. Hence, our study also serves as a catalyst for innovative approaches in diverse fields where behavior modification is crucial.

{{< pagebreak >}}

# Materials and Methods

## Study 1

### Data Collection

The dataset used in this study was automatically collected by Zearn's servers during the 2019-2020 academic year (September 2019 to May 2020). The platform tracked user interactions, such as teacher and student logins, completed lessons by students, and professional development modules completed by teachers. Teacher actions were timestamped to the second, providing granular data on their behavior. To protect student privacy, student data was aggregated at the classroom-week level, including measures of student achievement and indicators of student struggles. We merged these data with a version of teacher data aggregated to the weekly level.

This study was conducted in accordance with ethical standards and received exempt status from the Institutional Review Board (IRB) at the University of Pennsylvania. The studyâs methodologies were designed to ensure the confidentiality and anonymity of all participants involved, adhering strictly to ethical guidelines for educational research.

To promote transparency and replicability of our study, we deposited the de-identified dataset and code used in our analyses in a publicly accessible database, available at the GitHub repository: <https://github.com/SeanHu0727/zearn_nudge.git>

### Inclusion Criteria

The dataset included various schools across Louisiana (see Fig S1 for geographic distribution). We also excluded inactive teachers (those with no recorded activity for over two months) from the dataset. We defined the following inclusion criteria for classrooms:

1.  Classrooms linked to a single teacher

2.  Classrooms with no more than seven months of inactivity during the academic year

3.  Classrooms with an average of no less than five actively engaged students

In study 1a, we further categorized Zearn usage into 'curriculum' and 'non-curriculum' cases. 'Curriculum' refers to scenarios where Zearn is integrated as a core component of the school's daily schedule. 'Non-curriculum' cases, meanwhile, involve Zearn being used alongside different core curricula, resulting in varied consistency in usage.

## Analysis of Study 1a

### Independent Component Analysis (ICA)

We extracted all teacher behavioral variables from the dataset that displayed non-zero variance and standardized them to have a zero mean and unit variance. To determine the ideal number of independent components, we performed ICA using a range of components from 1 to 10. Our decision on the optimal number was informed by recognizing the "elbow" on the scree plot (i.e., the point at which adding more components yields diminishing increases in total explained variance), yielding three independent components. We used the `icafast` function from the R `ica` package for all ICAs conducted [@helwig2022].

### Panel Regression

We estimated a fixed-effects panel regression model with the `plm` package [@croissant2008] in R. The dependent variable was defined as:

::: {.content-visible when-format="pdf"}
```{=tex}
\begin{align*}
\ln (\text{Badges}_{itc} + 1) = & \ \beta_1 \text{IC1}_{it} + \beta_2 \text{IC2}_{it} + \beta_3 \text{IC3}_{it}  \\
& + \text{FE}_{\text{Teacher}, i} + \text{FE}_{\text{Week}, t} + \text{FE}_{\text{Class}, c} + \epsilon_{itc}
\end{align*}
```
:::

::: {.content-visible when-format="docx"}
$$
\begin{align*}
\ln (\text{Badges}_{itc} + 1) = & \ \beta_1 \text{IC1}_{it} + \beta_2 \text{IC2}_{it} + \beta_3 \text{IC3}_{it}  \\
& + \text{FE}_{\text{Teacher}, i} + \text{FE}_{\text{Week}, t} + \text{FE}_{\text{Class}, c} + \epsilon_{itc}
\end{align*}
$$
:::

where $i$, $c$, and $t$ index the teacher, class, and week, respectively. The model includes fixed effects for teacher ($\text{FE}_{\text{Teacher}, i}$), week ($\text{FE}_{\text{Week}, t}$), and class ($\text{FE}_{\text{Class}, c}$). Standard errors were clustered at the teacher level, calculated using the `vcovHC` function [@millo2017] in R.

## Study 1b

### Linear Regression

We estimated a linear model using the `lm` function [@rcoreteam2023] in R:

::: {.content-visible when-format="pdf"}
```{=tex}
\begin{align*}
\sum_{c=1}^{C_i} \text{(Avg. Badges)}_{c} = & \ \beta_0 + \sum_{m=1, m \neq 7}^{12} \beta_m \text{Login\%}_{m,i} + \sum_{d=1, d \neq 7}^{7} \beta_{12+d} \text{Login\%}_{d,i} \\
& + \beta_{19} \text{(Avg. Minutes)}_i + \beta_{20} \text{(Avg. Streak)}_i \\
& + \beta_{21} \text{(Avg. Days Between Logins)}_i + \text{FE}_{\text{School}} + \epsilon_i
\end{align*}
```
:::

::: {.content-visible when-format="docx"}
$$
\begin{align*}
\sum_{c=1}^{C_i} \text{(Avg. Badges)}_{c} = & \ \beta_0 + \sum_{m=1, m \neq 7}^{12} \beta_m \text{Login\%}_{m,i} + \sum_{d=1, d \neq 7}^{7} \beta_{12+d} \text{Login\%}_{d,i} \\
& + \beta_{19} \text{(Avg. Minutes)}_i + \beta_{20} \text{(Avg. Streak)}_i \\
& + \beta_{21} \text{(Avg. Days Between Logins)}_i + \text{FE}_{\text{School}} + \epsilon_i
\end{align*}
$$
:::

for teacher $i$ with $C_i$ classes. `Login%` represents the percentage of logins by teacher $i$ during each month $m$ and on each day of the week $d$ relative to other months and days, respectively. We exclude July and Sunday to prevent multicollinearity. Standard errors are clustered by school with the `vcovCL` function [@zeileis2004; @zeileis2020] in R.

## Study 2

We used the findings from Study 1 to inform the creation of two interventions as part of a larger multi-arm "mega study" that involved 15 sets of nudges.

### Implementation

Study 2 was conducted in collaboration with Zearn [@zearnma2023] and was pre-registered for the fall of 2021. To incentivize teacher participation during our "intervention period" from September 15, 2021, to October 12, 2021, all teachers on the platform received two messages on September 1 and 8, 2021. These messages informed them they had been enrolled in the "Zearn Math Giveaway" and that every email opened until October 12, 2021, would earn them tickets. These tickets were used to enter drawings for various prizes, such as autographed children's books, stickers, and gift cards.

### Data Preprocessing

Following the megastudyâs pre-registered analysis plan (<https://osf.io/dgpkn),> we restricted analyses to Zearn Math teachers who were assigned to one of the megastudyâs conditions and who taught in at least one classroom with at least one student. However, our analyses excluded teachers who: (1) did not receive any emails because they had inactive accounts, invalid email addresses, or had opted out of receiving messages (n = 133,722 teachers), (2) neither logged onto the Zearn Math platform nor had an associated student who logged on the platform between March 1, 2021 and September 14, 2021 (n = 126,856 teachers), (3) had more than 150 students associated with their Zearn Math account as of October 18, 2021 (n = 6,766 teachers), or (4) had more than 6 classes associated with their Zearn Math account as of October 18, 2021 (n = 6,675 teachers). Among Zearn Math classrooms associated with the remaining 149,097 teachers, we further excluded: (5) 9,143 classrooms associated with more than one Zearn Math teacher (n = 12,012 teachers) and (6) 346 classrooms with grade levels corresponding to high school or post-high school (n = 141 teachers).

After exclusions, we randomized $N = 140,461$ teachers across 22,281 schools who served 2,992,077 students in 161,722 classrooms into one of the intervention conditions or the control condition ($N_{\text{control}} = 29,513$). The control condition was larger than the interventions to account for multiple comparisons. Among $n = 16,372$, or $11.66\%$, of teachers, at least one of two problems occurred in the emails sent by Zearn Math during the intervention period: an email message that was intended but not sent ($n = 13,568$, or $9.66\%$ of teachers), or an email message that was sent but not intended (i.e., from a different treatment condition; $n = 2,804$, or $2.00\%$ of teachers). As these email problems were systematically related to treatment assignment ($\chi^2 = 33.01$, $df = 15$, $p = .005$), we did not exclude these participants and conducted intent-to-treat analyses. Refer to the SI for email problem prevalence by condition and study analyses that exclude or adjust for email problems, respectively.

### Statistical Analysis

We followed our pre-registered analysis plan to assess the effect of each treatment on the primary outcome of interest: math lessons completed by students during our four-week intervention period [@gallo2022a; @gallo2022b]. We estimated a weighted ordinary least squares (OLS) regression with the `areg` command in Stata [@statacorp2023]. Each teacherâs observations were weighted proportionally to the total number of students in their Zearn classroom(s).

The primary predictors were indicators for each intervention, omitting the control condition. The regression also included the following control variables: (1) school fixed effects, (2) an indicator for the teacher's account type (free or paid), (3) the number of times the teacher logged into Zearn prior to the study, from August 1 to September 14, 2021, (4) the total number of students in the teacher's classroom(s) as of October 18, 2021, (5) the number of classrooms associated with the teacher as of October 18, 2021, (6) the number of days since the teacher obtained a Zearn account prior to the study's launch, (7) the number of days separating the study's launch and the start of the teacher's school year, (8) the average number of lessons completed by a teacher's students from the start of their school year to the start of the intervention (or from July 14, 2021, if the school year start was not known), (9) whether the teacher opened our September 1, 2021 email announcing the upcoming Zearn Math Giveaway, (10) a similar indicator for our September 8, 2021, email reminding them of the giveaway, and (11) the percentage of a teacher's students in each grade except for third grade to avoid multicollinearity, since for most teachers, students were in a single grade.

### Study 2a: Empathy Intervention Design and Implementation

We designed four emails encouraging teachers to adopt a more student-centered perspective when engaging with the Zearn platform to gain pedagogical content knowledge. The emails included testimonials from experienced teachers, research-based insights on the role of empathy in math education, and specific strategies for using Zearn's features to understand and address student challenges. A population of 7,443 teachers was chosen at random to receive these emails.

### Study 2b: Friday Logins Intervention Design and Implementation

We designed four emails encouraging teachers to regularly log into the Zearn platform on Fridays. The emails emphasized the benefits of using Fridays for reflective review and proactive planning, highlighting how this practice could help teachers better support student learning. Teachers were provided with specific suggestions for activities during these Friday sessions, such as analyzing student progress data, identifying areas for improvement, and planning targeted interventions for the upcoming week. A total of 7,476 teachers was randomly selected to receive these emails. Additionally, to assess the unique impact of the Friday login habit, we included an active control condition that received similar email prompts on Wednesdays with links to specific actions on Zearn but without a focus on Fridays. A group of 7,577 teachers were randomly chosen to participate in this control group.

# Acknowledgments

We thank Billy McRae, Michael Irvine, and Audrieanna Burgin for their valuable insights and support during our data analysis. We thank Zearn administrators and teachers for feedback on our intervention design. We also thank Cassandra Horri for the helpful comments that improved this manuscript.

{{< pagebreak >}}

# References

::: {#refs}
:::

{{< pagebreak >}}

# Supplementary Information {.appendix}

## **Differences in the Original vs. Revised Analysis**

As noted in the main text, we have corrected a few inaccuracies in our initial analysis, improving the robustness of our methodology. To be fully transparent, we have outlined below all the changes made from the original to the revised analysis. Please refer to Tables S3 to S5 in this supplementary document for the original results that inspired our treatments.

1.  Temporal Alignment Adjustments: The original method used the standard week and year delineation based on the Gregorian calendar. The new analysis uses the ISO week date system to ensure correct week numbering, particularly around the transition from one year to the next, which corrected previous week misclassifications.

2.  Duplicate Record Management: The dataset contains a small number of duplicate classroom-week pairs generated in classrooms with more than one teacher linked to it. The original method removed the first duplicate occurrence, consequently discarding data from teachers with larger ID numbers. The revised analysis orders and filters duplicate records by the number of classes each teacher manages, retaining data from teachers involved in fewer classes, thereby minimizing the inclusion of supervisory rather than direct instructional roles.

3.  Independent Component Analysis: The original ICA used the `fastICA` package, presenting some inconsistencies: non-deterministic component sequencing, arbitrary sign inversions, and varied loading coefficients, even with a set seed. The updated approach adopts the `ica` package, ensuring orderly component arrangement and consistent outputs. Sign orientation is now standardized, maintaining the largest loading variable as positive. Further, for the models restricted to schools that use Zearn as a main component of their curriculum, the ICA was estimated separately (see Table S4).

4.  Model Selection Adjustment: Initially, we estimated a random effects model. Following a Hausman test indicating inconsistency (chi-square = 81.31, df = 4, p \< .001), we transitioned to a fixed effects model. The revised approach also incorporates robust standard errors, adjusting for heteroskedasticity and autocorrelation [@arellano1987; @long2000].

5.  Removed User.Session: The original ICA included "User Session." This variable measures the frequency of a teacher's logins to the Zearn platform. As such, it does not offer substantive insight into the pedagogical nature of the teachers' interactions with the platform. We aimed to understand the educational impact of specific usage patterns rather than their frequency. Hence, "User Session" was deemed a nuisance variable as it risked overshadowing more pertinent patterns related to instructional engagement and effective pedagogical strategies. In particular, this variable mainly loads onto the "empathy" component. Its exclusion in our revised analysis has confirmed that the identified patterns genuinely reflect empathy in teaching approaches (see Tables S3 and S4).

## Supplementary Methods

### Independent Component Analysis (ICA)

We implement the FastICA algorithm [@hyvarinen2000] to estimate independent components from our dataset. In this model, matrix $X = \{x_{ij}\}_{I \times J}$ , consisting of $I$ samples across $J$ random variables, is expressed as a linear mixture of independent components $C$, represented by:

$$
X = C' M + E.
$$

Here, $C$ holds the independent components, $M$ is a mixing matrix, and $E$ denotes the noise. The aim is to minimize mutual information between components in $C$, which is achieved by maximizing their marginal negentropy, thereby rendering the columns of $C$ statistically independent.

The FastICA process begins by transforming $X$ into a whitened matrix $Y$, ensuring uncorrelated variables with unit variance. This transformation is achieved through eigenvalue decomposition:

$$
Y = X \times \begin{bmatrix} \frac{\mathbf{v_1}}{\sqrt{\lambda_1}} & \frac{\mathbf{v_2}}{\sqrt{\lambda_2}} & \frac{\mathbf{v_3}}{\sqrt{\lambda_3}} \end{bmatrix},
$$

where ($\lambda_1, \lambda_2, \lambda_3$) and ($\mathbf{v_1}, \mathbf{v_2}, \mathbf{v_3}$) are, respectively, the eigenvalues and eigenvectors of $\frac{X'X}{I}$.

Afterward, the algorithm approximates the negentropy with

$$
\hat{\theta}(c_n) = (\mathbb{E}[\ln(\cosh(c_n))] â \mathbb{E}[\ln(\cosh(z))])^2
$$

where $c_n, n \in {1, 2, 3}$, is one of the components, and $z$ is a Gaussian variable with zero mean and unit variance. FastICA iteratively maximizes this value across all components, producing an orthogonal rotation matrix $R_{3 \times 3}$ such that $C = YR$.

For more details on ICA and the FastICA algorithm, see @hyvarinen2000 and @helwig2013.

### Focus Group Discussions

In 2021, we conducted regular meetings (once a month, on average) with Zearn employees to discuss the interpretation of our data analyses. Additionally, on April 6th and 13th, 2021, we held "office hours" with Zearn teachers and administrators. For these specific meeting we prepared the following questions, although due to time limitations, we were unable to ask them systematically:

1.  With what regularity do you find logging into Zearn most helpful?

2.  What tasks do you typically do on Zearn?

3.  Please order the tasks you mentioned in how important they are in helping your teaching (from most important to least important).

4.  Please explain why \[top option from question 3\] is the most important.

5.  Have you done activities and exercises designed for the students on Zearn?

6.  If so, with what regularity do you do activities and exercises designed for the students on Zearn?

Below are some of the most relevant quotes from these meetings:

âWe could layer in some of the insights here, \[for example\], prompting teachers to go and do towers and get towers and go through mediation paths. I think that could be something we layer into one of those A/B tests.â - B.M., Zearn administrator

âIf students are not successful in Tower of Power, they get stuck, and it is the loop of self defeat - I know what I need to do, but I canât do it. Teachers donât check the tower alerts report enough so they can defeat that cycle.â - Math coach from CA

âFor everything I do, I try to look through the kidâs eyes. I look for the lightbulb moment, and the thing that finally gets kids to understand it.â -5th grade teacher from IL

âOne thing is when kids really understand something and have that âahaâ moment - and you know you have gotten through. And that could be academically or socially.â - Middle school teacher from CA

âIt took my students 50-60 minutes to get through a lesson. I have some kids who get fatigued by the length of the digital instruction and program. The boosts may not help them. Need to orient them to the Zearn lesson.â - Zearn Teacher

### PCS Analysis

```{r PCS-regression}
#| eval: false

library(pbapply)
library(parallel)
library(glmnet)
library(MLmetrics)
library(pROC)

df <- fread(file = "./Data/df_habits_long.csv") %>%
  mutate(dayofweek = factor(dayofweek, ordered = FALSE,
                            labels = c("Sun", "Mon", "Tue", "Wed",
                                       "Thu", "Fri", "Sat")))

# Delete users with fewer than 48 (k*8) log-ins: 8 data points per fold
df[,n_logged:=sum(logged), Adult.User.ID]
df_transformed <- df[n_logged>48]
# Create folds
df_transformed[,folds:=createFolds(logged,k=6, list = FALSE), Adult.User.ID]

# DEFINE LASSO FUNCTION 
indiv_lasso = function(lambdas = 10^seq(0,-3, by = -0.05),
                       data, normalize = TRUE){
## THIS FUNCTION TAKES AN INDIVIDUAL TIME SERIES DATA, SPLIT THE DATA INTO TRAIN AND TEST SET AND FIT THE LASSO MODEL TO THE TRAIN SET
  ##-----INPUT-----
  # lambdas       : grid of tuning parameters, default values provided
  # data          : individual time series (include both train and test data)
  # normalize     : whether to scale the LASSO coefficients (default is TRUE)
  # metric        : predictability measurement to use (AUC vs PRAUC), default is AUC
  # pre_post      : if TRUE, will include indicators for pre/post-habit interacted with reward revaluation variables
  ##-----OUTPUT-----
  # full_model    : full LASSO model, can be used to obtain prediction, calculate AUC in a test data
  # best_lambda   : the best lambda obtained from cross-validation procedure
  # best_coef     : LASSO coefficients using best_lambda
  # auc_holdout   : AUC on the test set
  # auc_train     : AUC on the train set
  # holdout_y     : vector of outcome variables on the test set
  # holdout_pred  : vector of predictions made on the test set
  # insample_pred : vector of predictions made on the train set
  # participant_id: ID of the corresponding individual
  # X_full        : full matrix of covariates in the train data
  # X_holdout     : full matrix of covariates in the test data
  data          = data[!is.na(time_lag)]
  pred_list     = as.list(seq_len(5))
  model_list    = as.list(seq_len(5))
  holdout_data  = data[folds == 6]
  train_data    = data[folds <  6]
  X = model.matrix(logged ~ streak + 
                     I(streak^2) + 
                     streak_dow + 
                     I(streak_dow^2) +
                     factor(Month) +
                     factor(dayofweek) +
                     time_lag +
                     I(time_lag^2) +
                     prev_week_visits +
                     prev_min_zearn - 1, 
                   data = data)
  for(i in 1:5){
    validate_data = data[folds == i]
    X_train = X[data[folds!=i, which = TRUE],]
    lasso = glmnet(
      x      = X_train,
      y      = data[folds != i]$logged,
      alpha  = 1,
      lambda = lambdas,
      family = "binomial",
      intercept = TRUE,
      standardize = TRUE
    )
    X_validate = X[data[folds==i, which = TRUE],]
    pred = predict(lasso, newx = X_validate, type = 'response')
    auc   = sapply(c(1:length(lambdas)), function(u) 
      PRAUC(y_true = validate_data$logged, y_pred = pred[,u]))  
    pred_list[[i]] = auc
    model_list[[i]] = lasso
  }
  X_full    = X[data[folds < 6, which = TRUE],]
  X_holdout = X[data[folds == 6, which = TRUE],]
  all_auc    = do.call('rbind', pred_list)
  lambda_auc = apply(all_auc, 2, mean)
  full_model = glmnet(
    x      = X_full,
    y      = train_data$logged,
    alpha  = 1,
    lambda = lambdas,
    family = "binomial",
    intercept = TRUE,
    standardize = normalize
  )
  best_lambda   = lambdas[which.max(lambda_auc)]
  best_coef     = coef(full_model, s = best_lambda)
  insample_pred = predict(full_model,
                          newx = X_full,
                          s = best_lambda, type = "response")
  pred_holdout  = predict(full_model,
                          newx = X_holdout,
                          s = best_lambda, type = "response")
  auc_train     = PRAUC(y_true = train_data$logged,
                        y_pred = c(insample_pred))
  auc_holdout   = PRAUC(y_true = holdout_data$logged,
                        y_pred = c(pred_holdout))  
  return(
    list("full_model" = full_model,
         "best_lambda" = best_lambda,
         "best_coef" = best_coef, 
         "auc_holdout" = auc_holdout,
         "auc_train" = auc_train,
         "holdout_y" = holdout_data$logged,
         "holdout_pred" = c(pred_holdout),
         "insample_pred" = c(insample_pred), 
         "participant_id" = as.character(data$Adult.User.ID[1]),
         "X_full" = X_full,
         "X_holdout" = X_holdout)
  )
}

# CALCULATE INDIVIDUAL AUC SEQUENCE
individual_auc_seq = function(model, placebo = FALSE){
  ## THIS FUNCTION TAKES AN OUTPUT OF THE gym_indiv_lasso FUNCTION AND RETURN THE AUC SEQUENCE OF THE CORRESPONDING INDIVIDUAL
  ##-----INPUT-----
  # model         : object returned by the gym_indiv_lasso function
  ##-----OUTPUT-----
  # A sequence of AUC (2-week windows) for the corresponding individual
  
  data       = df_transformed[Adult.User.ID == model$participant_id]
  data       = data[!is.na(time_lag)]
  if(placebo == TRUE){
    set.seed(2021)
    data[,fake_time:=sample(time, replace = FALSE)]
    data[,time:=fake_time]
  }
  max_time   = max(data$time)
  data       = data[order(time)]
  X_pred = model.matrix(logged ~ streak + 
                          I(streak^2) + 
                          streak_dow + 
                          I(streak_dow^2) + 
                          factor(Month) +
                          factor(dayofweek) +
                          time_lag +
                          prev_week_visits +
                          prev_min_zearn +
                          I(time_lag^2) - 1, 
                        data = data)
  data       = data[,c("time", "logged")]
  chunk_ends = seq(14, max_time, 14)
  get_auc = function(end_time){
    ytrue      = data[time <= end_time]$logged
    pred       = predict(model$full_model,
                         newx = X_pred[data[time<= end_time, which = TRUE],], 
                         s = model$best_lambda, type = "response")
    if(mean(ytrue) != 0){
      roc  = roc(response = factor(ytrue, levels = c(0,1)),
                 predictor = c(pred), plot = FALSE,
                 quiet = TRUE)
      return(roc$auc[1])  
    }
    if(mean(ytrue) == 0){
      return(NA)
    }
  }
  return(sapply(chunk_ends, get_auc))
}

auc_exp_fit = function(chunk){
  ## THIS FUNCTION TAKES A SEQUENCE OF NUMBERS (e.g. SEQUENCE OF AUC RETURNED BY THE individual_auc_seq FUNCTION) AND FIT AN EXPONENTIAL CURVE OF THE FORM a+bexp(-ct)
  ##-----INPUT-----
  # chunk         : sequence of numbers
  ##-----OUTPUT-----
  # a,b,c         : parameters of the exponential curve 
  # fit           : fitted values as predicted by the exponential curve
  # Rsq           : R-squared of the fit
  T_grid = seq(14, length(chunk)*14, 14)             
  init   = list(a = 0.1, b = 0.01, c = 0.002)
  fit    = NULL
  i=1
  while(is.null(fit) & i<500){
    fit    = tryCatch(minpack.lm::nlsLM(chunk~a-(b/(c*T_grid)) *
                                          (1-exp(-c*T_grid)),
                                        start = init,
                                        lower = c(0, 0, 0),
                                        upper = c(Inf, Inf, Inf),
                                        control = list(maxiter = 1000)),
                      error = function(e) NULL)
    init   = lapply(init, function(u) u*1.1)
    i      = i+1
  }
  return(list(
    "a" = coef(fit)[1],
    "b" = coef(fit)[2],
    "c" = coef(fit)[3],
    "fit" = fit$m$fitted(),
    "Rsq" = 1-sum(fit$m$resid()^2)/((length(chunk)-1)*var(chunk, na.rm = TRUE))
  ))
}


# Run Lasso
# Set up parallel backend
cl <- makeCluster(detectCores() - 1) # leave one core available
clusterExport(cl, varlist = c("indiv_lasso", "individual_auc_seq",
                              "df_transformed", "auc_exp_fit"))
# Load required libraries on each worker
clusterEvalQ(cl, {
  library(dplyr)
  library(data.table)
  library(caret)
  library(glmnet)
  library(MLmetrics)
  library(pROC)
  library(minpack.lm)
})

# Create list of individuals
p_list <- as.list(unique(as.character(df_transformed$Adult.User.ID)))
zearn_lasso <- pblapply(X = p_list, cl = cl, FUN = function(p) 
  tryCatch(indiv_lasso(data = df_transformed[Adult.User.ID == p]),
           error = function(e) NULL))
lasso_true = zearn_lasso[sapply(zearn_lasso, function(x) !is.null(x))]

#-----Fit AUC sequences-----:
zearn_auc_seq = pblapply(X = lasso_true, cl = cl, FUN = function(u) 
  tryCatch(individual_auc_seq(u), error = function(e) NULL))

#-----Fit exponential curve to each AUC sequence-----:
zearn_auc_fit = pblapply(zearn_auc_seq, cl = cl, function(p)
  tryCatch(auc_exp_fit(p), error = function(e) NULL))

stopCluster(cl)

zearn_fittables = lapply(zearn_auc_fit, function(u) !is.null(u))
zearn_fittables = zearn_auc_fit[which(unlist(zearn_fittables))]

## Format into data
p_lasso = lapply(lasso_true, function(u) as.character(u$participant_id))
p_habitized = p_lasso[which(unlist(lapply(zearn_auc_fit, function(u)
  !is.null(u))))]  # Participants with fittable exp. curve
habitized_people = data.table(participant_id = unlist(p_habitized),
                              a = unlist(lapply(zearn_fittables,
                                                function(u) u$a)),
                              b = unlist(lapply(zearn_fittables,
                                                function(u) u$b)),
                              c = unlist(lapply(zearn_fittables,
                                                function(u) u$c)),
                              Rsq = unlist(lapply(zearn_fittables,
                                                  function(u) u$Rsq)))

## Get time to habituation
habitized_people[,time_max:=2/c] # Time for the g function to reaches maximum value 
habitized_people[,Tstar95:=-log(a/(20*b))/c, participant_id]  # Time to 95% habituation

#-----Analyze fit-----:
habitized_teachers = habitized_people[a >= 0.7 & a <= 1.5 & Rsq >= 0.5]

# AUC and achievement (log(Badges) ~ LASSO Scores + AUC)
# Create an empty list to store dataframes
df_list <- list()
# Get all possible variable names from all elements of lasso_true
all_vars <- unique(unlist(sapply(lasso_true, function(x) 
  if (!is.null(x$best_coef)) rownames(as.matrix(x$best_coef)) else NULL)))
# Create a template data frame with all variables
template_df <- as.data.frame(matrix(NA, ncol = length(all_vars), nrow = 1))
names(template_df) <- all_vars

# Loop through each element of lasso_true
for (i in 1:length(lasso_true)) {
  if (is.null(lasso_true[[i]]$best_coef)) {
    print(paste("Iteration", i, ": best_coef is NULL"))
  } else {
    # Extract the coefficients from the sparse matrix and transpose the dataframe
    coef <- as.data.frame(t(as.matrix(lasso_true[[i]]$best_coef)))
    
    # Make a copy of the template dataframe
    tmp <- template_df
    
    # Fill the corresponding coefficients
    for (var in names(coef)) {
      tmp[[var]] <- coef[[var]]
    }
    
    # Add participant_id and auc_holdout
    tmp$auc_holdout <- lasso_true[[i]]$auc_holdout
    tmp$participant_id <- lasso_true[[i]]$participant_id
    
    # Add the dataframe to the list
    df_list[[i]] <- tmp
  }
}
# Combine all dataframes into one
final_df <- do.call(rbind, df_list)
final_df <- final_df %>%
  filter(participant_id %in% habitized_teachers$participant_id) %>%
  mutate(participant_id = as.integer(participant_id)) %>%
  inner_join(teacher_student_usage_subset %>%
               group_by(Adult.User.ID) %>%
               summarise(Badges = mean(Badges.per.Active.User)),
             by = c("participant_id" = "Adult.User.ID")) %>%
  # Fill in the NAs
  mutate(across(where(is.numeric), ~ ifelse(is.na(.), 0, .)))

# Perform linear regression
PCS_model <- lm(log1p(Badges) ~ ., data = final_df %>%
                  select(-participant_id, -`(Intercept)`))

```

## Supplementary Tables

```{r}
#| label: tbl-ica
#| tbl-cap: "Independent Component Analysis (ICA) Results. This table displays the weights of each teacher activity in the three independent components (ICs). Notably, these metrics pertain to teacher activity on the platform, not student actions."

rownames(imod_fast$M) <- names(ICA_cols)
imod_fast$M %>%
  as.data.frame() %>%
  rename(
    `IC 1` = V1,
    `IC 2` = V2,
    `IC 3` = V3
  ) %>%
  arrange(-`IC 1`) %>%
  mutate(
    `IC 1` = round(`IC 1`, 2),
    `IC 2` = round(`IC 2`, 2),
    `IC 3` = round(`IC 3`, 2)
  )

```

```{r}
#| label: tbl-marginal-effects
#| tbl-cap: "Marginal Effects of ICA Components on Badges. Marginal effects are calculated by multiplying the ICA coefficients by the mean of each component and dividing by the standard deviation of the corresponding variable."


t(
  100 * rbind(
    tcrossprod(summary(ica_fe_model)$coefficients[-1,"Estimate"],
               imod_fast$M[,1:3]),
    tcrossprod(summary(ica_fe_model)$coefficients[-1,"Estimate"],
               imod_fast$M[,1:3]) /
      apply(teacher_student_usage_subset %>%
              select(RD.elementary_schedule:RD.foundational_lesson_guidance) %>%
              select(where(~ sd(.x, na.rm = TRUE) != 0)),
            FUN = sd, MARGIN = 2))
  ) %>%
  as.data.frame() %>%
  rename(
    `Marginal Effect of 1 SD` = V1,
    `Marginal Effect of 1 Unit` = V2
  ) %>% 
  arrange(-`Marginal Effect of 1 SD`)

```

## Supplementary Figures

```{r}
#| label: fig-scree
#| fig-cap: "Elbow (Scree) Plot for Determining Optimal Number of Components. The plot displays the proportion of variance explained by each independent component (IC). The optimal number of components is indicated by the 'elbow' of the plot, where the variance explained by each additional component is minimal."

elbow_plot

```

```{r}
#| cache: true
#| label: fig-teachers-map
#| fig-cap: "Geographical distribution of teachers across various parishes in Louisiana, and the top 5 cities with the highest number of teachers."

library(sf)
# Set the library path to where proj.db is located
# Sys.setenv(PROJ_LIB = "")
library(tidygeocoder)
library(tigris)
library(furrr)
plan(strategy = "multisession", workers = availableCores())

# Batch geocoding
# Sys.setenv(GEOCODIO_API_KEY = "")
unique_zipcodes <- unique(teacher_student_usage_subset$zipcode) %>%
  as.list()
address_geodata <- furrr::future_map_dfr(.x = unique_zipcodes, 
                               ~ geo(postalcode = .x,
                                     country = "United States",
                                     method = 'geocodio',
                                     full_results = TRUE,
                                     progress_bar = FALSE)) %>%
  select(postalcode,
         address_components.city,
         address_components.county,
         lat, long) %>%
  rename(
    city = address_components.city,
    county = address_components.county
  ) %>%
  mutate(
    postalcode = as.integer(postalcode)
  )

# Merge the geocoding results back into the original data.table
n_teachers <- teacher_student_usage_subset %>%
  group_by(Classroom.ID, Adult.User.ID, MDR.School.ID, zipcode) %>%
  summarize(Students...Total = mean(Students...Total), 
            .groups = "keep") %>%
  left_join(address_geodata, by = c("zipcode" = "postalcode"))

# Get the top 5 cities by number of teachers
# Aggregate the data to get the number of teachers in each city
top_cities <- n_teachers %>%
  group_by(city, county) %>%
  summarize(
    num_teachers = n_distinct(Adult.User.ID),
    lat = mean(lat, na.rm = TRUE),
    long = mean(long, na.rm = TRUE),
    .groups = 'drop'
  ) %>%
  arrange(desc(num_teachers)) %>%
  slice_head(n = 5)

# Get the Louisiana county map data
df_map <- tigris::counties(state = "LA",
                           progress_bar = FALSE) %>%
  # sf::st_set_crs(4326) %>%
  left_join(
    n_teachers %>%
      group_by(county) %>%
      summarize(num_teachers = n_distinct(Adult.User.ID)) %>%
      mutate(county = case_when(
        county == "Lasalle Parish" ~ "LaSalle Parish",
        county == "St. John The Baptist Parish" ~ "St. John the Baptist Parish",
        .default = county)),
    by = c("NAMELSAD" = "county")
    ) %>%
  mutate(num_teachers = ifelse(is.na(num_teachers),0,num_teachers)) %>%
  sf::st_as_sf()

ggplot() +
  geom_sf(data = df_map, aes(fill = num_teachers)) +
  scale_fill_continuous(name = "Number of Teachers",
                        low = "white", high = "red", na.value = "gray90") +
  labs(
    title = "Number of Teachers by Parish in Louisiana"
  ) +
  theme_minimal() +
  theme(
    legend.position = "bottom",
    panel.grid = element_blank(),
    axis.text = element_blank(),
    axis.title = element_blank()
  ) +
  geom_point(data = top_cities, aes(x = long, y = lat)) +
  geom_text_repel(data = top_cities, aes(x = long, y = lat, label = city),
                  size = 3, color = "black")

```

## Supplementary Discussion

### Original Analysis

```{r ICA-User-Session}
#| label: tbl-ica-original
#| tbl-cap: "Independent Component Analysis (ICA) Results without including User Sessions."

# Perform ICA to choose number of components
# Removing columns with zero variance

ICA_cols <- teacher_student_usage_subset %>%
  select(`User Session`:RD.grade_level_teacher_materials) %>%
  select(where(~ sd(.x, na.rm = TRUE) != 0)) %>%
  mutate_all(scale)

imod_fast2 <- ica(ICA_cols, nc = 3)

# Iterate through each principal component
for (i in 1:ncol(imod_fast2$M)) {
    # Find the index of the variable with the largest absolute coefficient
    largest_coef_index <- which.max(abs(imod_fast2$M[, i]))

    # Check if the largest coefficient is negative
    if (imod_fast2$M[largest_coef_index, i] < 0) {
        # Multiply all coefficients in this principal component by -1
        imod_fast2$M[, i] <- -1 * imod_fast2$M[, i]
        imod_fast2$S[, i] <- -1 * imod_fast2$S[, i]
    }
}

rownames(imod_fast2$M) <- names(ICA_cols)
imod_fast2$M %>%
  as.data.frame() %>%
  arrange(-V1) %>%
  mutate(
    V1 = round(V1, 2),
    V2 = round(V2, 2),
    V3 = round(V3, 2)
  ) %>%
  rename(
    `IC 1` = V1,
    `IC 2` = V2,
    `IC 3` = V3
  )
  
imod_fast2$vafs

```

### Robustnes Checks

```{r PCA}
#| eval: false

pca_result <- prcomp(PCA_cols, scale. = TRUE)
prop_variance <- summary(pca_result)$importance[2, 1:10]

# Add to dataset
teacher_student_usage_subset <- teacher_student_usage_subset %>%
  mutate(
    pc1 = pca_result$x[, 1],
    pc2 = pca_result$x[, 2],
    pc3 = pca_result$x[, 3]
  )


## Robustness
# Exclude User.Session
ICA_cols <- teacher_student_usage_subset %>%
  select(RD.elementary_schedule:RD.foundational_lesson_guidance) %>%
  select(where(~ sd(.x, na.rm = TRUE) != 0)) %>%
  mutate_all(scale)

imod_fast2 <- ica(ICA_cols, nc = 3)

# Variance explained
imod_fast2

# Creating a data frame for the ICA loadings
ica_loadings_df <- as.data.frame(t(ica_result$A))
colnames(ica_loadings_df) <- paste("Component", 1:ncol(ica_loadings_df))
ica_loadings_df$Feature <- colnames(teacher_student_usage_subset[13:39])[ICA_cols]

# Creating the table with gt
ica_loadings_table <- ica_loadings_df %>%
  relocate(Feature) %>%
  gt() %>%
  tab_header(
    title = "ICA Loadings",
    subtitle = "Loadings of each feature on the independent components"
  ) %>%
  cols_label(
    Feature = "Variable",
    `Component 1` = "Component 1",
    `Component 2` = "Component 2",
    `Component 3` = "Component 3"
  )

# Display the table
ica_loadings_table

```
