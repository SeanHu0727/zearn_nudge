---
title: "Discovering Data-Driven Nudges to Help Students Learn More Math"
abstract: ""
keywords: "Pedagogical Decision-Making, Digital Education Platforms, Empirical Field Data, Instructional Adaptation"
author:
  - name: Marcos Gallo
    orcid: 0000-0002-8227-2661
format: pdf
bibliography: references.bib

execute:
  echo: false
  warning: false
  error: false
---


```{r import}

library(data.table)
library(dtplyr)
library(tidyverse)
# library(zoo)
library(ggrepel)
library(gt)
library(Statamarkdown)
set.seed(794563797)
# Timestamp: 2023-12-21 21:42:25 UTC
# © 1998-2023 RANDOM.ORG

```

# Results

-   Data-driven Nudge Engineering with Habit-based Regression

-   Justification of model choices. There is no command for a fixed-effects model, because there does not exist a sufficient statistic allowing the fixed effects to be conditioned out of the likelihood.

-   Detailed steps in the regression analysis.

-   Explanation of fixed effects model.

-   Software and tools used in analysis.

-   Publishing Negative Data: Include any negative data that did not support hypotheses.

-   List of all statistical software and tools.

-   Versions and settings used.

-   Thorough Description of Methods: Clearly report experimental parameters.

# Materials and Methods

## Data

```{r preprocessing}
#| eval: false

teacher_usage <- read.csv("Data/raw/Teacher Usage - Time Series 2020-10-09T1635.csv")
classroom_student_usage <- read.csv("Data/raw/Classroom Student Usage - Time Series 2020-10-09T1616.csv")
classroom_info <- read.csv("Data/raw/Classroom Info 2020-10-09T1617.csv")
classroom_teacher_lookup <- read.csv("Data/raw/Classroom-Teacher Lookup 2020-10-09T1618.csv")
school_info <- read.csv("Data/raw/School Info 2020-10-09T1619.csv")
la_usage_types <- read_csv("Data/raw/la_usage_types.csv")

teacher_usage <- teacher_usage %>%
  mutate(
    # Noting week, month, and year to summarize teacher behavior:
    week = lubridate::isoweek(Usage.Time),
    year = lubridate::year(Usage.Time),
    # Transforming Event.Type for "Resource Downloaded"
    Event.Type = ifelse(Event.Type == "Resource Downloaded",
                        paste0("RD.", Curriculum.Resource.Category),
                        Event.Type)
    )

# Summarize teacher behavior
teacher_usage_total <- teacher_usage %>%
  count(Adult.User.ID, Event.Type, week, year) %>%
  pivot_wider(names_from = Event.Type, values_from = n,
              values_fill = list(n = 0)) %>%
  full_join(teacher_usage %>%
              group_by(Adult.User.ID, week, year) %>%
              summarize(Minutes.on.Zearn...Total = sum(Minutes.on.Zearn...Total)),
            by = c("Adult.User.ID", "week", "year"))

# Teacher exclusion
active_teachers <- teacher_usage_total %>%
  count(Adult.User.ID) %>%
  # Remove teachers active for less than 8 weeks
  filter(n >= 8) %>%
  pull(Adult.User.ID)

# Classroom exclusion
classroom_teacher_lookup <- classroom_teacher_lookup %>%
  group_by(Teacher.User.ID) %>%
  mutate(teacher_number_classes = n()) %>%
  ungroup() %>%
  group_by(Classroom.ID) %>%
  # Delete classrooms with multiple teachers
  filter(n_distinct(Teacher.User.ID) == 1) %>%
  ungroup() %>%
  inner_join(classroom_info %>%
               group_by(Classroom.ID) %>%
               # Delete classrooms with multiple schools
               filter(n_distinct(MDR.School.ID) == 1) %>%
               ungroup(), by = "Classroom.ID")

# Merge teacher and student data
# Merging with the classroom info for further merge with teacher info
teacher_student_usage_subset <- classroom_student_usage %>%
  inner_join(classroom_teacher_lookup %>%
               # Filtering active teachers
               filter(Teacher.User.ID %in% active_teachers),
             by = "Classroom.ID") %>%
  mutate(week = lubridate::isoweek(Usage.Week),
         year = lubridate::year(Usage.Week)) %>%
  full_join(teacher_usage_total %>%
              filter(Adult.User.ID %in% active_teachers),
             by = c("Teacher.User.ID" = "Adult.User.ID",
                    "week", "year")) %>%
  # Replace NAs with 0
  mutate(Badges.per.Active.User = ifelse(is.na(Badges.per.Active.User),
                                         0, Badges.per.Active.User))

# Process la_usage_types
la_usage_types <- la_usage_types %>%
  mutate(
    curriculum = as.integer(`Schools and Districts Usage Type` %in%
                              c("20-21 Curriculum", "Core Complement"))
    )

# Further data merging
teacher_student_usage_subset <- teacher_student_usage_subset %>%
  left_join(la_usage_types %>%
              mutate(`Schools and Districts MDR School ID` =
                       as.integer(`Schools and Districts MDR School ID`)),
            by = c("MDR.School.ID" = "Schools and Districts MDR School ID")) %>%
  select(!`Schools and Districts Usage Type`) %>%
  left_join(school_info %>% 
              select(MDR.School.ID, District.Rollup.ID,
                     Demographics...Zipcode.Median.Income,
                     Demographics...Poverty.Level,
                     Is.Charter..Yes...No.,
                     MDR.School.has.School.Account..Yes...No.,
                     School.Address...Zipcode),
            by = "MDR.School.ID") %>%
  rename(medianIncome = Demographics...Zipcode.Median.Income,
         povertyLevel = Demographics...Poverty.Level,
         charterSchool = Is.Charter..Yes...No.,
         schoolAccount = MDR.School.has.School.Account..Yes...No.,
         zipcode = School.Address...Zipcode) %>%
  mutate(charterSchool = case_when(charterSchool == "No" ~ 0,
                                   charterSchool == "Yes" ~ 1,
                                   .default = NA),
         schoolAccount = case_when(schoolAccount == "No" ~ 0,
                                   schoolAccount == "Yes" ~ 1,
                                   .default = NA),
         month = month(Usage.Week))

# Filtering active teacher-classrooms
active_classrooms <- teacher_student_usage_subset %>%
  # At least 5 students enrolled
  filter(Students...Total >= 5) %>%
  # Do not count June/July/August
  filter(!month %in% c(6, 7, 8),
         # Do not count inactive weeks
         !(is.na(Sessions.per.Active.User) & is.na(`User Session`))) %>%
  count(Classroom.ID) %>%
  # Remove teachers or classes active for at least 3 months
  filter(n >= 12) %>%
  pull(Classroom.ID)
teacher_student_usage_subset <- teacher_student_usage_subset %>%
  filter(Classroom.ID %in% active_classrooms)

average_student <- teacher_student_usage_subset %>%
  group_by(Classroom.ID) %>%
  summarise(Active.Users...Total = mean(Active.Users...Total)) %>%
  # Positive number of active students per week
  filter(Active.Users...Total > 1/3)
teacher_student_usage_subset <- teacher_student_usage_subset %>%
  filter(Classroom.ID %in% average_student$Classroom.ID)

teacher_student_usage_subset <- teacher_student_usage_subset %>%
  # Do not include inactive weeks
  filter(!(is.na(Sessions.per.Active.User) & is.na(`User Session`))) %>%
  mutate(
    District.Rollup.ID = as.character(District.Rollup.ID),
    curriculum = as.character(curriculum),
    Minutes.per.Active.User = as.numeric(Minutes.per.Active.User)
    ) %>%
  # Fill all NAs with 0 (except `curriculum`)
  mutate(across(where(is.numeric), ~ replace_na(., 0)))

# Write to csv
write.csv(teacher_student_usage_subset, "./Data/df.csv")

```

### Summary Statistics

```{r}
#| label: tbl-summary
#| tbl-cap: "Pre-experimental Intervention Data. This table summarizes key educational metrics for Zearn teachers. The data was collected from July 2019 to June 2020."

teacher_student_usage_subset <- read.csv(file = "./Data/df.csv")
teacher_usage <- read.csv("Data/raw/Teacher Usage - Time Series 2020-10-09T1635.csv")

# Pre-calculate unique teachers and classrooms
unique_teachers <- unique(teacher_student_usage_subset$Teacher.User.ID)
unique_classrooms <- unique(
  teacher_student_usage_subset[, c('Teacher.User.ID', 'Classroom.ID')]
  )

# Statistics for "# of classes"
n_teachers <- length(unique_teachers)
n_classes <- nrow(unique_classrooms)
mean_classes <- n_classes / n_teachers
classes_per_teacher <- unique_classrooms %>% count(Teacher.User.ID)

# Statistics for "# of badges"
teacher_student_usage_subset$Total.Badges <- 
  teacher_student_usage_subset$Active.Users...Total *
  teacher_student_usage_subset$Badges.per.Active.User
badges_per_teacher <- teacher_student_usage_subset %>% 
  group_by(Teacher.User.ID) %>% 
  summarize(Total.Badges = sum(Total.Badges))

# Statistics for "Total minutes on Zearn"
minutes_per_teacher <- teacher_usage %>%
  filter(!Adult.User.ID %in% unique_teachers) %>%
  group_by(Adult.User.ID) %>%
  summarize(Total.Minutes = sum(Minutes.on.Zearn...Total))

# Creating a function for common statistics
calculate_stats <- function(data, value) {
  list(
    N = sum(data[[value]]),
    Mean = mean(data[[value]]),
    SD = sd(data[[value]]),
    Quantiles = quantile(data[[value]], probs = c(0.25, 0.5, 0.75))
  )
}

# Calculating statistics
stats_classes <- calculate_stats(classes_per_teacher, "n")
stats_badges <- calculate_stats(badges_per_teacher, "Total.Badges")
stats_minutes <- calculate_stats(minutes_per_teacher, "Total.Minutes")

# Formatting the table with gt
gt_table <- tibble(
  Category = c("# of teachers", "# of classes", "# of badges", "Total minutes on Zearn"),
  N = c(n_teachers, stats_classes$N, stats_badges$N, stats_minutes$N),
  Mean = c(NA, stats_classes$Mean, stats_badges$Mean, stats_minutes$Mean),
  `Standard Deviation` = c(NA, stats_classes$SD, stats_badges$SD, stats_minutes$SD),
  `1st Quartile` = c(NA, stats_classes$Quantiles[1], stats_badges$Quantiles[1], stats_minutes$Quantiles[1]),
  `2nd Quartile` = c(NA, stats_classes$Quantiles[2], stats_badges$Quantiles[2], stats_minutes$Quantiles[2]),
  `3rd Quartile` = c(NA, stats_classes$Quantiles[3], stats_badges$Quantiles[3], stats_minutes$Quantiles[3])
) %>%
  gt() %>%
  tab_spanner(
    label = "Aggregated Statistics per Teacher",
    columns = c("Mean", "Standard Deviation", "1st Quartile", "2nd Quartile", "3rd Quartile")
  ) %>%
  cols_label(
    Category = "",
    N = "N",
    Mean = "Mean",
    `Standard Deviation` = "Standard Deviation",
    `1st Quartile` = "1st Quartile",
    `2nd Quartile` = "2nd Quartile",
    `3rd Quartile` = "3rd Quartile"
  ) %>%
  fmt_number(
    columns = c("N"),
    decimals = 0
  ) %>%
  fmt_number(
    columns = c("Mean", "Standard Deviation"),
    decimals = 2,
    use_seps = TRUE
  ) %>%
  fmt_number(
    columns = c("1st Quartile", "2nd Quartile", "3rd Quartile"),
    decimals = 1
  ) %>%
  sub_missing(
    columns = c("Mean", "Standard Deviation", "1st Quartile", "2nd Quartile", "3rd Quartile"),
    missing_text = "-"
  )

# Display the table
gt_table

```

```{r clean-environment}
#| include: false

rm(list = setdiff(ls(), c("teacher_student_usage_subset", "teacher_usage")))
gc(verbose = FALSE)

```

-   Specific teacher and student metrics collected.

<!-- SEAN: Help troubleshoot map -->

```{r}
#| eval: false
#| cache: true
#| label: fig-teachers-map
#| fig-cap: "Geographical distribution of teachers across various parishes in Louisiana, and the top 5 cities with the highest number of teachers."

library(sf)
library(tidygeocoder)
library(tigris)
library(furrr)
plan(strategy = "multisession", workers = availableCores())

# Batch geocoding
# Sys.setenv(GEOCODIO_API_KEY = "")
unique_zipcodes <- unique(teacher_student_usage_subset$zipcode) %>%
  as.list()
address_geodata <- furrr::future_map_dfr(.x = unique_zipcodes, 
                               ~ geo(postalcode = .x,
                                     country = "United States",
                                     method = 'geocodio',
                                     full_results = TRUE,
                                     progress_bar = FALSE)) %>%
  select(postalcode,
         address_components.city,
         address_components.county,
         lat, long) %>%
  rename(
    city = address_components.city,
    county = address_components.county
  ) %>%
  mutate(
    postalcode = as.integer(postalcode)
  )

# Merge the geocoding results back into the original data.table
n_teachers <- teacher_student_usage_subset %>%
  group_by(Classroom.ID, Teacher.User.ID, MDR.School.ID,
           District.Rollup.ID, zipcode) %>%
  summarize(Students...Total = mean(Students...Total), 
            .groups = "keep") %>%
  full_join(address_geodata, by = c("zipcode" = "postalcode"))

# Get the top 5 cities by number of teachers
# Aggregate the data to get the number of teachers in each city
top_cities <- n_teachers %>%
  group_by(city, county) %>%
  summarize(
    num_teachers = n_distinct(Teacher.User.ID),
    lat = mean(lat, na.rm = TRUE),
    long = mean(long, na.rm = TRUE),
    .groups = 'drop' # Optionally, drop grouping structure from the result
  ) %>%
  arrange(desc(num_teachers)) %>%
  slice_head(n = 5)

# Get the Louisiana county map data
df_map <- tigris::counties(class = "sf",
                           state = "LA",
                           progress_bar = FALSE) %>%
  # sf::st_set_crs(4326) %>%
  left_join(
    n_teachers %>%
      group_by(county) %>%
      summarize(num_teachers = n_distinct(Teacher.User.ID)),
    by = c("NAMELSAD" = "county")
    ) %>%
  mutate(num_teachers = ifelse(is.na(num_teachers),0,num_teachers)) %>%
  sf::st_as_sf()

ggplot() +
  geom_sf(data = df_map, aes(fill = num_teachers)) +
  scale_fill_continuous(name = "Number of Teachers",
                        low = "white", high = "red", na.value = "gray90") +
  labs(
    title = "Number of Teachers by Parish in Louisiana"
  ) +
  theme_minimal() +
  theme(
    legend.position = "bottom",
    panel.grid = element_blank(),
    axis.text = element_blank(),
    axis.title = element_blank()
  ) +
  geom_point(data = top_cities, aes(x = long, y = lat)) +
  geom_text_repel(data = top_cities, aes(x = long, y = lat, label = city),
                  size = 3, color = "black")

```

## Study 1

### Data Collection

The Zearn math educational learning platform provided us with administrative data spanning the 2019-2020 academic year (September 2019 to May 2020). This dataset included detailed teacher actions and classroom-week-level student achievement metrics. Teacher actions on the platform were timestamped to the second. For privacy considerations, Zearn aggregated student data at the classroom-week level, including student achievement measures and indications of student struggles. The dataset covered various schools across Louisiana (@fig-teachers-map).

To promote transparency and replicability of our study, we have deposited the data and code used in our analyses in a publicly accessible database. Researchers and interested parties can access the complete dataset and all related processing scripts at the GitHub repository: <https://github.com/SeanHu0727/zearn_nudge.git>

### Inclusion Criteria

We aggregated teacher behavior data to the weekly level and merged it with the student data at the classroom-week level. We also excluded inactive teachers (those with no recorded activity for over two months) from the dataset.

We defined the inclusion criteria for classrooms strictly as such:

1.  Classrooms linked to a single teacher

2.  Classrooms with no more than seven months of inactivity during the academic year

3.  Classrooms with an average of no less than five actively engaged students

## Study 1a

### Independent Component Analysis (ICA)

We extracted all teacher behavioral variables from the dataset that displayed non-zero variance and standardized them to have a zero mean and unit variance. To determine the ideal number of independent components, we performed ICA using a range of components from 1 to 10. Our decision on the optimal number was informed by recognizing the 'elbow' on the scree plot generated from the results, yielding three independent components. We used the `icafast` function from the R `ica` package for all ICAs conducted [@helwig2022].

```{r ICA}

library(ica)

# Perform ICA to choose number of components
# Removing columns with zero variance
ICA_cols <- teacher_student_usage_subset %>%
  select(RD.elementary_schedule:RD.foundational_lesson_guidance) %>%
  select(where(~ sd(.x, na.rm = TRUE) != 0)) %>%
  mutate_all(scale)

imod.fast <- ica(ICA_cols, nc = 10)

# Create Elbow Plot
## SEAN: Fix plot to show integers in x-axis and % in y-axis
elbow_plot <- ggplot(data.frame(Component = 1:10, Variance = imod.fast$vafs),
                     aes(x = Component, y = Variance)) +
  geom_line(color = "gray") +
  geom_point() +
  labs(x = "Component", y = "Proportion of Variance",
       title = "Scree Plot for Determining Optimal k") +
  theme_minimal()


# ICA via different algorithms

## SEAN: Check for the 3 methods:
## 1) http://research.ics.aalto.fi/ica/icasso/
## 2) https://www.cs.helsinki.fi/u/ahyvarin/code/isctest/
## We will choose the best method accordingly (most stable and interpretable)

imod.fast <- ica(ICA_cols, nc = 3)
# imod.imax <- ica(ICA_cols, nc = 3, method = "imax")
# imod.jade <- ica(ICA_cols, nc = 3, method = "jade")


# Iterate through each principal component
for (i in 1:ncol(imod.fast$M)) {
    # Find the index of the variable with the largest absolute coefficient
    largest_coef_index <- which.max(abs(imod.fast$M[, i]))

    # Check if the largest coefficient is negative
    if (imod.fast$M[largest_coef_index, i] < 0) {
        # Multiply all coefficients in this principal component by -1
        imod.fast$M[, i] <- -1 * imod.fast$M[, i]
        imod.fast$S[, i] <- -1 * imod.fast$S[, i]
    }
}

# Add ICA components to the dataset
teacher_student_usage_subset <- teacher_student_usage_subset %>%
  mutate(
    ic1 = imod.fast$S[, 1],
    ic2 = imod.fast$S[, 2],
    ic3 = imod.fast$S[, 3]
  )

```

### Censored Panel Regression

We estimated a censored panel regression model using a first difference approach, with the `pldv` function from the `plm` package [@croissant2008] in R and a lower bound of 0 for the dependent variable:

$$
\Delta \ln (\text{Badges}_{itc} + 1) = \beta_0 + \beta_1 \Delta \text{(IC1)}_{it} + \beta_2 \Delta \text{(IC2)}_{it} + \beta_3 \Delta \text{(IC3)}_{it} + \Delta \epsilon_{itc}
$$

where $i, c, t$ index the teacher, class, and week, respectively. Standard errors are clustered at the classroom level with the `vcovHC` function [@millo2017] in R..

```{r}
library(plm)

# Define the columns to be used in the formula
columns_used <- colnames(teacher_student_usage_subset %>%
                           select(teacher_number_classes, Grade.Level, MDR.School.ID, 
                                  Students...Total, 
                                  # medianIncome:schoolAccount,
                                  ic1:ic3))
# Create the formula
fmla_str <- sprintf("logBadges ~ %s",
                    paste(sprintf("`%s`", columns_used), collapse = " + "))
fmla <- as.formula(fmla_str)

# Define the panel model index
pUsage <- pdata.frame(
  teacher_student_usage_subset %>%
    arrange(Classroom.ID, year, week) %>%
    mutate(Classroom.ID = factor(Classroom.ID, ordered = FALSE),
           Teacher.User.ID = factor(Teacher.User.ID, ordered = FALSE),
           MDR.School.ID = factor(MDR.School.ID, ordered = FALSE),
           District.Rollup.ID = factor(District.Rollup.ID, ordered = FALSE),
           Grade.Level = factor(Grade.Level, ordered = TRUE),
           medianIncome = factor(medianIncome, ordered = TRUE),
           povertyLevel = factor(povertyLevel, ordered = TRUE),
           zipcode = factor(zipcode, ordered = FALSE)) %>%
    group_by(Teacher.User.ID) %>%
    mutate(week = week - lubridate::isoweek(min(Usage.Week)) + 1) %>%
    mutate(week = ifelse(week < 1 | year == 2020,
                         week + 52 * (2020 - lubridate::year(min(Usage.Week))),
                         week)) %>%
    ungroup() %>%
    mutate(logBadges = log(Badges.per.Active.User + 1)),
  index = c("Classroom.ID", "week", "Teacher.User.ID"))

# Create and summarize the 'within' model directly
## SEAN: please compare:
## 1) RE vs FD: "There is no command for a fixed-effects model, because there does not exist a sufficient statistic allowing the fixed effects to be conditioned out of the likelihood.
## 2) with objfun = "lad" (maybe more robust to outliers?)
ica_fd_model <- pldv(fmla, data = pUsage, model = "fd", objfun = "lsq",
                         lower = 0, sample = "cens")
# ica_fe_model <- censReg::censReg(fmla, data = pUsage)
summary(ica_fd_model, vcov = vcovHC(ica_fd_model, type = "HC3", cluster = "group"))

# Hausman Test supports Fixed Effects
# ica_re_model <- pldv(fmla, data = pUsage, model = "random", objfun = "lsq",
#                      lower = 0, sample = "cens")
# phtest(ica_fd_model, ica_re_model)
# 
# data:  fmla
# chisq = 385.17, df = 3, p-value < 2.2e-16
# alternative hypothesis: one model is inconsistent

```

## Study 1b

### Fixed-Effects Regression

We estimated a fixed effects model using the `plm` function[@croissant2008] in R:

$$

\begin{align*}
\ln \left( \sum_{c=1}^{C_i} \text{(Avg. Badges)}_{c} + 1 \right) = & \ \beta_0 + \sum_{m=1, m \neq 7}^{12} \beta_m \text{LoginRatio}_{m,i} + \sum_{d=1, d \neq 7}^{7} \beta_{12+d} \text{LoginRatio}_{d,i} \\
& + \beta_{19} \text{(Avg. Minutes)}_i + \beta_{20} \text{(Avg. Streak)}_i \\
& + \beta_{21} \text{(Avg. Days Between Logins)}_i + \text{School FEs} + \epsilon_i
\end{align*}

$$

for teacher $i$ with $C_i$ classes. `LoginRatio` represents the ratio of logins by teacher $i$ during each month $m$ and on each day of the week $d$ relative to other months and days, respectively. We exclude July and Sunday to prevent multicollinearity. Standard errors are clustered by school with the `vcovHC` function [@millo2017] in R.

```{r preprocessing-habits}
#| eval: false

# Prepare data
setDT(teacher_usage)
teacher_usage[, Date := as.Date(Usage.Time)]

# Sequence of dates for the overall grid expand
date_seq <- seq.Date(min(teacher_usage$Date),
                     max(teacher_usage$Date),
                     by = "day")

# Logged indicator
teacher_usage[, logged := 1]

# Create combinations of Adult.User.ID and Date, and merge with logged data
df <- CJ(Adult.User.ID = unique(teacher_usage$Adult.User.ID), Date = date_seq)
df <- df[teacher_usage, on = .(Adult.User.ID, Date),
         `:=`(logged = i.logged,
              Minutes.on.Zearn...Total = i.Minutes.on.Zearn...Total)]
df[is.na(logged), logged := 0]
df[is.na(Minutes.on.Zearn...Total), Minutes.on.Zearn...Total := 0]

# Code datetime variables and compute additional metrics
setorder(df, Adult.User.ID, Date)
df[, `:=`(
  Year = year(Date),
  Month = month(Date),
  Week = isoweek(Date),
  dayofweek = wday(Date, label = TRUE, week_start = 1),
  prev_visits = cumsum(logged) - logged
  ), by = Adult.User.ID]
df[, `:=`(
  prev_dow_visits = cumsum(logged) - logged
  ), by = .(Adult.User.ID, dayofweek)]

# Streak, time lag, and other computations
df[, `:=`(
  lag.logged = shift(logged, 1, type = "lag", fill = 0)
  ), by = Adult.User.ID]
df[, start_streak:=as.integer(logged == 1 & lag.logged == 0)]
df[, start_streak:=cumsum(start_streak), by = Adult.User.ID]
df[lag.logged==0, streak:=as.double(0)]
df[is.na(streak), streak:=as.double(1:.N), by = .(Adult.User.ID, start_streak)]

# Time lag calculation
df[, `:=`(
  start_streak_lag = shift(start_streak, 1, fill = 0)
), by = Adult.User.ID]

# Streak calculation by day of week
df[,attended_lag_dow:=shift(logged, 1, fill = 0),
   by = .(Adult.User.ID, dayofweek)]
df[, `:=`(
  streak_dow = fcase(
    attended_lag_dow == 0, as.double(0),
    default = as.double(NA)
  ),
  start_streak_dow = as.numeric(logged == 1 & attended_lag_dow == 0)
  ), by = .(Adult.User.ID, dayofweek)]
df[, start_streak_dow := cumsum(start_streak_dow),
   by = .(Adult.User.ID, dayofweek)]
df[is.na(streak_dow),
   streak_dow := 1:.N,
   by = .(Adult.User.ID, start_streak_dow, dayofweek)]

# Previous week visits calculations
## Calculate weekly visits
weekly_logged <- df[, .(week_visits = sum(logged)),
                    by = .(Adult.User.ID, Week, Year)]
## Calculate previous week visits
setorder(weekly_logged, Adult.User.ID, Year, Week)
weekly_logged[, prev_week_visits := shift(week_visits, 1, fill = 0),
               by = Adult.User.ID]
df <- merge(df, weekly_logged,
            by = c("Adult.User.ID", "Week", "Year"), all.x = TRUE)
setorder(df, Adult.User.ID, Date)

# Calculate previous day of week visit
df[, prev_dow_visit := shift(logged, 1, fill = 0),
   by = .(Adult.User.ID, dayofweek)]
# Previous minutes spent
df[, prev_min_zearn := shift(Minutes.on.Zearn...Total, 1, fill = 0),
   by = Adult.User.ID]

# Add a time column, a sequence for each Adult.User.ID
df[, time := seq_len(.N), by = Adult.User.ID]
# Calculate the first time point for each Adult.User.ID where logged == 1
first_time_index <- df[logged == 1,
                       .(first_time = min(time)),
                       by = Adult.User.ID]
df[first_time_index,
   on = .(Adult.User.ID),
   `:=`(first_time = i.first_time)]
df[, first_time := time - first_time]
# Calculate the last time point for each Adult.User.ID where logged == 1
last_time_index <- df[logged == 1,
                      .(last_time = max(time)),
                      by = Adult.User.ID]
df[last_time_index,
   on = .(Adult.User.ID),
   `:=`(last_time = i.last_time)]
df[, last_time := last_time - time]
# Filter out rows outside the first and last logged==1
df <- df[first_time >= 0 & last_time >= 0]

df[, `:=`(
  first_week = isoweek(min(Date)),
  first_year = year(min(Date)),
  time = 1:.N
  ), by = Adult.User.ID]
df[, Week := Week - first_week + 1]
df[, Week := fifelse(Week < 1 | Year == 2020,
                     Week + 52 * (2020 - first_year),
                     Week), by = Adult.User.ID]

df[, `:=`(
  mean_visits = mean(logged),
  prev_freq = prev_visits / time,
  prev_dow_freq = prev_dow_visits / Week
  ), by = Adult.User.ID]
  
df[lag.logged==1, time_lag:=1]
df[lag.logged==0, time_lag:=as.double(1:.N)+1,
   by = .(Adult.User.ID, start_streak_lag)]
df[time == 1, time_lag:=0, by = Adult.User.ID]

# Finalize the dataset
# Cleanup: Remove temporary columns
df[, c("first_time", "last_time", "first_week", "first_year") := NULL]
setorder(df, Adult.User.ID, Date)
# Write to csv
fwrite(df, "./Data/df_habits.csv")

```

<!-- SEAN: Let's try a Panel model here as well. We might need to use LASSO because we have so many variables (we also want to include interactions of weekdays with streak_dow). Panel is what we did for the previous analysis, but it was a bit more complicated with the habitization part. We can just do one mixed-effects LASSO and see if the results coincide with the analysis below and with the previous LASSO analysis. -->

```{r habits}

# Read in the data
df <- fread(file = "./Data/df_habits.csv") %>%
  lazy_dt()

df_transformed <- df %>%
  filter(Adult.User.ID %in%
           unique(teacher_student_usage_subset$Teacher.User.ID)) %>%
  mutate(dayofweek = factor(dayofweek, ordered = FALSE,
                            levels = c("Mon", "Tue", "Wed", "Thu",
                                       "Fri", "Sat", "Sun")),
         Month = factor(Month, ordered = FALSE),
         week_label = isoweek(Date)) %>%
  pivot_wider(names_from = dayofweek, values_from = logged, 
              values_fill = list(logged = 0)) %>%
  group_by(Adult.User.ID, Week) %>%
  summarize(across(c(Mon:Sun), sum, na.rm = TRUE),
            Total_Minutes_on_Zearn = sum(Minutes.on.Zearn...Total, na.rm = TRUE),
            mean_streak = mean(streak, na.rm = TRUE),
            mean_streak_dow = mean(streak_dow, na.rm = TRUE),
            mean_visits = mean(mean_visits, na.rm = TRUE),
            mean_time_lag = mean(time_lag, na.rm = TRUE),
            Month = first(Month),
            Year = first(Year),
            week_label = first(week_label),
            .groups = 'drop')

# Pivot longer to aggregate by month
df_monthly <- df_transformed %>%
  pivot_longer(cols = Mon:Sun, names_to = "dayofweek", values_to = "logged") %>%
  group_by(Adult.User.ID, Month) %>%
  summarize(total_logged_per_month = as.double(sum(logged, na.rm = TRUE))) %>%
  # Pivot wider to create a column for each month
  pivot_wider(names_from = Month, values_from = total_logged_per_month,
              names_prefix = "month_",
              values_fill = list(total_logged_per_month = 0)) %>%
  mutate(row_total = rowSums(select(., starts_with("month_")))) %>%
  mutate(across(starts_with("month_"), ~ ./row_total)) %>%
  select(-row_total)

pUsage <- pdata.frame(
  df_transformed %>%
    group_by(Adult.User.ID) %>%
    summarize(across(c(Mon:Sun), sum, na.rm = TRUE),
              across(c(Total_Minutes_on_Zearn:mean_time_lag), mean, na.rm = TRUE),
              .groups = 'drop') %>%
    mutate(row_total = rowSums(select(., c(Mon:Sun)))) %>%
    mutate(across(c(Mon:Sun), ~ ./row_total)) %>%
    select(-row_total) %>%
    inner_join(df_monthly, by = "Adult.User.ID") %>%
    inner_join(teacher_student_usage_subset %>%
                 lazy_dt() %>%
                 group_by(Teacher.User.ID, week, MDR.School.ID) %>%
                 summarize(Badges.per.Active.User = sum(Badges.per.Active.User),
                           Year = first(year),
                           .groups = 'drop') %>%
                 group_by(Teacher.User.ID, MDR.School.ID) %>%
                 summarize(Badges.per.Active.User = mean(Badges.per.Active.User),
                           .groups = 'drop'),
               by = c("Adult.User.ID" = "Teacher.User.ID")
               # by = c("Adult.User.ID" = "Teacher.User.ID",
               #        "week_label" = "week", "Year")
               ) %>%
    mutate(Adult.User.ID = factor(Adult.User.ID, ordered = FALSE),
           MDR.School.ID = factor(MDR.School.ID, ordered = FALSE),
           logBadges = log(Badges.per.Active.User + 1)) %>%
    as_tibble(),
  index = c("MDR.School.ID"))
  # index = c("Adult.User.ID", "Week", "MDR.School.ID"))

habits_fe_model <- plm(logBadges ~ Mon + Tue + Wed + Thu + Fri + Sat +
                     month_1 + month_2 + month_3 + month_4 + month_5 + month_6 +
                     month_8 + month_9 + month_10 + month_11 + month_12 +
                     Total_Minutes_on_Zearn + mean_streak + mean_time_lag,
                   data = pUsage, model = "within")
# habits_fd_model <- pldv(logBadges ~ Mon + Tue + Wed + Thu + Fri + Sat + Sun + Month +
#                      Total_Minutes_on_Zearn + mean_streak + mean_streak_dow +
#                      mean_visits + mean_time_lag,
#                     data = pUsage, model = "fd", objfun = "lsq",
#                     lower = 0, sample = "cens")

summary(habits_fe_model,vcov = vcovHC(habits_fe_model, type = "HC3", cluster = "group"))

# Hausman Test supports Fixed Effects
# habits_re_model <- plm(logBadges ~ Mon + Tue + Wed + Thu + Fri + Sat +
#                      month_1 + month_2 + month_3 + month_4 + month_5 + month_6 +
#                      month_8 + month_9 + month_10 + month_11 + month_12 +
#                      Total_Minutes_on_Zearn + mean_streak + mean_streak_dow +
#                      mean_visits + mean_time_lag,
#                    data = pUsage, model = "random", effect = "individual")
# phtest(habits_fe_model, habits_re_model)
# 
# data:  logBadges ~ Mon + Tue + Wed + Thu + Fri + Sat + month_1 + month_2 +  ...
# chisq = 227.82, df = 22, p-value < 2.2e-16
# alternative hypothesis: one model is inconsistent

```

## Study 2

We used the findings from Study 1 to inform the creation of two interventions as part of a larger multi-arm "mega study" that involved 15 sets of nudges.

### Implementation

Study 2 was conducted in collaboration with Zearn [@zearnma] and was preregistered for the fall of 2021. To incentivize teacher participation during our "intervention period" from September 15, 2021, to October 12, 2021, all teachers on the platform received two messages on September 1 and 8, 2021. These messages informed them they had been enrolled in the "Zearn Math Giveaway" and that every email opened until October 12, 2021, would earn them tickets. These tickets were used to enter drawings for various prizes, such as autographed children's books, stickers, and gift cards.

### Data

We excluded Zearn elementary school teachers from the study if they (a) taught grades other than first through eighth ($n=X$), (b) lacked a valid email address in the Zearn system as of September 8, 2021 ($n=X$), (c) had less than one or more than 150 students associated with their Zearn account as of October 18, 2021 ($n=X$), (d) had fewer than one or more than six classrooms associated with their Zearn account as of DATE ($n=X$), (e) had other teachers associated with their Zearn classroom(s) as of DATE ($n=X$), or (f) had not logged onto the Zearn platform (or had no associated student who logged onto the platform) between March 1, 2021 and September 14, 2021 ($n=X$).

After exclusions, we randomized $N = 140,461$ teachers across 22,281 schools who served 2,992,077 students in 161,722 classrooms into one of the intervention conditions or the control condition ($N_{\text{control}} = 29,513$). The control condition was larger than the interventions to account for multiple comparisons. Among $n = 16,372$, or $11.66\%$, of teachers, at least one of two problems occurred in the emails sent by Zearn Math during the intervention period: an email message that was intended but not sent ($n = 13,568$, or $9.66\%$ of teachers), or an email message that was sent but not intended (i.e., from a different treatment condition; $n = 2,804$, or $2.00\%$ of teachers). As these email problems were systematically related to treatment assignment ($\chi^2 = 33.01$, $df = 15$, $p = .005$), we did not exclude these participants and conducted intent-to-treat analyses. Refer to the SI for email problem prevalence by condition and study analyses that exclude or adjust for email problems, respectively.

### Impact Assessment

We followed our preregistered analysis plan to assess the effect of each treatment on the primary outcome of interest: math lessons completed by students during our four-week intervention period [@gallo2022a; @gallo2022b]. We estimated a weighted ordinary least squares (OLS) regression with the `areg` command in Stata [@statacorp2023]. Each teacher’s observations were weighted proportionally to the total number of students in their Zearn classroom(s).

The primary predictors were indicators for each intervention, omitting the control condition. The regression also included the following control variables: (1) school fixed effects, (2) an indicator for the teacher's account type (free or paid), (3) the number of times the teacher logged into Zearn prior to the study, from August 1 to September 14, 2021, (4) the total number of students in the teacher's classroom(s) as of October 18, 2021, (5) the number of classrooms associated with the teacher as of October 18, 2021, (6) the number of days since the teacher obtained a Zearn account prior to the study's launch, (7) the number of days separating the study's launch and the start of the teacher's school year, (8) the average number of lessons completed by a teacher's students from the start of their school year to the start of the intervention (or from July 14, 2021, if the school year start was not known), (9) whether the teacher opened our September 1, 2021 email announcing the upcoming Zearn Math Giveaway, (10) a similar indicator for our September 8, 2021, email reminding them of the giveaway, and (11) the percentage of a teacher's students in each grade except for third grade to avoid multicollinearity, since for most teachers, students were in a single grade.

### Study 2a: Empathy-Based Intervention Design and Implementation

We designed four emails highlighting empathy and encouraging teacher engagement, specifically regarding pedagogical content knowledge. A group of 7,443 teachers were chosen at random to receive these emails.

### Study 2b: Habitization-Based Intervention Design and Implementation

We designed four emails to be sent on Fridays, emphasizing the importance of regular Friday logins and student progress tracking. A total of 7,476 teachers were randomly selected to receive these emails. Additionally, we created a condition-specific control group that received messages on Wednesdays without Friday-specific content but with links to specific actions on Zearn. A group of 7,577 teachers were randomly chosen to participate in this control group.

```{stata}

*********************************************************
* Adapted from:                       							    *
* Title: Zearn Main and Per Protocol Analyses				    *
* Author: Ramon Silvera Zumaran		      						    *
* Date: 13 Jul 2023										   	              *
*********************************************************

clear all
set more off
set type double
* ssc install blindschemes
set scheme plotplain

*************************
**# Defining Programs #**
*************************

* Data Transformation and BH Adjustment Program
/* This program performs a Benjamini-Hochberg Adjustment for multiple hypothesis
   testing and summarizes the original results alongside the adjusted p-values */

cap drop bhadjustment

program define bhadjustment
	args cond_num
	
	matrix results = r(table)
	local colnames : colfullnames results
	local colnum : colsof results
	local cond_num = `cond_num'
	
	gen category_num = .
	gen category = ""
	gen coef = .
	gen standard_error = .
	gen p_val = .
	gen p_val_BH = .
	
	forvalues j = 1/`cond_num' {
		levelsof condition_cat if condition == `j', local(level) clean
		replace category_num = `j' in `j'
		replace category = "`level'" in `j'
		replace coef = results[1,`j'] in `j'
		replace standard_error = results[2,`j'] in `j'
		replace p_val = results[4,`j'] in `j'
	}
	
	keep category_num-p_val
	
	drop if mi(category)
	
	sort category_num
	drop category_num
	
	sort p_val
	gen rank = _n

	gen p_val_BH = p_val*(`cond_num'/rank)

	forvalues i = 1/`cond_num' {
		local num = `cond_num'+1
		local ind = `num'-`i'
		if p_val_BH[`ind'+1] < p_val_BH[`ind'] {
			replace p_val_BH = p_val_BH[`ind'+1] in `ind'
		}
	}
	
	replace p_val_BH = round(p_val_BH, 0.001)

	drop rank
	
	list

end

***************************************
**# Defining globals for covariates #**
***************************************
/* These are our pre-registered covariates for our primary DV regressions. */

* Main Analysis: Badges
global badges_covs perc_G1 perc_G2 perc_G4 perc_G5 perc_G6 perc_G7 perc_G8 is_free teacher_badges_pre pre_intv_logins total_associated_students n_classes acct_age missing_classroom_grade opened_weekneg1 opened_week0

********************************************
**# Primary and Secondary DV Regressions #**
********************************************

* Student Math Performance
use "Data/megastudy_clean.dta", clear
// Pre-registered regression
areg teacher_badges_during ib16.condition $badges_covs [aweight=total_associated_students], absorb(combined_school_id)
// Wald test for checking whether friday and weekly treatment are postive 
test 7.condition 8.condition
test 8.condition-7.condition=0
local sign_car = sign(_b[8.condition]-_b[7.condition])
display "H_1: Wednesday >= Friday p-value = " normal(`sign_car'*sqrt(r(F)))

```

## Ethical Considerations and IRB Approval

This study was conducted in accordance with ethical standards and received exempt status from the Institutional Review Board (IRB) at the University of Pennsylvania. The study's methodologies were designed to ensure the confidentiality and anonymity of all participants involved, adhering strictly to ethical guidelines for educational research. Data received from Zearn was aggregated at the classroom-level, with no identifying information about teachers or students.

# Supplementary Information

## Supplementary Methods

### Independent Component Analysis (ICA)

We implement the FastICA algorithm [@hyvarinen2000] to estimate independent components from our dataset. In this model, matrix $X = {x_{ij}}_{I \times J}$ , consisting of $I$ samples across $J$ random variables, is expressed as a linear mixture of independent components $C$, represented by:

$$
X = C' M + E.
$$

Here, $C$ holds the independent components, $M$ is a mixing matrix, and $E$ denotes the noise. The aim is to minimize mutual information between components in $C$, which is achieved by maximizing their marginal negentropy, thereby rendering the columns of $C$ statistically independent.

The FastICA process begins by transforming $X$ into a whitened matrix $Y$, ensuring uncorrelated variables with unit variance. This transformation is achieved through eigenvalue decomposition:

$$
Y = X \times \begin{bmatrix} \frac{\mathbf{v_1}}{\sqrt{\lambda_1}} & \frac{\mathbf{v_2}}{\sqrt{\lambda_2}} & \frac{\mathbf{v_3}}{\sqrt{\lambda_3}} \end{bmatrix},
$$

where ($\lambda_1, \lambda_2, \lambda_3$) and ($\mathbf{v_1}, \mathbf{v_2}, \mathbf{v_3}$) are, respectively, the eigenvalues and eigenvectors of $\frac{X'X}{I}$.

Afterward, the algorithm approximates the negentropy with

$$
\hat{\theta}(c_n) = (\mathbb{E}[\ln(\cosh(c_n))] − \mathbb{E}[\ln(\cosh(z))])^2
$$

where $c_n, n \in {1, 2, 3}$, is one of the components, and $z$ is a Gaussian variable with zero mean and unit variance. FastICA iteratively maximizes this value across all components, producing an orthogonal rotation matrix $R_{3 \times 3}$ such that $C = YR$.

For more details on ICA and the FastICA algorithm, see [@hyvarinen2000; @helwig2013].

### Censored Panel Regression

We can't really use fixed effects... [@honore1992].

### Fixed-Effects Regression

Hausman Test

## Supplementary Tables

## Supplementary Discussion

### Robustnes Checks

```{r PCA}
#| eval: false

pca_result <- prcomp(PCA_cols, scale. = TRUE)
prop_variance <- summary(pca_result)$importance[2, 1:10]

# Add to dataset
teacher_student_usage_subset <- teacher_student_usage_subset %>%
  mutate(
    pc1 = pca_result$x[, 1],
    pc2 = pca_result$x[, 2],
    pc3 = pca_result$x[, 3]
  )


## Robustness
# Exclude User.Session
ICA_cols <- teacher_student_usage_subset %>%
  select(RD.elementary_schedule:RD.foundational_lesson_guidance) %>%
  select(where(~ sd(.x, na.rm = TRUE) != 0)) %>%
  mutate_all(scale)

imod.fast2 <- ica(ICA_cols, nc = 3)

# Variance explained
imod.fast2

# Creating a data frame for the ICA loadings
ica_loadings_df <- as.data.frame(t(ica_result$A))
colnames(ica_loadings_df) <- paste("Component", 1:ncol(ica_loadings_df))
ica_loadings_df$Feature <- colnames(teacher_student_usage_subset[13:39])[ICA_cols]

# Creating the table with gt
ica_loadings_table <- ica_loadings_df %>%
  relocate(Feature) %>%
  gt() %>%
  tab_header(
    title = "ICA Loadings",
    subtitle = "Loadings of each feature on the independent components"
  ) %>%
  cols_label(
    Feature = "Variable",
    `Component 1` = "Component 1",
    `Component 2` = "Component 2",
    `Component 3` = "Component 3"
  )

# Display the table
ica_loadings_table

```

-   Additional Analyses: Detailed list of supplementary analyses to be conducted.

## Supplementary Equations

## Supplementary Notes

<!-- (including notes clarifying statistical analyses, acknowledgements, grant or other numbers) -->
