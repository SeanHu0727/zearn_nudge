---
title: "zearn_nudge"
format: pdf
bibliography: references.bib
---

```{r import}
library(data.table)
library(dtplyr)
library(tidyverse)
# library(zoo)
library(ggrepel)
library(gt)
set.seed(794563797)
# Timestamp: 2023-12-21 21:42:25 UTC
# © 1998-2023 RANDOM.ORG

```

# Data

```{r preprocessing}
#| eval: false

teacher_usage <- read.csv("Data/raw/Teacher Usage - Time Series 2020-10-09T1635.csv")
classroom_student_usage <- read.csv("Data/raw/Classroom Student Usage - Time Series 2020-10-09T1616.csv")
classroom_info <- read.csv("Data/raw/Classroom Info 2020-10-09T1617.csv")
classroom_teacher_lookup <- read.csv("Data/raw/Classroom-Teacher Lookup 2020-10-09T1618.csv")
school_info <- read.csv("Data/raw/School Info 2020-10-09T1619.csv")
la_usage_types <- read_csv("Data/raw/la_usage_types.csv")

teacher_usage <- teacher_usage %>%
  mutate(
    # Noting week, month, and year to summarize teacher behavior:
    week = lubridate::isoweek(Usage.Time),
    year = lubridate::year(Usage.Time),
    # Transforming Event.Type for "Resource Downloaded"
    Event.Type = ifelse(Event.Type == "Resource Downloaded",
                        paste0("RD.", Curriculum.Resource.Category),
                        Event.Type)
    )

# Summarize teacher behavior
teacher_usage_total <- teacher_usage %>%
  count(Adult.User.ID, Event.Type, week, year) %>%
  pivot_wider(names_from = Event.Type, values_from = n,
              values_fill = list(n = 0)) %>%
  full_join(teacher_usage %>%
              group_by(Adult.User.ID, week, year) %>%
              summarize(Minutes.on.Zearn...Total = sum(Minutes.on.Zearn...Total)),
            by = c("Adult.User.ID", "week", "year"))

# Teacher exclusion
active_teachers <- teacher_usage_total %>%
  count(Adult.User.ID) %>%
  # Remove teachers active for less than 8 weeks
  filter(n >= 8) %>%
  pull(Adult.User.ID)

# Classroom exclusion
classroom_teacher_lookup <- classroom_teacher_lookup %>%
  group_by(Teacher.User.ID) %>%
  mutate(teacher_number_classes = n()) %>%
  ungroup() %>%
  group_by(Classroom.ID) %>%
  # Delete classrooms with multiple teachers
  filter(n_distinct(Teacher.User.ID) == 1) %>%
  ungroup() %>%
  inner_join(classroom_info %>%
               group_by(Classroom.ID) %>%
               # Delete classrooms with multiple schools
               filter(n_distinct(MDR.School.ID) == 1) %>%
               ungroup(), by = "Classroom.ID")

# Merge teacher and student data
# Merging with the classroom info for further merge with teacher info
teacher_student_usage_subset <- classroom_student_usage %>%
  inner_join(classroom_teacher_lookup %>%
               # Filtering active teachers
               filter(Teacher.User.ID %in% active_teachers),
             by = "Classroom.ID") %>%
  mutate(week = lubridate::isoweek(Usage.Week),
         year = lubridate::year(Usage.Week)) %>%
  full_join(teacher_usage_total %>%
              filter(Adult.User.ID %in% active_teachers),
             by = c("Teacher.User.ID" = "Adult.User.ID",
                    "week", "year")) %>%
  # Replace NAs with 0
  mutate(Badges.per.Active.User = ifelse(is.na(Badges.per.Active.User),
                                         0, Badges.per.Active.User))

# Process la_usage_types
la_usage_types <- la_usage_types %>%
  mutate(
    curriculum = as.integer(`Schools and Districts Usage Type` %in%
                              c("20-21 Curriculum", "Core Complement"))
    )

# Further data merging
teacher_student_usage_subset <- teacher_student_usage_subset %>%
  left_join(la_usage_types %>%
              mutate(`Schools and Districts MDR School ID` =
                       as.integer(`Schools and Districts MDR School ID`)),
            by = c("MDR.School.ID" = "Schools and Districts MDR School ID")) %>%
  select(!`Schools and Districts Usage Type`) %>%
  left_join(school_info %>% 
              select(MDR.School.ID, District.Rollup.ID,
                     Demographics...Zipcode.Median.Income,
                     Demographics...Poverty.Level,
                     Is.Charter..Yes...No.,
                     MDR.School.has.School.Account..Yes...No.,
                     School.Address...Zipcode),
            by = "MDR.School.ID") %>%
  rename(medianIncome = Demographics...Zipcode.Median.Income,
         povertyLevel = Demographics...Poverty.Level,
         charterSchool = Is.Charter..Yes...No.,
         schoolAccount = MDR.School.has.School.Account..Yes...No.,
         zipcode = School.Address...Zipcode) %>%
  mutate(charterSchool = case_when(charterSchool == "No" ~ 0,
                                   charterSchool == "Yes" ~ 1,
                                   .default = NA),
         schoolAccount = case_when(schoolAccount == "No" ~ 0,
                                   schoolAccount == "Yes" ~ 1,
                                   .default = NA),
         month = month(Usage.Week))

# Filtering active teacher-classrooms
active_classrooms <- teacher_student_usage_subset %>%
  # At least 5 students enrolled
  filter(Students...Total >= 5) %>%
  # Do not count June/July/August
  filter(!month %in% c(6, 7, 8),
         # Do not count inactive weeks
         !(is.na(Sessions.per.Active.User) & is.na(`User Session`))) %>%
  count(Classroom.ID) %>%
  # Remove teachers or classes active for at least 3 months
  filter(n >= 12) %>%
  pull(Classroom.ID)
teacher_student_usage_subset <- teacher_student_usage_subset %>%
  filter(Classroom.ID %in% active_classrooms)

average_student <- teacher_student_usage_subset %>%
  group_by(Classroom.ID) %>%
  summarise(Active.Users...Total = mean(Active.Users...Total)) %>%
  # Positive number of active students per week
  filter(Active.Users...Total > 1/3)
teacher_student_usage_subset <- teacher_student_usage_subset %>%
  filter(Classroom.ID %in% average_student$Classroom.ID)

teacher_student_usage_subset <- teacher_student_usage_subset %>%
  # Do not include inactive weeks
  filter(!(is.na(Sessions.per.Active.User) & is.na(`User Session`))) %>%
  mutate(
    District.Rollup.ID = as.character(District.Rollup.ID),
    curriculum = as.character(curriculum),
    Minutes.per.Active.User = as.numeric(Minutes.per.Active.User)
    ) %>%
  # Fill all NAs with 0 (except `curriculum`)
  mutate(across(where(is.numeric), ~ replace_na(., 0)))

# Write to csv
write.csv(teacher_student_usage_subset, "./Data/df.csv")

```

### Summary Statistics

```{r}
#| label: tbl-summary
#| tbl-cap: "Pre-experimental Intervention Data. This table summarizes key educational metrics for Zearn teachers. The data was collected from July 2019 to June 2020."

teacher_student_usage_subset <- read.csv(file = "./Data/df.csv")
teacher_usage <- read.csv("Data/raw/Teacher Usage - Time Series 2020-10-09T1635.csv")

# Pre-calculate unique teachers and classrooms
unique_teachers <- unique(teacher_student_usage_subset$Teacher.User.ID)
unique_classrooms <- unique(
  teacher_student_usage_subset[, c('Teacher.User.ID', 'Classroom.ID')]
  )

# Statistics for "# of classes"
n_teachers <- length(unique_teachers)
n_classes <- nrow(unique_classrooms)
mean_classes <- n_classes / n_teachers
classes_per_teacher <- unique_classrooms %>% count(Teacher.User.ID)

# Statistics for "# of badges"
teacher_student_usage_subset$Total.Badges <- 
  teacher_student_usage_subset$Active.Users...Total *
  teacher_student_usage_subset$Badges.per.Active.User
badges_per_teacher <- teacher_student_usage_subset %>% 
  group_by(Teacher.User.ID) %>% 
  summarize(Total.Badges = sum(Total.Badges))

# Statistics for "Total minutes on Zearn"
minutes_per_teacher <- teacher_usage %>%
  filter(!Adult.User.ID %in% unique_teachers) %>%
  group_by(Adult.User.ID) %>%
  summarize(Total.Minutes = sum(Minutes.on.Zearn...Total))

# Creating a function for common statistics
calculate_stats <- function(data, value) {
  list(
    N = sum(data[[value]]),
    Mean = mean(data[[value]]),
    SD = sd(data[[value]]),
    Quantiles = quantile(data[[value]], probs = c(0.25, 0.5, 0.75))
  )
}

# Calculating statistics
stats_classes <- calculate_stats(classes_per_teacher, "n")
stats_badges <- calculate_stats(badges_per_teacher, "Total.Badges")
stats_minutes <- calculate_stats(minutes_per_teacher, "Total.Minutes")

# Formatting the table with gt
gt_table <- tibble(
  Category = c("# of teachers", "# of classes", "# of badges", "Total minutes on Zearn"),
  N = c(n_teachers, stats_classes$N, stats_badges$N, stats_minutes$N),
  Mean = c(NA, stats_classes$Mean, stats_badges$Mean, stats_minutes$Mean),
  `Standard Deviation` = c(NA, stats_classes$SD, stats_badges$SD, stats_minutes$SD),
  `1st Quartile` = c(NA, stats_classes$Quantiles[1], stats_badges$Quantiles[1], stats_minutes$Quantiles[1]),
  `2nd Quartile` = c(NA, stats_classes$Quantiles[2], stats_badges$Quantiles[2], stats_minutes$Quantiles[2]),
  `3rd Quartile` = c(NA, stats_classes$Quantiles[3], stats_badges$Quantiles[3], stats_minutes$Quantiles[3])
) %>%
  gt() %>%
  tab_spanner(
    label = "Aggregated Statistics per Teacher",
    columns = c("Mean", "Standard Deviation", "1st Quartile", "2nd Quartile", "3rd Quartile")
  ) %>%
  cols_label(
    Category = "",
    N = "N",
    Mean = "Mean",
    `Standard Deviation` = "Standard Deviation",
    `1st Quartile` = "1st Quartile",
    `2nd Quartile` = "2nd Quartile",
    `3rd Quartile` = "3rd Quartile"
  ) %>%
  fmt_number(
    columns = c("N"),
    decimals = 0
  ) %>%
  fmt_number(
    columns = c("Mean", "Standard Deviation"),
    decimals = 2,
    use_seps = TRUE
  ) %>%
  fmt_number(
    columns = c("1st Quartile", "2nd Quartile", "3rd Quartile"),
    decimals = 1
  ) %>%
  sub_missing(
    columns = c("Mean", "Standard Deviation", "1st Quartile", "2nd Quartile", "3rd Quartile"),
    missing_text = "-"
  )

# Display the table
gt_table

```

```{r clean-environment}
#| include: false

rm(list = setdiff(ls(), c("teacher_student_usage_subset", "teacher_usage")))
gc(verbose = FALSE)

```

-   Specific teacher and student metrics collected.

<!-- SEAN: Help troubleshoot map -->

```{r}
#| eval: false
#| cache: true
#| label: fig-teachers-map
#| fig-cap: "Geographical distribution of teachers across various parishes in Louisiana, and the top 5 cities with the highest number of teachers."

library(sf)
library(tidygeocoder)
library(tigris)
library(furrr)
plan(strategy = "multisession", workers = availableCores())

# Batch geocoding
# Sys.setenv(GEOCODIO_API_KEY = "")
unique_zipcodes <- unique(teacher_student_usage_subset$zipcode) %>%
  as.list()
address_geodata <- furrr::future_map_dfr(.x = unique_zipcodes, 
                               ~ geo(postalcode = .x,
                                     country = "United States",
                                     method = 'geocodio',
                                     full_results = TRUE,
                                     progress_bar = FALSE)) %>%
  select(postalcode,
         address_components.city,
         address_components.county,
         lat, long) %>%
  rename(
    city = address_components.city,
    county = address_components.county
  ) %>%
  mutate(
    postalcode = as.integer(postalcode)
  )

# Merge the geocoding results back into the original data.table
n_teachers <- teacher_student_usage_subset %>%
  group_by(Classroom.ID, Teacher.User.ID, MDR.School.ID,
           District.Rollup.ID, zipcode) %>%
  summarize(Students...Total = mean(Students...Total), 
            .groups = "keep") %>%
  full_join(address_geodata, by = c("zipcode" = "postalcode"))

# Get the top 5 cities by number of teachers
# Aggregate the data to get the number of teachers in each city
top_cities <- n_teachers %>%
  group_by(city, county) %>%
  summarize(
    num_teachers = n_distinct(Teacher.User.ID),
    lat = mean(lat, na.rm = TRUE),
    long = mean(long, na.rm = TRUE),
    .groups = 'drop' # Optionally, drop grouping structure from the result
  ) %>%
  arrange(desc(num_teachers)) %>%
  slice_head(n = 5)

# Get the Louisiana county map data
df_map <- tigris::counties(class = "sf",
                           state = "LA",
                           progress_bar = FALSE) %>%
  # sf::st_set_crs(4326) %>%
  left_join(
    n_teachers %>%
      group_by(county) %>%
      summarize(num_teachers = n_distinct(Teacher.User.ID)),
    by = c("NAMELSAD" = "county")
    ) %>%
  mutate(num_teachers = ifelse(is.na(num_teachers),0,num_teachers)) %>%
  sf::st_as_sf()

ggplot() +
  geom_sf(data = df_map, aes(fill = num_teachers)) +
  scale_fill_continuous(name = "Number of Teachers",
                        low = "white", high = "red", na.value = "gray90") +
  labs(
    title = "Number of Teachers by Parish in Louisiana"
  ) +
  theme_minimal() +
  theme(
    legend.position = "bottom",
    panel.grid = element_blank(),
    axis.text = element_blank(),
    axis.title = element_blank()
  ) +
  geom_point(data = top_cities, aes(x = long, y = lat)) +
  geom_text_repel(data = top_cities, aes(x = long, y = lat, label = city),
                  size = 3, color = "black")

```

# Results

-   Data-driven Nudge Engineering with Habit-based Regression
-   Justification of model choices.

# Materials and Methods

## Study 1

### Data Collection

The Zearn math educational learning platform provided us with administrative data spanning the 2019-2020 academic year (September 2019 to May 2020). This dataset included detailed teacher actions and classroom-week-level student achievement metrics. Teacher actions on the platform were timestamped to the second. For privacy considerations, Zearn aggregated student data at the classroom-week level, including student achievement measures and indications of student struggles. The dataset covered various schools across Louisiana (@fig-teachers-map).

To promote transparency and replicability of our study, we have deposited the data and code used in our analyses in a publicly accessible database. Researchers and interested parties can access the complete dataset and all related processing scripts at the GitHub repository: <https://github.com/SeanHu0727/zearn_nudge.git>

### Inclusion Criteria

We aggregated teacher behavior data to the weekly level and merged it with the student data at the classroom-week level. We also excluded inactive teachers (those with no recorded activity for over two months) from the dataset.

We defined the inclusion criteria for classrooms strictly as such:

1.  Classrooms linked to a single teacher

2.  Classrooms with no more than seven months of inactivity during the academic year

3.  Classrooms with an average of no less than five actively engaged students

## Study 1a

### Independent Component Analysis (ICA)

We extracted all teacher behavioral variables from the dataset that displayed non-zero variance and standardized them to have a zero mean and unit variance. To determine the ideal number of independent components, we performed ICA using a range of components from 1 to 10. Our decision on the optimal number was informed by recognizing the 'elbow' on the scree plot generated from the results. Ultimately, the scree plot revealed three independent components. We used the `icafast` function from the R `ica` package for all ICAs conducted [@helwig2022].

```{r ICA}

library(ica)

# Perform ICA to choose number of components
# Removing columns with zero variance
ICA_cols <- teacher_student_usage_subset %>%
  select(RD.elementary_schedule:RD.foundational_lesson_guidance) %>%
  select(where(~ sd(.x, na.rm = TRUE) != 0)) %>%
  mutate_all(scale)

imod.fast <- ica(ICA_cols, nc = 10)

# Create Elbow Plot
## SEAN: Fix plot to show integers in x-axis and % in y-axis
elbow_plot <- ggplot(data.frame(Component = 1:10, Variance = imod.fast$vafs),
                     aes(x = Component, y = Variance)) +
  geom_line(color = "gray") +
  geom_point() +
  labs(x = "Component", y = "Proportion of Variance",
       title = "Scree Plot for Determining Optimal k") +
  theme_minimal()


# ICA via different algorithms

## SEAN: Check for the 3 methods:
## 1) http://research.ics.aalto.fi/ica/icasso/
## 2) https://www.cs.helsinki.fi/u/ahyvarin/code/isctest/
## We will choose the best method accordingly (most stable and interpretable)

imod.fast <- ica(ICA_cols, nc = 3)
# imod.imax <- ica(ICA_cols, nc = 3, method = "imax")
# imod.jade <- ica(ICA_cols, nc = 3, method = "jade")


# Iterate through each principal component
for (i in 1:ncol(imod.fast$M)) {
    # Find the index of the variable with the largest absolute coefficient
    largest_coef_index <- which.max(abs(imod.fast$M[, i]))

    # Check if the largest coefficient is negative
    if (imod.fast$M[largest_coef_index, i] < 0) {
        # Multiply all coefficients in this principal component by -1
        imod.fast$M[, i] <- -1 * imod.fast$M[, i]
        imod.fast$S[, i] <- -1 * imod.fast$S[, i]
    }
}

# Add ICA components to the dataset
teacher_student_usage_subset <- teacher_student_usage_subset %>%
  mutate(
    ic1 = imod.fast$S[, 1],
    ic2 = imod.fast$S[, 2],
    ic3 = imod.fast$S[, 3]
  )

```

### Censored Panel Regression

-   Explanation of statistical models used.
-   Details of any sensitivity or robustness checks.
-   Training on Statistical Methods: Detail any specific statistical training or standardized methodologies used.

```{r}
library(plm)

# Define the columns to be used in the formula
columns_used <- colnames(teacher_student_usage_subset %>%
                           select(teacher_number_classes, Grade.Level, MDR.School.ID, 
                                  Students...Total, teacher_number_classes, 
                                  # medianIncome:schoolAccount,
                                  ic1:ic3))
# Create the formula
fmla_str <- sprintf("logBadges ~ %s",
                    paste(sprintf("`%s`", columns_used), collapse = " + "))
fmla <- as.formula(fmla_str)

# Define the panel model index
pUsage <- pdata.frame(
  teacher_student_usage_subset %>%
    mutate(Classroom.ID = factor(Classroom.ID, ordered = FALSE),
           Teacher.User.ID = factor(Teacher.User.ID, ordered = FALSE),
           MDR.School.ID = factor(MDR.School.ID, ordered = FALSE),
           District.Rollup.ID = factor(District.Rollup.ID, ordered = FALSE),
           Grade.Level = factor(Grade.Level, ordered = TRUE),
           medianIncome = factor(medianIncome, ordered = TRUE),
           povertyLevel = factor(povertyLevel, ordered = TRUE),
           zipcode = factor(zipcode, ordered = FALSE)) %>%
    group_by(Teacher.User.ID) %>%
    mutate(week = week - lubridate::isoweek(min(Usage.Week)) + 1) %>%
    mutate(week = ifelse(week < 1 | year == 2020,
                         week + 52 * (2020 - lubridate::year(min(Usage.Week))),
                         week)) %>%
    ungroup() %>%
    mutate(logBadges = log(Badges.per.Active.User + 1)),
  index = c("Classroom.ID", "week", "Teacher.User.ID"))

# Create and summarize the 'within' model directly
## SEAN: please compare with objfun = "lad" (maybe more robust to outliers?)
within_model <- pldv(fmla, data = pUsage, model = "fd", objfun = "lsq",
                     lower = 0, sample = "cens")
# within_model <- plm(fmla, data = pUsage, model = "within")
summary(within_model)

# Hausman Test supports Fixed Effects
# re <- pldv(fmla, data = pUsage, model = "random", objfun = "lsq",
#            lower = 0, sample = "cens")
# phtest(within_model, re)
# 
# data:  fmla
# chisq = 385.17, df = 3, p-value < 2.2e-16
# alternative hypothesis: one model is inconsistent

```

## Study 1b

### Regression Analysis Methodology

```{r preprocessing-habits}
#| eval: false

# Prepare data
setDT(teacher_usage)
teacher_usage[, Date := as.Date(Usage.Time)]

# Sequence of dates for the overall grid expand
date_seq <- seq.Date(min(teacher_usage$Date),
                     max(teacher_usage$Date),
                     by = "day")

# Logged indicator
teacher_usage[, logged := 1]

# Create combinations of Adult.User.ID and Date, and merge with logged data
df <- CJ(Adult.User.ID = unique(teacher_usage$Adult.User.ID), Date = date_seq)
df <- df[teacher_usage, on = .(Adult.User.ID, Date),
         `:=`(logged = i.logged,
              Minutes.on.Zearn...Total = i.Minutes.on.Zearn...Total)]
df[is.na(logged), logged := 0]
df[is.na(Minutes.on.Zearn...Total), Minutes.on.Zearn...Total := 0]

# Code datetime variables and compute additional metrics
setorder(df, Adult.User.ID, Date)
df[, `:=`(
  Year = year(Date),
  Month = month(Date),
  Week = isoweek(Date),
  dayofweek = wday(Date, label = TRUE, week_start = 1),
  prev_visits = cumsum(logged) - logged
  ), by = Adult.User.ID]
df[, `:=`(
  prev_dow_visits = cumsum(logged) - logged
  ), by = .(Adult.User.ID, dayofweek)]

# Streak, time lag, and other computations
df[, `:=`(
  lag.logged = shift(logged, 1, type = "lag", fill = 0)
  ), by = Adult.User.ID]
df[, start_streak:=as.integer(logged == 1 & lag.logged == 0)]
df[, start_streak:=cumsum(start_streak), by = Adult.User.ID]
df[lag.logged==0, streak:=as.double(0)]
df[is.na(streak), streak:=as.double(1:.N), by = .(Adult.User.ID, start_streak)]

# Time lag calculation
df[, `:=`(
  start_streak_lag = shift(start_streak, 1, fill = 0)
), by = Adult.User.ID]

# Streak calculation by day of week
df[,attended_lag_dow:=shift(logged, 1, fill = 0),
   by = .(Adult.User.ID, dayofweek)]
df[, `:=`(
  streak_dow = fcase(
    attended_lag_dow == 0, as.double(0),
    default = as.double(NA)
  ),
  start_streak_dow = as.numeric(logged == 1 & attended_lag_dow == 0)
  ), by = .(Adult.User.ID, dayofweek)]
df[, start_streak_dow := cumsum(start_streak_dow),
   by = .(Adult.User.ID, dayofweek)]
df[is.na(streak_dow),
   streak_dow := 1:.N,
   by = .(Adult.User.ID, start_streak_dow, dayofweek)]

# Previous week visits calculations
## Calculate weekly visits
weekly_logged <- df[, .(week_visits = sum(logged)),
                    by = .(Adult.User.ID, Week, Year)]
## Calculate previous week visits
setorder(weekly_logged, Adult.User.ID, Year, Week)
weekly_logged[, prev_week_visits := shift(week_visits, 1, fill = 0),
               by = Adult.User.ID]
df <- merge(df, weekly_logged,
            by = c("Adult.User.ID", "Week", "Year"), all.x = TRUE)
setorder(df, Adult.User.ID, Date)

# Calculate previous day of week visit
df[, prev_dow_visit := shift(logged, 1, fill = 0),
   by = .(Adult.User.ID, dayofweek)]
# Previous minutes spent
df[, prev_min_zearn := shift(Minutes.on.Zearn...Total, 1, fill = 0),
   by = Adult.User.ID]

# Add a time column, a sequence for each Adult.User.ID
df[, time := seq_len(.N), by = Adult.User.ID]
# Calculate the first time point for each Adult.User.ID where logged == 1
first_time_index <- df[logged == 1,
                       .(first_time = min(time)),
                       by = Adult.User.ID]
df[first_time_index,
   on = .(Adult.User.ID),
   `:=`(first_time = i.first_time)]
df[, first_time := time - first_time]
# Calculate the last time point for each Adult.User.ID where logged == 1
last_time_index <- df[logged == 1,
                      .(last_time = max(time)),
                      by = Adult.User.ID]
df[last_time_index,
   on = .(Adult.User.ID),
   `:=`(last_time = i.last_time)]
df[, last_time := last_time - time]
# Filter out rows outside the first and last logged==1
df <- df[first_time >= 0 & last_time >= 0]

df[, `:=`(
  first_week = isoweek(min(Date)),
  first_year = year(min(Date)),
  time = 1:.N
  ), by = Adult.User.ID]
df[, Week := Week - first_week + 1]
df[, Week := fifelse(Week < 1 | Year == 2020,
                     Week + 52 * (2020 - first_year),
                     Week), by = Adult.User.ID]

df[, `:=`(
  mean_visits = mean(logged),
  prev_freq = prev_visits / time,
  prev_dow_freq = prev_dow_visits / Week
  ), by = Adult.User.ID]
  
df[lag.logged==1, time_lag:=1]
df[lag.logged==0, time_lag:=as.double(1:.N)+1,
   by = .(Adult.User.ID, start_streak_lag)]
df[time == 1, time_lag:=0, by = Adult.User.ID]

# Finalize the dataset
# Cleanup: Remove temporary columns
df[, c("first_time", "last_time", "first_week", "first_year") := NULL]
setorder(df, Adult.User.ID, Date)
# Write to csv
fwrite(df, "./Data/df_habits.csv")

```

<!-- SEAN: Let's try a Panel model here as well. We might need to use LASSO because we have so many variables (we also want to include interactions of weekdays with streak_dow). Panel is what we did for the previous analysis, but it was a bit more complicated with the habitization part. We can just do one mixed-effects LASSO and see if the results coincide with the analysis below and with the previous LASSO analysis. -->

```{r habits}

# Read in the data
df <- fread(file = "./Data/df_habits.csv") %>%
  lazy_dt()

df_transformed <- df %>%
  filter(Adult.User.ID %in%
           unique(teacher_student_usage_subset$Teacher.User.ID)) %>%
  mutate(dayofweek = factor(dayofweek, ordered = FALSE,
                            levels = c("Mon", "Tue", "Wed", "Thu",
                                       "Fri", "Sat", "Sun")),
         Month = factor(Month, ordered = FALSE),
         week_label = isoweek(Date)) %>%
  pivot_wider(names_from = dayofweek, values_from = logged, 
              values_fill = list(logged = 0)) %>%
  group_by(Adult.User.ID, Week) %>%
  summarize(across(c(Mon:Sun), sum, na.rm = TRUE),
            Total_Minutes_on_Zearn = sum(Minutes.on.Zearn...Total, na.rm = TRUE),
            mean_streak = mean(streak, na.rm = TRUE),
            mean_streak_dow = mean(streak_dow, na.rm = TRUE),
            mean_visits = mean(mean_visits, na.rm = TRUE),
            mean_time_lag = mean(time_lag, na.rm = TRUE),
            Month = first(Month),
            Year = first(Year),
            week_label = first(week_label),
            .groups = 'drop')

# Pivot longer to aggregate by month
df_monthly <- df_transformed %>%
  pivot_longer(cols = Mon:Sun, names_to = "dayofweek", values_to = "logged") %>%
  group_by(Adult.User.ID, Month) %>%
  summarize(total_logged_per_month = as.double(sum(logged, na.rm = TRUE))) %>%
  # Pivot wider to create a column for each month
  pivot_wider(names_from = Month, values_from = total_logged_per_month,
              names_prefix = "month_",
              values_fill = list(total_logged_per_month = 0)) %>%
  mutate(row_total = rowSums(select(., starts_with("month_")))) %>%
  mutate(across(starts_with("month_"), ~ ./row_total)) %>%
  select(-row_total)

pUsage <- pdata.frame(
  df_transformed %>%
    group_by(Adult.User.ID) %>%
    summarize(across(c(Mon:Sun), sum, na.rm = TRUE),
              across(c(Total_Minutes_on_Zearn:mean_time_lag), mean, na.rm = TRUE),
              .groups = 'drop') %>%
    mutate(row_total = rowSums(select(., c(Mon:Sun)))) %>%
    mutate(across(c(Mon:Sun), ~ ./row_total)) %>%
    select(-row_total) %>%
    inner_join(df_monthly, by = "Adult.User.ID") %>%
    inner_join(teacher_student_usage_subset %>%
                 lazy_dt() %>%
                 group_by(Teacher.User.ID, week, MDR.School.ID) %>%
                 summarize(Badges.per.Active.User = sum(Badges.per.Active.User),
                           Year = first(year),
                           .groups = 'drop') %>%
                 group_by(Teacher.User.ID, MDR.School.ID) %>%
                 summarize(Badges.per.Active.User = mean(Badges.per.Active.User),
                           .groups = 'drop'),
               by = c("Adult.User.ID" = "Teacher.User.ID")
               # by = c("Adult.User.ID" = "Teacher.User.ID",
               #        "week_label" = "week", "Year")
               ) %>%
    mutate(Adult.User.ID = factor(Adult.User.ID, ordered = FALSE),
           MDR.School.ID = factor(MDR.School.ID, ordered = FALSE),
           logBadges = log(Badges.per.Active.User + 1)) %>%
    as_tibble(),
  index = c("MDR.School.ID"))
  # index = c("Adult.User.ID", "Week", "MDR.School.ID"))

panel_model <- plm(logBadges ~ Mon + Tue + Wed + Thu + Fri + Sat +
                     month_1 + month_2 + month_3 + month_4 + month_5 + month_6 +
                     month_8 + month_9 + month_10 + month_11 + month_12 +
                     Total_Minutes_on_Zearn + mean_streak + mean_streak_dow +
                     mean_visits + mean_time_lag,
                   data = pUsage, model = "within")
# panel_model <- pldv(logBadges ~ Mon + Tue + Wed + Thu + Fri + Sat + Sun + Month +
#                      Total_Minutes_on_Zearn + mean_streak + mean_streak_dow +
#                      mean_visits + mean_time_lag,
#                     data = pUsage, model = "fd", objfun = "lsq",
#                     lower = 0, sample = "cens")

summary(panel_model)

```

-   Detailed steps in the regression analysis.
-   Explanation of fixed effects model.
-   Software and tools used in analysis.
-   Publishing Negative Data: Include any negative data that did not support hypotheses.
-   List of all statistical software and tools.
-   Versions and settings used.
-   Thorough Description of Methods: Clearly report experimental parameters.

## Study 2

### Pre-registration Details

-   Date created: September 14, 2021.
-   Confirmation that no data were collected prior to the study.
-   Hypotheses: H1 and H2 with detailed descriptions.
-   Dependent Variables: Primary and secondary variables with specific definitions and timeframes.
-   Conditions: Description of "Empathy" treatment and "Control Condition".
-   Analysis Plan: Description of regression analysis, control variables, handling of missing values, and adjustments for multiple comparisons.
-   Outliers and Exclusions: Specific criteria for exclusion of observations.
-   Sample Size: Number of participants and criteria for eligibility.
-   Additional Analyses: Detailed list of supplementary analyses to be conducted.

### Evaluation / Impact Assessment

-   Methods of measuring intervention impact.
-   Statistical analysis of results.
-   Detailed Reporting of Methodologies: Include comprehensive description of all methodologies used.
-   Outcome Measure
-   Specific measures used to evaluate the impact of interventions.
-   Techniques for evaluating nudge effectiveness.
-   Comprehensive Methodological Reporting: Ensure thorough reporting of all methods and experimental parameters.

## Study 2a

### Intervention Design

-   Steps in developing empathy-based interventions.
-   Criteria for intervention selection.
-   Nudge Engineering / Increasing Engagement Through Empathy

### Implementation Method

-   Detailed procedure for implementing interventions.
-   Timeline and phases of implementation.

## Study 2b

### Intervention Design

-   Nudge Engineering / Increasing Engagement Through Habitization

### Implementation Method

-   Detailed procedure for implementing interventions.

```{stata}



```

### Ethical Considerations and IRB Approval

This study was conducted in accordance with ethical standards and received approval from the Institutional Review Board (IRB) at the University of Pennsylvania. Given the nature of the data, which included de-identified and aggregated information, the IRB determined that informed consent from individual participants was not required for this research. The study's methodologies were designed to ensure the confidentiality and anonymity of all participants involved, adhering strictly to ethical guidelines for educational research.

# Supplementary Information

## Supplementary Methods

### Independent Component Analysis (ICA)

We implement the FastICA algorithm [@hyvärinen2000] to estimate independent components from our dataset. In this model, matrix $X = {x_{ij}}_{I \times J}$ , consisting of $I$ samples across $J$ random variables, is expressed as a linear mixture of independent components $C$, represented by:

$$
X = C' M + E.
$$

Here, $C$ holds the independent components, $M$ is a mixing matrix, and $E$ denotes the noise. The aim is to minimize mutual information between components in $C$, which is achieved by maximizing their marginal negentropy, thereby rendering the columns of $C$ statistically independent.

The FastICA process begins by transforming $X$ into a whitened matrix $Y$, ensuring uncorrelated variables with unit variance. This transformation is achieved through eigenvalue decomposition:

$$
Y = X \times \begin{bmatrix} \frac{\mathbf{v_1}}{\sqrt{\lambda_1}} & \frac{\mathbf{v_2}}{\sqrt{\lambda_2}} & \frac{\mathbf{v_3}}{\sqrt{\lambda_3}} \end{bmatrix},
$$

where ($\lambda_1, \lambda_2, \lambda_3$) and ($\mathbf{v_1}, \mathbf{v_2}, \mathbf{v_3}$) are, respectively, the eigenvalues and eigenvectors of $\frac{X'X}{I}$.

Afterward, the algorithm approximates the negentropy with

$$
\hat{\theta}(c_n) = (\mathbb{E}[\ln(\cosh(c_n))] − \mathbb{E}[\ln(\cosh(z))])^2
$$

where $c_n, n \in {1, 2, 3}$, is one of the components, and $z$ is a Gaussian variable with zero mean and unit variance. FastICA iteratively maximizes this value across all components, producing an orthogonal rotation matrix $R_{3 \times 3}$ such that $C = YR$.

For more details on ICA and the FastICA algorithm, see [@hyvärinen2000a; @helwig2013].

## Supplementary Tables

## Supplementary Discussion

### Robustnes Checks

```{r PCA}
#| eval: false

pca_result <- prcomp(PCA_cols, scale. = TRUE)
prop_variance <- summary(pca_result)$importance[2, 1:10]

# Add to dataset
teacher_student_usage_subset <- teacher_student_usage_subset %>%
  mutate(
    pc1 = pca_result$x[, 1],
    pc2 = pca_result$x[, 2],
    pc3 = pca_result$x[, 3]
  )


## Robustness
# Exclude User.Session
ICA_cols <- teacher_student_usage_subset %>%
  select(RD.elementary_schedule:RD.foundational_lesson_guidance) %>%
  select(where(~ sd(.x, na.rm = TRUE) != 0)) %>%
  mutate_all(scale)

imod.fast2 <- ica(ICA_cols, nc = 3)

# Variance explained
imod.fast2

# Creating a data frame for the ICA loadings
ica_loadings_df <- as.data.frame(t(ica_result$A))
colnames(ica_loadings_df) <- paste("Component", 1:ncol(ica_loadings_df))
ica_loadings_df$Feature <- colnames(teacher_student_usage_subset[13:39])[ICA_cols]

# Creating the table with gt
ica_loadings_table <- ica_loadings_df %>%
  relocate(Feature) %>%
  gt() %>%
  tab_header(
    title = "ICA Loadings",
    subtitle = "Loadings of each feature on the independent components"
  ) %>%
  cols_label(
    Feature = "Variable",
    `Component 1` = "Component 1",
    `Component 2` = "Component 2",
    `Component 3` = "Component 3"
  )

# Display the table
ica_loadings_table

```

## Supplementary Equations

## Supplementary Notes

<!-- (including notes clarifying statistical analyses, acknowledgements, grant or other numbers) -->
