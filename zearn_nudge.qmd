---
title: "Discovering Data-Driven Nudges to Help Students Learn More Math"
abstract: ""
keywords: "Pedagogical Decision-Making, Digital Education Platforms, Empirical Field Data, Instructional Adaptation"
author:
  - name: Marcos Gallo
    orcid: 0000-0002-8227-2661
  - name: Sean Hu
  - name: Ben Manning
  - name: Angela Duckworth
  - name: Colin Camerer
format: pdf
bibliography: references.bib

execute:
  echo: false
  warning: false
  error: false
---

```{r import}

library(data.table)
library(dtplyr)
library(tidyverse)
# library(zoo)
library(ggrepel)
library(gt)
library(Statamarkdown)
set.seed(794563797)
# Timestamp: 2023-12-21 21:42:25 UTC
# Â© 1998-2023 RANDOM.ORG

```

```{r preprocessing}
#| eval: false

teacher_usage <- read.csv("Data/raw/Teacher Usage - Time Series 2020-10-09T1635.csv")
classroom_student_usage <- read.csv("Data/raw/Classroom Student Usage - Time Series 2020-10-09T1616.csv")
classroom_info <- read.csv("Data/raw/Classroom Info 2020-10-09T1617.csv")
classroom_teacher_lookup <- read.csv("Data/raw/Classroom-Teacher Lookup 2020-10-09T1618.csv")
school_info <- read.csv("Data/raw/School Info 2020-10-09T1619.csv")
la_usage_types <- read.csv("Data/raw/la_usage_types.csv")

teacher_usage <- teacher_usage %>%
  mutate(
    # Noting week, and year to summarize teacher behavior:
    week = lubridate::isoweek(Usage.Time),
    year = lubridate::year(Usage.Time),
    # Transforming Event.Type for "Resource Downloaded"
    Event.Type = ifelse(Event.Type == "Resource Downloaded",
                        paste0("RD.", Curriculum.Resource.Category),
                        Event.Type)) %>%
  mutate(year = ifelse(week == 1 & year == 2019, 2020, year))

classroom_student_usage <- classroom_student_usage %>% 
  mutate(Minutes.per.Active.User = as.numeric(Minutes.per.Active.User))


# Teacher activity
teacher_usage_total <- teacher_usage %>%
  # Summarize teacher behavior
  count(Adult.User.ID, Event.Type, week, year) %>%
  pivot_wider(names_from = Event.Type, values_from = n,
              values_fill = list(n = 0)) %>%
  full_join(teacher_usage %>%
              group_by(Adult.User.ID, week, year) %>%
              # Summarize teacher minutes
              summarize(Minutes.on.Zearn...Total = sum(Minutes.on.Zearn...Total)) %>%
              ungroup(),
            by = c("Adult.User.ID", "week", "year"))

# Classroom activity
# Filter out classrooms
classroom_info <- classroom_info %>%
  group_by(Classroom.ID) %>%
  # At least 5 students enrolled
  filter(Students...Total > 5) %>%
  # Remove classrooms with multiple schools
  filter(n_distinct(MDR.School.ID) == 1) %>%
  ungroup()
# Merge with classroom information
classroom_student_usage_total <- classroom_teacher_lookup %>%
  inner_join(classroom_info,
             by = "Classroom.ID",
             relationship = "many-to-many") %>%
  ungroup() %>%
  # Merge with classroom activity
  inner_join(classroom_student_usage %>%
               # # Remove empty weeks (not present in original code)
               # filter(rowSums(select(., -c(1:2)), na.rm = TRUE) > 0) %>%
               group_by(Classroom.ID) %>%
               # At least 5 active students per week on average
               filter(mean(Active.Users...Total, na.rm = T) > 5) %>%
               ungroup() %>%
               # Fill in NAs with 0
               mutate(across(-c(1:2), ~ replace_na(., 0))),
             by = "Classroom.ID", relationship = "many-to-many") %>%
  # Use only mergeable teachers
  inner_join(teacher_usage_total %>% count(Adult.User.ID, name = "teacher_weeks"),
             by = c("Teacher.User.ID" = "Adult.User.ID"),
             relationship = "many-to-many") %>%
  mutate(week = lubridate::isoweek(Usage.Week),
         year = lubridate::year(Usage.Week)) %>%
  mutate(year = ifelse(week == 1 & year == 2019, 2020, year))

# Merging and Filtering active teacher-classrooms
teacher_student_usage_subset <- classroom_teacher_lookup %>%
  # Merge with Teacher activity
  inner_join(teacher_usage_total,
             by = c("Teacher.User.ID" = "Adult.User.ID"),
             relationship = "many-to-many") %>%
  # Merge with classroom information
  inner_join(classroom_info %>% select(Classroom.ID, MDR.School.ID),
             by = "Classroom.ID", relationship = "many-to-many") %>%
  # Notice inner_join: we only match weeks for which both teachers and 
  # classrooms have data, making it fit for fixed effects
  inner_join(classroom_student_usage_total,
             by = c("Classroom.ID", "Teacher.User.ID", "MDR.School.ID",
                    "week", "year"),
             relationship = "many-to-many") %>%
  group_by(Classroom.ID) %>%
  # Remove teachers or classes inactive for at least 5 months
  filter(n() > 20) %>%
  # Remove June, July, and August
  filter(!week %in% c(23:35)) %>%
  # Calculate the number of classes per teacher
  ungroup() %>% group_by(Teacher.User.ID) %>%
  mutate(teacher_number_classes = n_distinct(Classroom.ID)) %>%
  ungroup() %>% group_by(Classroom.ID) %>%
  # Remove duplicate (classroom, week) pairs
  mutate(unique_id = paste(Classroom.ID, Usage.Week)) %>%
  # Remove those most likely to be "supervisors"
  arrange(teacher_number_classes, -teacher_weeks) %>%
  filter(!duplicated(unique_id)) %>% select(-unique_id) %>%
  ungroup() %>% group_by(Teacher.User.ID) %>%
  # Adjust the number of weeks for each teacher
  mutate(week = week - lubridate::isoweek(min(Usage.Week)) + 1) %>%
  mutate(week = ifelse(week < 1 | year == 2020,
                       week + 52 * (2020 - min(year)),
                       week)) %>%
  ungroup() %>% arrange(Classroom.ID, Teacher.User.ID, Usage.Week)


# Merge with school types
teacher_student_usage_subset <- teacher_student_usage_subset %>%
  left_join(la_usage_types %>% mutate(
    curriculum = as.integer(`Schools.and.Districts.Usage.Type` %in%
                              c("20-21 Curriculum", "Core Complement")),
    MDR.School.ID = as.integer(`Schools.and.Districts.MDR.School.ID`)) %>%
      select(MDR.School.ID, curriculum), by = c("MDR.School.ID")) %>%
  left_join(school_info %>% 
              select(MDR.School.ID, District.Rollup.ID,
                     Demographics...Zipcode.Median.Income,
                     Demographics...Poverty.Level,
                     Is.Charter..Yes...No.,
                     MDR.School.has.School.Account..Yes...No.,
                     School.Address...Zipcode),
            by = "MDR.School.ID") %>%
  rename(medianIncome = Demographics...Zipcode.Median.Income,
         povertyLevel = Demographics...Poverty.Level,
         charterSchool = Is.Charter..Yes...No.,
         schoolAccount = MDR.School.has.School.Account..Yes...No.,
         zipcode = School.Address...Zipcode) %>%
  mutate(charterSchool = case_when(charterSchool == "No" ~ 0,
                                   charterSchool == "Yes" ~ 1,
                                   .default = NA),
         schoolAccount = case_when(schoolAccount == "No" ~ 0,
                                   schoolAccount == "Yes" ~ 1,
                                   .default = NA)) %>%
  rename(Adult.User.ID = Teacher.User.ID)


# Write to csv
write.csv(teacher_student_usage_subset, "./Data/df.csv")

```

```{r preprocessing-habits}
#| eval: false

# Prepare data
setDT(teacher_usage)
teacher_usage[, Date := as.Date(Usage.Time)]

# Sequence of dates for the overall grid expand
date_seq <- seq.Date(min(teacher_usage$Date),
                     max(teacher_usage$Date),
                     by = "day")

# Logged indicator
teacher_usage[, logged := 1]

# Create combinations of Adult.User.ID and Date, and merge with logged data
df <- CJ(Adult.User.ID = unique(teacher_usage$Adult.User.ID), Date = date_seq)
df <- df[teacher_usage, on = .(Adult.User.ID, Date),
         `:=`(logged = i.logged,
              Minutes.on.Zearn...Total = i.Minutes.on.Zearn...Total)]
df[is.na(logged), logged := 0]
df[is.na(Minutes.on.Zearn...Total), Minutes.on.Zearn...Total := 0]

# Code datetime variables and compute additional metrics
setorder(df, Adult.User.ID, Date)
df[, `:=`(
  Year = year(Date),
  Month = month(Date),
  Week = isoweek(Date),
  dayofweek = wday(Date, label = TRUE, week_start = 1),
  prev_visits = cumsum(logged) - logged
  ), by = Adult.User.ID]
df[, `:=`(
  prev_dow_visits = cumsum(logged) - logged
  ), by = .(Adult.User.ID, dayofweek)]

# Streak, time lag, and other computations
df[, `:=`(
  lag.logged = shift(logged, 1, type = "lag", fill = 0)
  ), by = Adult.User.ID]
df[, start_streak:=as.integer(logged == 1 & lag.logged == 0)]
df[, start_streak:=cumsum(start_streak), by = Adult.User.ID]
df[lag.logged==0, streak:=as.double(0)]
df[is.na(streak), streak:=as.double(1:.N), by = .(Adult.User.ID, start_streak)]

# Time lag calculation
df[, `:=`(
  start_streak_lag = shift(start_streak, 1, fill = 0)
), by = Adult.User.ID]

# Streak calculation by day of week
df[,attended_lag_dow:=shift(logged, 1, fill = 0),
   by = .(Adult.User.ID, dayofweek)]
df[, `:=`(
  streak_dow = fcase(
    attended_lag_dow == 0, as.double(0),
    default = as.double(NA)
  ),
  start_streak_dow = as.numeric(logged == 1 & attended_lag_dow == 0)
  ), by = .(Adult.User.ID, dayofweek)]
df[, start_streak_dow := cumsum(start_streak_dow),
   by = .(Adult.User.ID, dayofweek)]
df[is.na(streak_dow),
   streak_dow := 1:.N,
   by = .(Adult.User.ID, start_streak_dow, dayofweek)]

# Previous week visits calculations
## Calculate weekly visits
weekly_logged <- df[, .(week_visits = sum(logged)),
                    by = .(Adult.User.ID, Week, Year)]
## Calculate previous week visits
setorder(weekly_logged, Adult.User.ID, Year, Week)
weekly_logged[, prev_week_visits := shift(week_visits, 1, fill = 0),
               by = Adult.User.ID]
df <- merge(df, weekly_logged,
            by = c("Adult.User.ID", "Week", "Year"), all.x = TRUE)
setorder(df, Adult.User.ID, Date)

# Calculate previous day of week visit
df[, prev_dow_visit := shift(logged, 1, fill = 0),
   by = .(Adult.User.ID, dayofweek)]
# Previous minutes spent
df[, prev_min_zearn := shift(Minutes.on.Zearn...Total, 1, fill = 0),
   by = Adult.User.ID]

# Add a time column, a sequence for each Adult.User.ID
df[, time := seq_len(.N), by = Adult.User.ID]
# Calculate the first time point for each Adult.User.ID where logged == 1
first_time_index <- df[logged == 1,
                       .(first_time = min(time)),
                       by = Adult.User.ID]
df[first_time_index,
   on = .(Adult.User.ID),
   `:=`(first_time = i.first_time)]
df[, first_time := time - first_time]
# Calculate the last time point for each Adult.User.ID where logged == 1
last_time_index <- df[logged == 1,
                      .(last_time = max(time)),
                      by = Adult.User.ID]
df[last_time_index,
   on = .(Adult.User.ID),
   `:=`(last_time = i.last_time)]
df[, last_time := last_time - time]
# Filter out rows outside the first and last logged==1
df <- df[first_time >= 0 & last_time >= 0]

df[, `:=`(
  first_week = isoweek(min(Date)),
  first_year = year(min(Date)),
  time = 1:.N
  ), by = Adult.User.ID]
df[, Week := Week - first_week + 1]
df[, Week := fifelse(Week < 1 | Year == 2020,
                     Week + 52 * (2020 - first_year),
                     Week), by = Adult.User.ID]

df[, `:=`(
  mean_visits = mean(logged),
  total_visits = sum(logged),
  prev_freq = prev_visits / time,
  prev_dow_freq = prev_dow_visits / Week
  ), by = Adult.User.ID]
  
df[lag.logged==1, time_lag:=1]
df[lag.logged==0, time_lag:=as.double(1:.N)+1,
   by = .(Adult.User.ID, start_streak_lag)]
df[time == 1, time_lag:=0, by = Adult.User.ID]

# Finalize the dataset
# Cleanup: Remove temporary columns
df[, c("first_time", "last_time", "first_week", "first_year") := NULL]
setorder(df, Adult.User.ID, Date)
# Write to csv
fwrite(df, "./Data/df_habits.csv")

```

# Introduction

The decline in math performance among American students has been a critical issue exacerbated by the COVID-19 pandemic, with some reporting an alarming half-year lag in math achievement among U.S. public school students in grades 3-8 [@fahle2023]. Learning losses and disparities, particularly among younger students, have been significant due to reduced instructional time and remote learning challenges [@zierer2021; @dipietro2023]. Moreover, @ewing2021 note that repeated school closures compounded these issues, emphasizing that the pandemic is not the only problem affecting math performance. Historical trends have also played a role, indicating a need for comprehensive solutions.

Emphasizing quality math instruction is paramount for improving math performance. Teacher proficiency in pedagogical strategies, guidance, and teacher-student relations is crucial, especially for students who have lost interest in mathematics or lack a sufficient foundation [@wang2023; @battey2016]. The correlation between teachers' deep mathematical knowledge and student success highlights the importance of specialized training [@hill2008; @battey2016].

The pandemic also accelerated the adoption of digital platforms for math education. Indeed, adaptive practice software can mitigate the impact of school closures and possibly reverse their adverse effects [@meeter2021; @alabdulaziz2021]. Recent meta-analyses demonstrate that integrating digital tools and blended learning approaches improves student outcomes significantly [@ran2021; @ran2020; @sadaf2021]. Leveraging these tools and insights gained during the pandemic should help address longstanding educational challenges [@ewing2021]. In particular, integrating technology, pedagogy, and content knowledge is essential, and professional development programs are most effective when they focus on using technology to foster a more engaging and effective learning environment [@young2016; @blanchard2016].

The rise of digital platforms has also underlined the importance of student engagement in online learning. Blending online and traditional teaching methods effectively enhances engagement and understanding [@chiang2016; @sadaf2021]. Teachers are essential in helping students develop meta-cognitive skills to enhance student engagement [@haleva2020]. Furthermore, teachers' beliefs and self-efficacy toward technology integration influence their willingness to adopt innovative teaching practices [@ertmer2012; @liljedahl2020]. This relationship between student engagement and teacher attitudes highlights the importance of providing cost-effective, high-quality solutions that can be implemented across diverse educational settings.

In this study, we partner with Zearn to address these topics. This math education platform reaches approximately 25% of United States elementary schools and over a million middle-school students. Zearn's approach integrates interactive digital lessons with hands-on teaching, aligning with the Common Core State Standards and providing a comprehensive educational experience [@zearnma2023].

Our study leverages this rich resource to offer an innovative approach to educational interventions using behavioral science principles. Focusing on teacher quality, we align with @hanushek2020, who maintains that the efficacy of resource utilization supersedes quantity. We also follow current trends in providing cost-effective, easy-to-implement interventions that align engagement incentives [@lavecchia2016; @koch2015].

Our two-step approach initially employs unsupervised learning techniques to analyze behavioral patterns in teacher activity on Zearn, aligning with the data mining value in educational research [@salazar2007; @hershcovits2020; @qiu2022; @al-shabandar2018; @shin2020]. Subsequently, we aim to establish the causality of our interventions through a large-scale field experiment guided by recommendations from @greene2022 for holistic, transparent, and reproducible research. Our unique integration of behavioral science with digital education seeks to provide impactful insights into math education and offer a blueprint for similar studies in other fields.

# Results

## Study 1: Data-Driven Nudge Engineering

Our study used a comprehensive dataset from Zearn's educational platform, encompassing the 2019-2020 academic year and spanning multiple schools in Louisiana. Zearn's approach combines concrete, pictorial, and abstract methods for teaching mathematics, offering a personalized experience that allows teachers to track student progress effectively. Key features like the Badge system, which tracks student lesson completion, and Tower Alerts, which notifies teachers when students repeatedly struggle with a given concept, motivate students and provide valuable insights for educators.

Key variables from the dataset included teacher logins, file downloads, and specific interactions with educational content. Additionally, we accessed alongside student data at the classroom-week level, encompassing metrics such as lesson completion (i.e., Badges) and instances of learning difficulties (i.e., Tower Alerts). This granularity allowed for an in-depth analysis of both teacher behaviors and student performance.

The data revealed diverse engagement patterns. Teachers logged into Zearn multiple times weekly, with notable variations in interaction frequency and duration. Student data reflected a broad spectrum of performance levels across different classrooms and schools. The standard deviations of key variables underscored this diversity, as seen in @tbl-summary, which displays comprehensive summary statistics. This rich combination of teacher behaviors and student performance metrics, carefully matched with a weekly frequency for each classroom, allowed for a thorough analysis while upholding privacy standards.

```{r}
#| label: tbl-summary
#| tbl-cap: "Study 1 Data. This table summarizes key educational metrics for Zearn teachers. The data was collected from July 2019 to June 2020."

teacher_student_usage_subset <- read.csv(file = "./Data/df.csv")
teacher_usage <- read.csv("Data/raw/Teacher Usage - Time Series 2020-10-09T1635.csv")

# Pre-calculate unique teachers and classrooms
unique_teachers <- unique(teacher_student_usage_subset$Adult.User.ID)
unique_classrooms <- unique(
  teacher_student_usage_subset[, c('Adult.User.ID', 'Classroom.ID')]
  )

# Statistics for "# of classes"
n_teachers <- length(unique_teachers)
n_classes <- nrow(unique_classrooms)
mean_classes <- n_classes / n_teachers
classes_per_teacher <- unique_classrooms %>% count(Adult.User.ID)

# Statistics for "# of badges"
teacher_student_usage_subset$Total.Badges <- 
  teacher_student_usage_subset$Active.Users...Total *
  teacher_student_usage_subset$Badges.per.Active.User
badges_per_teacher <- teacher_student_usage_subset %>% 
  group_by(Adult.User.ID) %>% 
  summarize(Total.Badges = sum(Total.Badges))

# Statistics for "Total minutes on Zearn"
minutes_per_teacher <- teacher_usage %>%
  filter(!Adult.User.ID %in% unique_teachers) %>%
  group_by(Adult.User.ID) %>%
  summarize(Total.Minutes = sum(Minutes.on.Zearn...Total))

# Creating a function for common statistics
calculate_stats <- function(data, value) {
  list(
    N = sum(data[[value]]),
    Mean = mean(data[[value]]),
    SD = sd(data[[value]]),
    Quantiles = quantile(data[[value]], probs = c(0.25, 0.5, 0.75))
  )
}

# Calculating statistics
stats_classes <- calculate_stats(classes_per_teacher, "n")
stats_badges <- calculate_stats(badges_per_teacher, "Total.Badges")
stats_minutes <- calculate_stats(minutes_per_teacher, "Total.Minutes")

# Formatting the table with gt
gt_table <- tibble(
  Category = c("`#` of teachers", "`#` of classes", "`#` of badges", "Total minutes \n on Zearn"),
  N = c(n_teachers, stats_classes$N, stats_badges$N, stats_minutes$N),
  Mean = c(NA, stats_classes$Mean, stats_badges$Mean, stats_minutes$Mean),
  `Standard Deviation` = c(NA, stats_classes$SD, stats_badges$SD, stats_minutes$SD),
  `1st Quartile` = c(NA, stats_classes$Quantiles[1], stats_badges$Quantiles[1], stats_minutes$Quantiles[1]),
  `2nd Quartile` = c(NA, stats_classes$Quantiles[2], stats_badges$Quantiles[2], stats_minutes$Quantiles[2]),
  `3rd Quartile` = c(NA, stats_classes$Quantiles[3], stats_badges$Quantiles[3], stats_minutes$Quantiles[3])
) %>%
  gt() %>%
  tab_spanner(
    label = "Aggregated Statistics per Teacher",
    columns = c("Mean", "Standard Deviation", "1st Quartile", "2nd Quartile", "3rd Quartile")
  ) %>%
  cols_label(
    Category = "",
    N = "N",
    Mean = "Mean",
    `Standard Deviation` = "Standard \n Deviation",
    `1st Quartile` = "1st \n Quartile",
    `2nd Quartile` = "Median",
    `3rd Quartile` = "3rd \n Quartile"
  ) %>% 
  fmt_markdown(columns = everything(), rows = TRUE) %>%
  fmt_number(
    columns = c("N"),
    decimals = 0
  ) %>%
  fmt_number(
    columns = c("Mean", "Standard Deviation"),
    decimals = 2,
    use_seps = TRUE
  ) %>%
  fmt_number(
    columns = c("1st Quartile", "2nd Quartile", "3rd Quartile"),
    decimals = 1
  ) %>%
  sub_missing(
    columns = c("Mean", "Standard Deviation", "1st Quartile", "2nd Quartile", "3rd Quartile"),
    missing_text = "-"
  )

# Display the table
gt_table

```

```{r clean-environment}
#| include: false

rm(list = setdiff(ls(), c("teacher_student_usage_subset", "teacher_usage")))
gc(verbose = FALSE)

```

<!-- SEAN: Help troubleshoot map -->

```{r}
#| eval: false
#| cache: true
#| label: fig-teachers-map
#| fig-cap: "Geographical distribution of teachers across various parishes in Louisiana, and the top 5 cities with the highest number of teachers."

library(sf)
library(tidygeocoder)
library(tigris)
library(furrr)
plan(strategy = "multisession", workers = availableCores())

# Batch geocoding
# Sys.setenv(GEOCODIO_API_KEY = "")
unique_zipcodes <- unique(teacher_student_usage_subset$zipcode) %>%
  as.list()
address_geodata <- furrr::future_map_dfr(.x = unique_zipcodes, 
                               ~ geo(postalcode = .x,
                                     country = "United States",
                                     method = 'geocodio',
                                     full_results = TRUE,
                                     progress_bar = FALSE)) %>%
  select(postalcode,
         address_components.city,
         address_components.county,
         lat, long) %>%
  rename(
    city = address_components.city,
    county = address_components.county
  ) %>%
  mutate(
    postalcode = as.integer(postalcode)
  )

# Merge the geocoding results back into the original data.table
n_teachers <- teacher_student_usage_subset %>%
  group_by(Classroom.ID, Teacher.User.ID, MDR.School.ID,
           District.Rollup.ID, zipcode) %>%
  summarize(Students...Total = mean(Students...Total), 
            .groups = "keep") %>%
  full_join(address_geodata, by = c("zipcode" = "postalcode"))

# Get the top 5 cities by number of teachers
# Aggregate the data to get the number of teachers in each city
top_cities <- n_teachers %>%
  group_by(city, county) %>%
  summarize(
    num_teachers = n_distinct(Teacher.User.ID),
    lat = mean(lat, na.rm = TRUE),
    long = mean(long, na.rm = TRUE),
    .groups = 'drop' # Optionally, drop grouping structure from the result
  ) %>%
  arrange(desc(num_teachers)) %>%
  slice_head(n = 5)

# Get the Louisiana county map data
df_map <- tigris::counties(class = "sf",
                           state = "LA",
                           progress_bar = FALSE) %>%
  # sf::st_set_crs(4326) %>%
  left_join(
    n_teachers %>%
      group_by(county) %>%
      summarize(num_teachers = n_distinct(Teacher.User.ID)),
    by = c("NAMELSAD" = "county")
    ) %>%
  mutate(num_teachers = ifelse(is.na(num_teachers),0,num_teachers)) %>%
  sf::st_as_sf()

ggplot() +
  geom_sf(data = df_map, aes(fill = num_teachers)) +
  scale_fill_continuous(name = "Number of Teachers",
                        low = "white", high = "red", na.value = "gray90") +
  labs(
    title = "Number of Teachers by Parish in Louisiana"
  ) +
  theme_minimal() +
  theme(
    legend.position = "bottom",
    panel.grid = element_blank(),
    axis.text = element_blank(),
    axis.title = element_blank()
  ) +
  geom_point(data = top_cities, aes(x = long, y = lat)) +
  geom_text_repel(data = top_cities, aes(x = long, y = lat, label = city),
                  size = 3, color = "black")

```

### Study 1a

Zearn offers a variety of student and teacher activities, which, in our data, can be simplified through dimensionality reduction techniques, enhancing interpretability. We employed Independent Component Analysis (ICA) on teacher behavioral variables due to their non-Gaussian nature. This statistical approach allowed us to uncover latent variables that may have been obscured with traditional methods. These components are weighted vectors of specific teacher activities and were estimated to maximize statistical independence in the data-generating process. Our findings revealed three pivotal independent components, as indicated by the 'elbow' of @fig-scree, which represents the optimal number of components that best portray the underlying dimensions of teacher behavior. (For more details, please refer to the SI section.)

```{r ICA}

library(ica)

# Perform ICA to choose number of components
# Removing columns with zero variance

ICA_cols <- teacher_student_usage_subset %>%
  select(User.Session:RD.grade_level_teacher_materials) %>%
  select(where(~ sd(.x, na.rm = TRUE) != 0)) %>%
  mutate_all(scale)

imod.fast <- ica(ICA_cols, nc = 10)

# Create Elbow Plot
## SEAN: Fix plot to show integers in x-axis and % in y-axis
elbow_plot <- ggplot(data.frame(Component = 1:10, Variance = imod.fast$vafs),
                     aes(x = Component, y = Variance)) +
  geom_line(color = "gray") +
  geom_point() +
  labs(x = "Component", y = "Proportion of Variance Explained",
       title = "Elbow Plot for Determining Number of Components") +
  theme_minimal() + 
  scale_x_continuous(breaks=c(2,4,6,8,10)) +
  scale_y_continuous(labels = scales::percent)


# ICA via different algorithms

## SEAN: Check for the 3 methods:
## 1) http://research.ics.aalto.fi/ica/icasso/
## 2) https://www.cs.helsinki.fi/u/ahyvarin/code/isctest/
## We will choose the best method accordingly (most stable and interpretable)

imod.fast <- ica(ICA_cols, nc = 3)
# imod.imax <- ica(ICA_cols, nc = 3, method = "imax")
# imod.jade <- ica(ICA_cols, nc = 3, method = "jade")


# Iterate through each principal component
for (i in 1:ncol(imod.fast$M)) {
    # Find the index of the variable with the largest absolute coefficient
    largest_coef_index <- which.max(abs(imod.fast$M[, i]))

    # Check if the largest coefficient is negative
    if (imod.fast$M[largest_coef_index, i] < 0) {
        # Multiply all coefficients in this principal component by -1
        imod.fast$M[, i] <- -1 * imod.fast$M[, i]
        imod.fast$S[, i] <- -1 * imod.fast$S[, i]
    }
}

# Add ICA components to the dataset
teacher_student_usage_subset <- teacher_student_usage_subset %>%
  mutate(
    ic1 = imod.fast$S[, 1],
    ic2 = imod.fast$S[, 2],
    ic3 = imod.fast$S[, 3]
  )

```

```{r}
#| label: fig-scree
#| fig-cap: "Elbow (Scree) Plot for Determining Optimal Number of Components. The plot displays the proportion of variance explained by each independent component (IC). The optimal number of components is indicated by the 'elbow' of the plot, where the variance explained by each additional component is minimal."

elbow_plot

```

The significant finding from the ICA was the prominence of IC1, accounting for `r round(imod.fast$vafs[1]*100, 2)`% of the variance in teacher Zearn activity, as indicated in @fig-ica-heatmap. This result is significant in nudge engineering, highlighting the need to focus interventions on elements encapsulated by IC1. Conversely, IC2 and IC3, with `r round(imod.fast$vafs[2]*100, 2)`% and `r round(imod.fast$vafs[3]*100, 2)`% variance explained, play lesser but still noteworthy roles.

```{r}
#| label: fig-ica-heatmap
#| fig-cap: "Independent Component Analysis (ICA) Results. The heatmap displays teacher actions in each row, while the columns represent the three independent components (IC1, IC2, IC3) that explained the most variance in the teacher behavioral data, with their respective percentage of variance explained in parentheses. The color gradient on the heatmap indicates the relative importance of each activity within these components. Note that these metrics pertain to teacher activity on the platform, not student actions."

library(pheatmap)

# Add a row for the percentage of variance explained
ica_weights <- imod.fast$M

# Define row names
variable_names <- c(
  "User.Session" = "User Session",
  "RD.optional_problem_sets" = "Optional Problem Sets RD",
  "RD.student_notes_and_exit_tickets" = "Student Notes and Exit Tickets RD",
  "RD.mission_overview" = "Mission Overview RD",
  "RD.pd_course_notes" = "PD Course Notes RD",
  "RD.elementary_schedule" = "Elementary Schedule RD",
  "RD.whole_group_fluency" = "Whole Group Fluency RD",
  "Guided.Practice.Completed" = "Guided Practice Completed",
  "RD.small_group_lessons" = "Small Group Lesson RD",
  "Tower.Completed" = "Tower Completed",
  "Fluency.Completed" = "Fluency Completed",
  "Number.Gym.Activity.Completed" = "Number Gym Activity Completed",
  "RD.grade_level_overview" = "Grade Level Overview RD",
  "Tower.Stage.Failed" = "Tower Stage Failed",
  "Kindergarten.Activity.Completed" = "Kindergarten Activity Completed",
  "Tower.Struggled" = "Tower Struggled",
  "RD.k_mission" = "Kindergarten Mission RD",
  "RD.whole_group_word_problems" = "Whole Group Word Problems RD",
  "RD.assessments" = "Assessments RD",
  "RD.teaching_and_learning_approach" = "Teaching and Learning Approach RD",
  "RD.optional_homework" = "Optional Homework RD",
  "RD.k_schedule" = "Kindergarten Schedule RD",
  "RD.curriculum_map" = "Curriculum Map RD",
  "RD.assessments_answer_key" = "Assessments Answer Key RD",
  "RD.pd_course_guide" = "PD Course Guide RD",
  "RD.grade_level_teacher_materials" = "Grade Level Teacher Materials RD"
)
rownames(ica_weights) <- variable_names[names(ICA_cols)]

# Order rows by descending IC1
ica_weights <- ica_weights[order(-ica_weights[, 1],
                                 -ica_weights[, 2],
                                 -ica_weights[, 3]), ]

# Create the heatmap
pheatmap(ica_weights,
         cluster_rows = FALSE,
         cluster_cols = FALSE,
         color = colorRampPalette(c("white", "red"))(10),
         breaks = seq(-0.1, 1, length.out = 10),
         show_colnames = TRUE,
         show_rownames = TRUE,
         fontsize_row = 10,
         fontsize_col = 12,
         labels_col = c(
           sprintf("IC1 \n (%.1f%%)", imod.fast$vafs[1] * 100),
           sprintf("IC2 \n (%.1f%%)", imod.fast$vafs[2] * 100),
           sprintf("IC3 \n (%.1f%%)", imod.fast$vafs[3] * 100)
           ),
         angle_col = 0,
         cell_fun = function(j, i, x, y, width, height) {
             if (ica_weights[i, j] > 0.6) {
                 text(x, y, round(ica_weights[i, j], 2), col = "black", cex = 0.8)
             }
         })

```

The activities with the highest weighting in IC1 were measures associated with Towers (Struggled, Failed, and Completed) in conjunction with other completed student activities such as Fluency, Guided Practice, and Number Gym Activity. Notably, these metrics pertain to teacher logins, not student actions, implying a direct link between teacher engagement and student performance, suggesting an educational "scaffolding" effect.

Further examination revealed an empathy-driven process behind this trend. In this context, "empathy" refers to teachers' ability to comprehend and engage with students' challenges, as demonstrated by their focused attention on areas where students struggled or succeeded. The weightings in IC1 for activities like Tower Struggles (`r round(ica_weights["Tower Struggled",1], 3)`), Fluency Completed (`r round(ica_weights["Fluency Completed",1], 3)`), Guided Practice Completed (`r round(ica_weights["Guided Practice Completed",1], 3)`), and Number Gym Activity Completed (`r round(ica_weights["Number Gym Activity Completed",1], 3)`) were significantly higher compared to other activities. This pattern underlines the empathy-driven process in teaching, where teachers' engagement with the platform is focused on understanding student challenges.

The second component, IC2, showed substantial weightings on variety of Resource Downloads (RD), particularly Small Group Lessons (`r round(ica_weights["Small Group Lesson RD",2], 3)`) and Whole Group (i.e., the entire class) Word Problems (`r round(ica_weights["Whole Group Word Problems RD",2], 3)`). We labeled this component as "Groups." IC3, with strong weights on Resource Download (RD) for the Professional Development (PD) course guide (`r round(ica_weights["PD Course Guide RD",3], 3)`) and course notes (`r round(ica_weights["PD Course Notes RD",3], 3)`), was labeled "Guidance." This component, accounting for a smaller variance (`r round(imod.fast$vafs[3]*100, 3)`%), was harder to interpret due to its complexity and diverse activities.

```{r plm-regression}

library(plm)

# Define the columns to be used in the formula
columns_used <- colnames(teacher_student_usage_subset %>%
                           select(teacher_number_classes,
                                  Students...Total,
                                  Grade.Level,
                                  # MDR.School.ID,
                                  ic1:ic3))

# Create the formula
fmla_str <- sprintf("logBadges ~ %s",
                    paste(sprintf("`%s`", columns_used), collapse = " + "))
fmla <- as.formula(fmla_str)

# Define the panel model index
pUsage <- pdata.frame(
  teacher_student_usage_subset %>%
    arrange(Classroom.ID, Adult.User.ID, year, week) %>%
    mutate(Classroom.ID = factor(Classroom.ID, ordered = FALSE),
           Adult.User.ID = factor(Adult.User.ID, ordered = FALSE),
           Usage.Week = as_date(Usage.Week),
           # MDR.School.ID = factor(MDR.School.ID, ordered = FALSE),
           # District.Rollup.ID = factor(District.Rollup.ID, ordered = FALSE),
           Grade.Level = factor(Grade.Level, ordered = TRUE),
           medianIncome = factor(medianIncome, ordered = TRUE, exclude = ""),
           povertyLevel = factor(povertyLevel, ordered = TRUE, exclude = ""),
           zipcode = factor(zipcode, ordered = FALSE),
           logBadges = log(Badges.per.Active.User + 1)),
  index = c("Classroom.ID", "Usage.Week", "Adult.User.ID"))

# 'Within' model
ica_fe_model <- plm(fmla, data = pUsage, model = "within")

# Hausman Test supports Fixed Effects
ica_re_model <- plm(fmla, data = pUsage, model = "random")
# phtest(ica_fe_model, ica_re_model)
# 
# data:  fmla
# chisq = 176.92, df = 4, p-value < 2.2e-16
# alternative hypothesis: one model is inconsistent


# Subsample model
pUsage <- pUsage %>% filter(schoolAccount == 1, curriculum == 1)

subsample_model <- colnames(teacher_student_usage_subset %>%
                           select(teacher_number_classes,
                                  Students...Total,
                                  Grade.Level,
                                  medianIncome,
                                  # MDR.School.ID,
                                  ic1:ic3))
fmla_str <- sprintf("logBadges ~ %s",
                    paste(sprintf("`%s`", subsample_model), collapse = " + "))
fmla <- as.formula(fmla_str)

ica_fe_subsample <- plm(fmla, data = pUsage, model = "within")

```

Building upon these findings, we advanced to a censored (lower bound = 0) panel regression model that accounted for various temporal and subject-specific variables and the potential impact of teachers handling multiple classes. This model applied a first-difference approach with a lower bound of zero for the dependent variable. The regression formula incorporated changes in independent components (IC1, IC2, IC3) as predictors for the change in the logarithm of badges (+1), accounting for individual teachers, classes, and weeks. As presented in @tbl-plm, the regression highlighted a strong positive contemporaneous correlation between IC1 and badges, with a coefficient of `r round(summary(ica_fe_model)$coefficients["ic1","Estimate"], 4)` (p \< `r round(summary(ica_fe_model)$coefficients["ic1","Pr(>|t|)"], 4)`), suggesting a significant impact on student achievement.

```{r}
#| label: tbl-plm
#| tbl-cap: "Regression of Log Badges on the Independent Components. This table displays the results of a censored panel regression model with a lower bound of 0 for the dependent variable. Standard errors are clustered at the teacher level."

# MARCOS: Add "intercept"
ica_fe_results <- summary(ica_fe_model,
                      vcov = vcovHC(ica_fe_model,
                                    type = "HC3",
                                    cluster = "group")
                      )$coefficients

# Convert to a gt table
gt_table <- ica_fe_results %>%
  as.data.frame() %>%
  rownames_to_column("Variable") %>%
  gt() %>%
  cols_label(
    Variable = "Variable",
    Estimate = "Coefficient",
    `Std. Error` = "Std. Error",
    `t-value` = "t value",
    `Pr(>|t|)` = "P Value"
  ) %>%
  fmt_number(
    columns = c(Estimate, `Std. Error`, `t-value`, `Pr(>|t|)`),
    decimals = 4
  )

# Print the table
gt_table

```

```{r}
#| label: tbl-plm-subsample
#| tbl-cap: "Regression of Log Badges on the Independent Components. This table displays the results of a censored panel regression model with a lower bound of 0 for the dependent variable. Standard errors are clustered at the teacher level."

# MARCOS: Add "intercept"
ica_fe_subsample_results <- summary(ica_fe_subsample,
                      vcov = vcovHC(ica_fe_subsample,
                                    type = "HC3",
                                    cluster = "group")
                      )$coefficients

# Convert to a gt table
gt_table <- ica_fe_subsample_results %>%
  as.data.frame() %>%
  rownames_to_column("Variable") %>%
  gt() %>%
  cols_label(
    Variable = "Variable",
    Estimate = "Coefficient",
    `Std. Error` = "Std. Error",
    `t-value` = "t value",
    `Pr(>|t|)` = "P Value"
  ) %>%
  fmt_number(
    columns = c(Estimate, `Std. Error`, `t-value`, `Pr(>|t|)`),
    decimals = 4
  )

# Print the table
gt_table

```


The correlation we discovered has significant implications. It indicates that an increase in activities associated with IC1, such as encountering one more 'Tower Struggle' than average, can lead to an approximate `r round(summary(ica_fe_model)$coefficients[c("ic1","ic2","ic3"),"Estimate"] %*% ica_weights["Tower Struggled",1:3] / sd(teacher_student_usage_subset$Tower.Struggled) *100, 2)` percent increase in student badges. Although this increase may seem small, it may be substantial in the context of complex human behaviors, where even modest changes often lead to far-reaching effects. This observation highlights the potential of targeted interventions based on our findings to yield significant improvements in educational outcomes.

### Study 1b

Prompted by @buyalskaya2023, we sought to uncover the subtleties of habitual behaviors within an educational setting. Our primary goal was to understand how regular teacher interactions with the Zearn platform impacted student learning outcomes. To measure this, we used the log-transformed average weekly badges per student over the entire school year as our dependent variable. We constructed our explanatory variables with careful consideration of the patterns that could identify habitual engagement and their relationship to student performance:

1.  Login Percentages across months and days of the week: The frequency of logins across different time periods. For example, among all the logins for a teacher, we assess how many are from January or Tuesdays, compared to all other months and days of the week, respectively. Note that our regression omitted July and Sunday, periods with low login incidence.

2.  Time Spent on Zearn: Teachers' weekly average time spent on the platform, measured in minutes.

3.  Average Streak: Average number of consecutive weekdays in which the teacher logs in.

4.  Average Days Between Logins: Average number of days between two login instances.

To account for any school-specific factors that may have influenced the relationship between teacher behavior and student achievement, we estimated a school-fixed-effects regression model. Unlike study 1, this regression did not follow each teacher or class across weeks, as our unit of analysis was a teacher summed across classrooms and averaged across all weeks.

The regression results, as detailed in @tbl-habit-regression (see Table S2 for full results), revealed that all weekday login percentages positively affected student badges. However, Friday logins stood out significantly, supporting our hypothesis that specific days of the week have more influence on habitual engagement.

```{r habit-regression}

# Read in the data
df <- fread(file = "./Data/df_habits.csv")

df_transformed <- df %>%
  lazy_dt() %>%
  filter(Adult.User.ID %in%
           unique(teacher_student_usage_subset$Adult.User.ID)) %>%
  mutate(dayofweek = factor(dayofweek, ordered = FALSE,
                            levels = c("Mon", "Tue", "Wed", "Thu",
                                       "Fri", "Sat", "Sun")),
         Month = factor(Month, ordered = FALSE),
         week_label = isoweek(Date)) %>%
  pivot_wider(names_from = dayofweek, values_from = logged, 
              values_fill = list(logged = 0)) %>%
  group_by(Adult.User.ID, Week) %>%
  summarize(across(c("Mon", "Tue", "Wed", "Thu", "Fri", "Sat", "Sun"),
                   sum, na.rm = TRUE),
            Total_Minutes_on_Zearn = sum(Minutes.on.Zearn...Total, na.rm = TRUE),
            mean_streak = mean(streak, na.rm = TRUE),
            mean_streak_dow = mean(streak_dow, na.rm = TRUE),
            mean_visits = mean(mean_visits, na.rm = TRUE),
            total_visits = mean(total_visits, na.rm = TRUE),
            mean_time_lag = mean(time_lag, na.rm = TRUE),
            Month = first(Month),
            Year = first(Year),
            week_label = first(week_label),
            .groups = 'drop')

# Pivot longer to aggregate by month
df_monthly <- df_transformed %>%
  pivot_longer(cols = Mon:Sun, names_to = "dayofweek", values_to = "logged") %>%
  group_by(Adult.User.ID, Month) %>%
  summarize(total_logged_per_month = as.double(sum(logged, na.rm = TRUE))) %>%
  # Pivot wider to create a column for each month
  pivot_wider(names_from = Month, values_from = total_logged_per_month,
              names_prefix = "month_",
              values_fill = list(total_logged_per_month = 0)) %>%
  mutate(row_total = rowSums(select(., starts_with("month_")))) %>%
  mutate(across(starts_with("month_"), ~ ./row_total * 100)) %>%
  select(-row_total)

feUsage <- pdata.frame(
  df_transformed %>%
    group_by(Adult.User.ID) %>%
    summarize(across(c(Mon:Sun), sum, na.rm = TRUE),
              across(c(Total_Minutes_on_Zearn:mean_time_lag), mean, na.rm = TRUE),
              total_weeks = n_distinct(Week),
              .groups = 'drop') %>%
    mutate(row_total = rowSums(select(., c(Mon:Sun)))) %>%
    mutate(across(c(Mon:Sun), ~ ./row_total * 100)) %>%
    select(-row_total) %>%
    inner_join(df_monthly, by = "Adult.User.ID") %>%
    inner_join(teacher_student_usage_subset %>%
                 lazy_dt() %>%
                 group_by(Adult.User.ID, week, MDR.School.ID) %>%
                 summarize(Badges.per.Active.User = sum(Badges.per.Active.User),
                           Year = first(year),
                           .groups = 'drop') %>%
                 group_by(Adult.User.ID, MDR.School.ID) %>%
                 summarize(Badges.per.Active.User = mean(Badges.per.Active.User),
                           .groups = 'drop'),
               by = c("Adult.User.ID")
               # by = c("Adult.User.ID" = "Teacher.User.ID",
               #        "week_label" = "week", "Year")
               ) %>%
    mutate(Adult.User.ID = factor(Adult.User.ID, ordered = FALSE),
           MDR.School.ID = factor(MDR.School.ID, ordered = FALSE),
           logBadges = log(Badges.per.Active.User + 1)) %>%
    as_tibble(),
  index = c("MDR.School.ID"))
  # index = c("Adult.User.ID", "Week", "MDR.School.ID"))

habits_fe_model <- plm(Badges.per.Active.User ~ Mon + Tue + Wed + Thu + Fri + Sat +
                     month_1 + month_2 + month_3 + month_4 + month_5 +
                     month_9 + month_10 + month_11 +
                     Total_Minutes_on_Zearn + mean_streak + mean_time_lag,
                   data = feUsage, model = "within")
# lm(Badges.per.Active.User ~ Mon + Tue + Wed + Thu + Fri + Sat +
#                      month_1 + month_2 + month_3 + month_4 + month_5 +
#                      month_9 + month_10 + month_11 +
#                      Total_Minutes_on_Zearn + mean_streak + mean_time_lag +
#                      mean_streak_dow + total_visits +
#                      factor(MDR.School.ID), data = feUsage)
# habits_fd_model <- pldv(logBadges ~ Mon + Tue + Wed + Thu + Fri + Sat + Sun + Month +
#                      Total_Minutes_on_Zearn + mean_streak + mean_streak_dow +
#                      mean_visits + mean_time_lag,
#                     data = feUsage, model = "fd", objfun = "lsq",
#                     lower = 0, sample = "cens")

# summary(habits_fe_model,vcov = vcovHC(habits_fe_model, type = "HC3", cluster = "group"))

# Hausman Test supports Fixed Effects
# habits_re_model <- plm(logBadges ~ Mon + Tue + Wed + Thu + Fri + Sat +
#                      month_1 + month_2 + month_3 + month_4 + month_5 + month_6 +
#                      month_8 + month_9 + month_10 + month_11 + month_12 +
#                      Total_Minutes_on_Zearn + mean_streak + mean_streak_dow +
#                      mean_visits + mean_time_lag,
#                    data = feUsage, model = "random", effect = "individual")
# phtest(habits_fe_model, habits_re_model)
# 
# data:  logBadges ~ Mon + Tue + Wed + Thu + Fri + Sat + month_1 + month_2 +  ...
# chisq = 227.82, df = 22, p-value < 2.2e-16
# alternative hypothesis: one model is inconsistent

```

```{r}
#| label: tbl-habit-regression
#| tbl-cap: "Regression of Log Average Badges on Habit Variables. This table displays the results of the regression model with school fixed effects. Coefficients on months and days measure the difference between the effects of login percentage relative to the July percentage (for months) and the Sunday percentage (for days). Standard errors are clustered at the school level."

habits_fe_results <- summary(habits_fe_model,
                      vcov = vcovHC(habits_fe_model,
                                    type = "HC3",
                                    cluster = "group")
                      )$coefficients

# Convert to a gt table
gt_table <- habits_fe_results %>%
  as.data.frame() %>%
  rownames_to_column("Variable") %>%
  mutate(Variable = case_when(
    Variable == "Mon" ~ "`%`Monday",
    Variable == "Tue" ~ "`%`Tuesday",
    Variable == "Wed" ~ "`%`Wednesday",
    Variable == "Thu" ~ "`%`Thursday",
    Variable == "Fri" ~ "`%`Friday",
    Variable == "Sat" ~ "`%`Saturday",
    Variable == "month_1" ~ "`%`January",
    Variable == "month_2" ~ "`%`February",
    Variable == "month_3" ~ "`%`March",
    Variable == "month_4" ~ "`%`April",
    Variable == "month_5" ~ "`%`May",
    # Variable == "month_6" ~ "`%`June",
    # Variable == "month_8" ~ "`%`August",
    Variable == "month_9" ~ "`%`September",
    Variable == "month_10" ~ "`%`October",
    Variable == "month_11" ~ "`%`November",
    Variable == "month_12" ~ "`%`December",
    Variable == "Total_Minutes_on_Zearn" ~ "Avg. Minutes",
    Variable == "mean_streak" ~ "Avg. Streak",
    Variable == "mean_time_lag" ~ "Avg. Days \n Between Logins",
    TRUE ~ Variable  # Default case
    )) %>%
  gt() %>%
  fmt_markdown(columns = TRUE, rows = TRUE) %>%
  cols_label(
    Variable = "Variable",
    Estimate = "Coefficient",
    `Std. Error` = "Std. Error",
    `t-value` = "t value",
    `Pr(>|t|)` = "P Value"
  ) %>%
  fmt_number(
    columns = c(Estimate, `Std. Error`, `t-value`, `Pr(>|t|)`),
    decimals = 4
  )

# Print the table
gt_table

```

```{r friday-example}
#| eval: false

mean(feUsage[,13]) *                              # Total Teacher Logins 
  (colMeans(as.matrix(feUsage[,c(2:6)])/100) -    # Times weekday percentages
     0.01281 * c(-1/4, -1/4, -1/4, -1/4, 1)) /    # Adjustment so Fri = 1/4
  mean(feUsage[,15])                              # Normalized by Total Weeks
#       Mon       Tue       Wed       Thu       Fri 
# 0.3104233 0.3284907 0.3240100 0.3174472 0.2500070 

mean(feUsage[,13]) *
  (colMeans(as.matrix(feUsage[,c(2:6)])/100) +
     0.1402 * c(-1/4, -1/4, -1/4, -1/4, 1)) /     # Adjustment so Fri = 1/2
  mean(feUsage[,15])
#       Mon       Tue       Wed       Thu       Fri 
# 0.2479156 0.2659830 0.2615024 0.2549396 0.5000375 

# 0.1402 - (-0.01281) = 0.1530

```

In particular, our analysis indicates that shifting 10% of logins from other weekdays to Fridays, without increasing overall platform usage, could boost student lesson completion by around `r round(10 * c(-1/4, -1/4, -1/4, -1/4, 1) %*% habits_fe_model$coefficients[1:5] * 100, 2)`%. For the typical teacher, this means switching from one Friday login per month to two while reducing one login from another weekday during that month, resulting in an increase of `r round(15.30 * c(-1/4, -1/4, -1/4, -1/4, 1) %*% habits_fe_model$coefficients[1:5] * 100, 2)`% in average weekly lesson completion. This outcome underscores the importance of strategic engagement rather than just login frequency.

We attribute this pronounced effect to two key factors. First, Friday logins facilitate "Reflective Catch-Up," enabling teachers to review and analyze the previous week's activities and make necessary adjustments. Second, "Foresighted Planning" occurs on Fridays as teachers proactively plan for the upcoming week, a practice less common on weekends.

## Study 2: Nudge Engineering - From Data to Intervention Design

In Study 1, we discovered that specific psychopedagogical strategies, habits, and timing of teacher interactions were crucial in promoting student success on digital platforms. With this foundation, we could craft effective educational interventions and strategies. By leveraging the insights from the first study, we aimed to develop targeted interventions that maximize student learning outcomes.

### Study 2a

We used the first component from the ICA in Study 1a to design an "empathy" nudge. This intervention involved sending emails to teachers, encouraging them to adopt a more student-centered teaching approach by viewing math problems from their students' perspective. Our analysis in Study 1a indicated that this process was linked to the highest weighted behaviors in IC1. The emails contained key messages emphasizing the importance of empathy in teaching math, advice from other teachers, and helpful tips for assisting students who struggle with a lesson. The emails also suggested specific actions, including using Zearn's features to view lessons from a student's perspective and checking the Tower Alerts Report (see SI for the complete emails). We hypothesized that this empathy approach would significantly enhance student performance.

### Study 2b

In Study 2b, we aimed to test whether nudging teachers to log in on Fridays, as opposed to an unspecified day, would improve student performance. Our approach involved sending emails to teachers, encouraging them to log in on Fridays, highlighting effective teaching habits, and emphasizing the benefits of regular check-ins on student progress. These emails included testimonials from other teachers, research insights, and motivational messages to encourage habit formation (see SI for the complete emails). Our rationale was that Friday logins would aid in reflecting on the week's activities and proactive planning for the following week.

In addition, we designed a control email tailored to this treatment. These emails were sent every Wednesday to remind Zearn teachers to review their Zearn Pace Report without the personalized behavioral prompts or motivational materials in the Friday treatment group emails. Although the control emails prompted participation with Zearn, they did not provide information on Friday logins, introspection, or strategic preparation.

We hypothesized that the Friday approach to teacher engagement with the platform would yield a greater impact on student achievement than the Wednesday control or our study-wide control.

### Impact Evaluation

<!-- SEAN: please double check these stats -->

In collaboration with Zearn, Study 2 became part of a large multi-arm âmegastudy,â set in motion during a critical four-week period in 2021, involving over 140,000 teachers and nearly 3 million students. Teachers in our study taught a median of 20 students (mean = 21.30, SD = 15.31) in a median of 1 classroom (mean = 1.15, SD = 0.59). Before the intervention, teachers in our study had, on average, logged into Zearn a total of 3.62 times between July 1, 2021, and September 14, 2021 (SD = 8.49) (see SI for a complete description of the sample).

```{stata}
#| include: false

*********************************************************
* Adapted from:                       							    *
* Title: Zearn Main and Per Protocol Analyses				    *
* Author: Ramon Silvera Zumaran		      						    *
* Date: 13 Jul 2023										   	              *
*********************************************************

clear all
set more off
set type double
* ssc install blindschemes
set scheme plotplain

*************************
**# Defining Programs #**
*************************

* Data Transformation and BH Adjustment Program
/* This program performs a Benjamini-Hochberg Adjustment for multiple hypothesis
   testing and summarizes the original results alongside the adjusted p-values */

cap drop bhadjustment

program define bhadjustment
	args cond_num
	
	matrix results = r(table)
	local colnames : colfullnames results
	local colnum : colsof results
	local cond_num = `cond_num'
	
	gen category_num = .
	gen category = ""
	gen coef = .
	gen standard_error = .
	gen p_val = .
	gen p_val_BH = .
	
	forvalues j = 1/`cond_num' {
		levelsof condition_cat if condition == `j', local(level) clean
		replace category_num = `j' in `j'
		replace category = "`level'" in `j'
		replace coef = results[1,`j'] in `j'
		replace standard_error = results[2,`j'] in `j'
		replace p_val = results[4,`j'] in `j'
	}
	
	keep category_num-p_val
	
	drop if mi(category)
	
	sort category_num
	drop category_num
	
	sort p_val
	gen rank = _n

	gen p_val_BH = p_val*(`cond_num'/rank)

	forvalues i = 1/`cond_num' {
		local num = `cond_num'+1
		local ind = `num'-`i'
		if p_val_BH[`ind'+1] < p_val_BH[`ind'] {
			replace p_val_BH = p_val_BH[`ind'+1] in `ind'
		}
	}
	
	replace p_val_BH = round(p_val_BH, 0.001)

	drop rank
	
	list

end

***************************************
**# Defining globals for covariates #**
***************************************
/* These are our pre-registered covariates for our primary DV regressions. */

* Main Analysis: Badges
global badges_covs perc_G1 perc_G2 perc_G4 perc_G5 perc_G6 perc_G7 perc_G8 is_free teacher_badges_pre pre_intv_logins total_associated_students n_classes acct_age missing_classroom_grade opened_weekneg1 opened_week0

********************************************
**# Primary and Secondary DV Regressions #**
********************************************

* Student Math Performance
use "Data/megastudy_clean.dta", clear
// Pre-registered regression
quietly areg teacher_badges_during ib16.condition $badges_covs [aweight=total_associated_students], absorb(combined_school_id)

quietly putexcel set study2regress.xlsx, replace

matrix coef = r(table)'
putexcel A1 = matrix(coef), names

quietly putexcel save

```

```{stata}
#| eval: false

quietly areg teacher_badges_during ib16.condition $badges_covs [aweight=total_associated_students], absorb(combined_school_id)
predict xb

# Regression-estimated average number of lessons completed by students in the control condition:
sum xb if 16.condition == 1
quietly sum xb if condition == 7 | condition == 8 | condition == 8
gen diff = r(mean) - 1.76114

// Wald test for checking whether friday and weekly treatment are equal 
test 9.condition-8.condition=0
local sign_car = sign(_b[8.condition]-_b[7.condition])
display "H_1: Wednesday >= Friday p-value = " normal(`sign_car'*sqrt(r(F)))

// Cohen's d
esize twosample xb if condition == 8  |  condition == 16, by(condition)
esize twosample xb if condition == 9  |  condition == 16, by(condition)
esize twosample xb if condition == 8  |  condition == 9, by(condition)


```

```{r}
#| label: tbl-megastudy-regression
#| tbl-cap: "Efficacy of Different Teacher Engagement Interventions on Student Learning Outcomes. The table showcases the effects of the 'Empathy' and 'Friday Login' interventions compared to the study-wide control, along with the results from the Friday-specific control group. We measure student achievement by the number of badges students earned."

megastudy_model <- readxl::read_xlsx("study2regress.xlsx")
names(megastudy_model)[1:5] <- c("Treatment", "Coefficient",
                                 "Std. Error", "t value", "P Value")
megastudy_model[c(7:9,33),1] <- c("Empathy", "Friday",
                                  "Friday-control", "Intercept")
unlink("regress.xlsx")

knitr::kable(megastudy_model[c(7:9,33),1:5], digits = 4)

```

As stated in our preregistration, we evaluated the impact of intervention messages on the number of math lessons completed by students during the four-week intervention period. Students in the megastudy control condition completed a regression-estimated 1.761 lessons during the 4-week intervention period. @tbl-megastudy-regression shows that our interventions increased the number of math lessons completed by students during the intervention period by a regression-estimated average of 0.0487 lessons, which is a 2.77% increase over the megastudy control condition. Specifically, the empathy treatment increased the number of lessons completed by students by 0.0721 lessons, or a 4.09% increase over the megastudy control condition (d = 0.0188, p = .018). The Friday treatment increased the number of lessons completed by students by 0.0550 lessons, or a 1.81% increase over the megastudy control condition (d = 0.0192, p = .068). The Friday treatment was not significantly different from the Friday-specific control (F(1,118137) = 0.85, p = .357).

Table S3 presents unstandardized coefficients from our primary regression analysis and unadjusted, robust SEs and CIs. Additionally, we utilized the Benjamini-Hochberg (BH) procedure to compute adjusted p-values, which help to control for false discovery rates when conducting multiple comparisons [@benjamini1995]. Before adjusting for multiple hypothesis testing, all treatments exhibited significant benefits. However, only our treatment-specific control had a BH-adjusted p-value of less than 0.05. This intervention involved encouraging teachers to log in to Zearn weekly to receive updated student performance reports. Although reliable, the effect of this intervention was small, resulting in an estimated 0.0898 extra lessons completed in four weeks, or a 5.10% increase over the megastudy control (d = 0.0235, p = .003). Even after applying the James-Stein shrinkage procedure to adjust for the winner's curse (i.e., the maximum of 15 estimated effects is upward biased), we estimated that this intervention still produced 0.061 extra lessons completed, or a 3.46% increase over the control condition [@james1992].

# Discussion

This study aimed to improve student math learning on the Zearn platform by integrating data analysis into educational intervention. We identified critical teacher behaviors influencing student performance and evaluated two novel interventions: empathy and Friday habitization.

Our first study revealed subtle but significant patterns in teacher engagement that traditional analyses might overlook. Study 1a identified a significant independent component strongly associated with metrics related to struggles and achievements, suggesting teachers' empathy-driven engagement, focusing on areas where students faced challenges or succeeded. This pattern also correlated with a significant increase in student achievement, aligning with findings emphasizing the importance of teacher empathy in educational outcomes [@hill2008; @battey2016]. In Study 1b, we discovered that teachers who logged into Zearn on Fridays had a notable impact on student math performance. This behavior indicated a commitment to continuous planning and support, echoing the findings of @blanchard2016 that technology integration does not need large-scale changes in practices to enhance student learning. Overall, our research highlights the importance of teacher engagement and personalized instruction and feedback in improving student performance on the Zearn platform. As @ertmer2012 have emphasized, aligning student-centered beliefs and practices is vital to success, regardless of technological, administrative, or assessment barriers.

In Study 2a, the success of the empathy intervention can be attributed to its alignment with psychological principles that emphasize the importance of emotional connectivity between teachers and students. This intervention likely fostered a more engaging and supportive learning environment, a feature essential for the success of digital platforms [@lavecchia2016; @koch2015]. In contrast, Study 2b's unexpected results highlight the complexities of behavior change in educational settings, suggesting that repetitive routines without meaningful engagement or context may not enhance learning outcomes. Notably, our study-specific control outperformed all other megastudy interventions. Initial analyses from Duckworth et al. (2024) suggest that this effect is due to the higher salience of personalization present, such as the suggestion of classroom-specific actions (e.g., "CLICK HERE to see which of your students are struggling"). The lack of additional content in this control highlighted actionable steps to engage with students, an effect which, in retrospect, aligns with previous literature.

Our study's insights transcend the immediate context. It showcases the potential of data-driven interventions to create strategies that cater to the unique needs of teachers and students. This approach paves the way for personalized and responsive pedagogy. In the realm of digital learning, our findings underscore the pivotal role of teacher engagement and tailored content, which are crucial for replicating and enhancing the benefits of traditional classrooms. In essence, our research provides a roadmap for designing online educational tools that are more effective and engaging.

Our study, while insightful, is limited by its focus on a specific demographic and educational context within the Zearn platform. It also only partially captures teacher-student interactions in traditional classroom settings. Consequently, generalizing our results to other educational settings, cultures, or age groups may be challenging. Additionally, while our approach was more cost-effective and less time-intensive, it achieved a more modest impact than the substantial effects seen in more intensive programs [@banerjee2007; @dipietro2023]. The simplicity of our email interventions and the short duration of the study likely contributed to these results, although their magnitude aligns with other reports from educational technology applications [@cheung2013]. Future research should explore more engaging and intensive intervention methods over extended periods to potentially yield greater impacts on learning outcomes.

While rooted in education data, our research introduces a paradigm with significant implications beyond its primary focus. By combining data-driven analysis with targeted behavioral interventions, our approach offers a flexible framework that can be adapted to various fields. Whether in healthcare, environmental behavior, or organizational management, our methodology demonstrates the potential to harness data insights for effective behavioral change. Hence, our study also serves as a catalyst for innovative approaches in diverse fields where behavior modification is crucial.

{{< pagebreak >}}

# Materials and Methods

## Study 1

### Data Collection

The Zearn math educational learning platform provided us with administrative data spanning the 2019-2020 academic year (September 2019 to May 2020). This dataset included detailed teacher actions and classroom-week-level student achievement metrics. Teacher actions on the platform were timestamped to the second. For privacy considerations, Zearn aggregated student data at the classroom-week level, including student achievement measures and indications of student struggles. The dataset covered various schools across Louisiana (@fig-teachers-map).

To promote transparency and replicability of our study, we have deposited the data and code used in our analyses in a publicly accessible database. Researchers and interested parties can access the complete dataset and all related processing scripts at the GitHub repository: <https://github.com/SeanHu0727/zearn_nudge.git>

### Inclusion Criteria

We aggregated teacher behavior data to the weekly level and merged it with the student data at the classroom-week level. We also excluded inactive teachers (those with no recorded activity for over two months) from the dataset.

We defined the inclusion criteria for classrooms strictly as such:

1.  Classrooms linked to a single teacher

2.  Classrooms with no more than seven months of inactivity during the academic year

3.  Classrooms with an average of no less than five actively engaged students

## Study 1a

### Independent Component Analysis (ICA)

We extracted all teacher behavioral variables from the dataset that displayed non-zero variance and standardized them to have a zero mean and unit variance. To determine the ideal number of independent components, we performed ICA using a range of components from 1 to 10. Our decision on the optimal number was informed by recognizing the 'elbow' on the scree plot generated from the results, yielding three independent components. We used the `icafast` function from the R `ica` package for all ICAs conducted [@helwig2022].

### Panel Regression

We estimated a censored panel regression model using a first difference approach, with the `pldv` function from the `plm` package [@croissant2008] in R and a lower bound of 0 for the dependent variable:

$$
\ln (\text{Badges}_{itc} + 1) = \beta_1 \text{IC1}_{it} + \beta_2 \text{IC2}_{it} + \beta_3 \text{IC3}_{it} + \epsilon_{itc}
$$

where $i, c, t$ index the teacher, class, and week, respectively. Standard errors were clustered at the teacher level with the `vcovHC` function [@millo2017] in R.

## Study 1b

### Fixed-Effects Regression

We estimated a fixed effects model using the `plm` function[@croissant2008] in R:

```{=tex}
\begin{align*}
\ln \left( \sum_{c=1}^{C_i} \text{(Avg. Badges)}_{c} + 1 \right) = & \ \beta_0 + \sum_{m=1, m \neq 7}^{12} \beta_m \text{Login\%}_{m,i} + \sum_{d=1, d \neq 7}^{7} \beta_{12+d} \text{Login\%}_{d,i} \\
& + \beta_{19} \text{(Avg. Minutes)}_i + \beta_{20} \text{(Avg. Streak)}_i \\
& + \beta_{21} \text{(Avg. Days Between Logins)}_i + \text{School FEs} + \epsilon_i
\end{align*}
```
for teacher $i$ with $C_i$ classes. `Login%` represents the percentage of logins by teacher $i$ during each month $m$ and on each day of the week $d$ relative to other months and days, respectively. We exclude July and Sunday to prevent multicollinearity. Standard errors are clustered by school with the `vcovHC` function [@millo2017] in R.

## Study 2

We used the findings from Study 1 to inform the creation of two interventions as part of a larger multi-arm "mega study" that involved 15 sets of nudges.

### Implementation

Study 2 was conducted in collaboration with Zearn [@zearnma2023] and was preregistered for the fall of 2021. To incentivize teacher participation during our "intervention period" from September 15, 2021, to October 12, 2021, all teachers on the platform received two messages on September 1 and 8, 2021. These messages informed them they had been enrolled in the "Zearn Math Giveaway" and that every email opened until October 12, 2021, would earn them tickets. These tickets were used to enter drawings for various prizes, such as autographed children's books, stickers, and gift cards.

### Data

We excluded Zearn elementary school teachers from the study if they (a) taught grades other than first through eighth ($n=X$), (b) lacked a valid email address in the Zearn system as of September 8, 2021 ($n=X$), (c) had less than one or more than 150 students associated with their Zearn account as of October 18, 2021 ($n=X$), (d) had fewer than one or more than six classrooms associated with their Zearn account as of DATE ($n=X$), (e) had other teachers associated with their Zearn classroom(s) as of DATE ($n=X$), or (f) had not logged onto the Zearn platform (or had no associated student who logged onto the platform) between March 1, 2021 and September 14, 2021 ($n=X$).

After exclusions, we randomized $N = 140,461$ teachers across 22,281 schools who served 2,992,077 students in 161,722 classrooms into one of the intervention conditions or the control condition ($N_{\text{control}} = 29,513$). The control condition was larger than the interventions to account for multiple comparisons. Among $n = 16,372$, or $11.66\%$, of teachers, at least one of two problems occurred in the emails sent by Zearn Math during the intervention period: an email message that was intended but not sent ($n = 13,568$, or $9.66\%$ of teachers), or an email message that was sent but not intended (i.e., from a different treatment condition; $n = 2,804$, or $2.00\%$ of teachers). As these email problems were systematically related to treatment assignment ($\chi^2 = 33.01$, $df = 15$, $p = .005$), we did not exclude these participants and conducted intent-to-treat analyses. Refer to the SI for email problem prevalence by condition and study analyses that exclude or adjust for email problems, respectively.

### Impact Assessment

We followed our preregistered analysis plan to assess the effect of each treatment on the primary outcome of interest: math lessons completed by students during our four-week intervention period [@gallo2022a; @gallo2022b]. We estimated a weighted ordinary least squares (OLS) regression with the `areg` command in Stata [@statacorp2023]. Each teacherâs observations were weighted proportionally to the total number of students in their Zearn classroom(s).

The primary predictors were indicators for each intervention, omitting the control condition. The regression also included the following control variables: (1) school fixed effects, (2) an indicator for the teacher's account type (free or paid), (3) the number of times the teacher logged into Zearn prior to the study, from August 1 to September 14, 2021, (4) the total number of students in the teacher's classroom(s) as of October 18, 2021, (5) the number of classrooms associated with the teacher as of October 18, 2021, (6) the number of days since the teacher obtained a Zearn account prior to the study's launch, (7) the number of days separating the study's launch and the start of the teacher's school year, (8) the average number of lessons completed by a teacher's students from the start of their school year to the start of the intervention (or from July 14, 2021, if the school year start was not known), (9) whether the teacher opened our September 1, 2021 email announcing the upcoming Zearn Math Giveaway, (10) a similar indicator for our September 8, 2021, email reminding them of the giveaway, and (11) the percentage of a teacher's students in each grade except for third grade to avoid multicollinearity, since for most teachers, students were in a single grade.

### Study 2a: Empathy-Based Intervention Design and Implementation

We designed four emails highlighting empathy and encouraging teacher engagement, specifically regarding pedagogical content knowledge. A group of 7,443 teachers were chosen at random to receive these emails.

### Study 2b: Habitization-Based Intervention Design and Implementation

We designed four emails to be sent on Fridays, emphasizing the importance of regular Friday logins and student progress tracking. A total of 7,476 teachers were randomly selected to receive these emails. Additionally, we created a condition-specific control group that received messages on Wednesdays without Friday-specific content but with links to specific actions on Zearn. A group of 7,577 teachers were randomly chosen to participate in this control group.

## Ethical Considerations and IRB Approval

This study was conducted in accordance with ethical standards and received exempt status from the Institutional Review Board (IRB) at the University of Pennsylvania. The study's methodologies were designed to ensure the confidentiality and anonymity of all participants involved, adhering strictly to ethical guidelines for educational research. Data received from Zearn was aggregated at the classroom-level, with no identifying information about teachers or students.

# References

::: {#refs}
:::


# Supplementary Information {.appendix}

## Supplementary Methods

### Independent Component Analysis (ICA)

We implement the FastICA algorithm [@hyvarinen2000] to estimate independent components from our dataset. In this model, matrix $X = \{x_{ij}\}_{I \times J}$ , consisting of $I$ samples across $J$ random variables, is expressed as a linear mixture of independent components $C$, represented by:

$$
X = C' M + E.
$$

Here, $C$ holds the independent components, $M$ is a mixing matrix, and $E$ denotes the noise. The aim is to minimize mutual information between components in $C$, which is achieved by maximizing their marginal negentropy, thereby rendering the columns of $C$ statistically independent.

The FastICA process begins by transforming $X$ into a whitened matrix $Y$, ensuring uncorrelated variables with unit variance. This transformation is achieved through eigenvalue decomposition:

$$
Y = X \times \begin{bmatrix} \frac{\mathbf{v_1}}{\sqrt{\lambda_1}} & \frac{\mathbf{v_2}}{\sqrt{\lambda_2}} & \frac{\mathbf{v_3}}{\sqrt{\lambda_3}} \end{bmatrix},
$$

where ($\lambda_1, \lambda_2, \lambda_3$) and ($\mathbf{v_1}, \mathbf{v_2}, \mathbf{v_3}$) are, respectively, the eigenvalues and eigenvectors of $\frac{X'X}{I}$.

Afterward, the algorithm approximates the negentropy with

$$
\hat{\theta}(c_n) = (\mathbb{E}[\ln(\cosh(c_n))] â \mathbb{E}[\ln(\cosh(z))])^2
$$

where $c_n, n \in {1, 2, 3}$, is one of the components, and $z$ is a Gaussian variable with zero mean and unit variance. FastICA iteratively maximizes this value across all components, producing an orthogonal rotation matrix $R_{3 \times 3}$ such that $C = YR$.

For more details on ICA and the FastICA algorithm, see @hyvarinen2000 and @helwig2013.

### Censored Panel Regression

We estimated a censored panel regression model using a first difference approach, with the `pldv` function from the `plm` package [@croissant2008] in R and a lower bound of 0 for the dependent variable:

$$
\Delta \ln (\text{Badges}_{itc} + 1) = \beta_0 + \beta_1 \Delta \text{(IC1)}_{it} + \beta_2 \Delta \text{(IC2)}_{it} + \beta_3 \Delta \text{(IC3)}_{it} + \Delta \epsilon_{itc}
$$

where $i, c, t$ index the teacher, class, and week, respectively. Standard errors were clustered at the teacher level with the `vcovHC` function [@millo2017] in R.

We cannot use fixed effects because there does not exist a sufficient statistic allowing the fixed effects to be conditioned out of the likelihood [@honore1992].

### Fixed-Effects Regression

Hausman Test

## Supplementary Tables

```{r}
#| label: tbl-ica
#| tbl-cap: "Independent Component Analysis (ICA) Results. This table displays the weights of each teacher activity in the three independent components (ICs). Notably, these metrics pertain to teacher activity on the platform, not student actions."

rownames(imod.fast$M) <- names(ICA_cols)
imod.fast$M
```

```{r}
#| label: tbl-marginal-effects
#| tbl-cap: "Marginal Effects of ICA Components on Badges. Marginal effects are calculated by multiplying the ICA coefficients by the mean of each component and dividing by the standard deviation of the corresponding variable."


t(
  100 * rbind(
    tcrossprod(summary(ica_fe_model)$coefficients[-1,"Estimate"],
               imod.fast$M[,1:3]),
    tcrossprod(summary(ica_fe_model)$coefficients[-1,"Estimate"],
               imod.fast$M[,1:3]) /
      apply(teacher_student_usage_subset %>%
              select(RD.elementary_schedule:RD.foundational_lesson_guidance) %>%
              select(where(~ sd(.x, na.rm = TRUE) != 0)),
            FUN = sd, MARGIN = 2))
  ) %>%
  as.data.frame() %>%
  rename(
    `Marginal Effect of 1 SD` = V1,
    `Marginal Effect of 1 Unit` = V2
  ) %>% 
  arrange(-`Marginal Effect of 1 SD`)

```

Marginal Effects

## Supplementary Discussion

### Robustnes Checks

```{r PCA}
#| eval: false

pca_result <- prcomp(PCA_cols, scale. = TRUE)
prop_variance <- summary(pca_result)$importance[2, 1:10]

# Add to dataset
teacher_student_usage_subset <- teacher_student_usage_subset %>%
  mutate(
    pc1 = pca_result$x[, 1],
    pc2 = pca_result$x[, 2],
    pc3 = pca_result$x[, 3]
  )


## Robustness
# Exclude User.Session
ICA_cols <- teacher_student_usage_subset %>%
  select(RD.elementary_schedule:RD.foundational_lesson_guidance) %>%
  select(where(~ sd(.x, na.rm = TRUE) != 0)) %>%
  mutate_all(scale)

imod.fast2 <- ica(ICA_cols, nc = 3)

# Variance explained
imod.fast2

# Creating a data frame for the ICA loadings
ica_loadings_df <- as.data.frame(t(ica_result$A))
colnames(ica_loadings_df) <- paste("Component", 1:ncol(ica_loadings_df))
ica_loadings_df$Feature <- colnames(teacher_student_usage_subset[13:39])[ICA_cols]

# Creating the table with gt
ica_loadings_table <- ica_loadings_df %>%
  relocate(Feature) %>%
  gt() %>%
  tab_header(
    title = "ICA Loadings",
    subtitle = "Loadings of each feature on the independent components"
  ) %>%
  cols_label(
    Feature = "Variable",
    `Component 1` = "Component 1",
    `Component 2` = "Component 2",
    `Component 3` = "Component 3"
  )

# Display the table
ica_loadings_table

```

```{r}

## SEAN: please compare:
## 1) RE vs FD: "There is no command for a fixed-effects model, because there does not exist a sufficient statistic allowing the fixed effects to be conditioned out of the likelihood.
## 2) with objfun = "lad" (maybe more robust to outliers?)
## Marcos: Move to robustness checks.
# ica_fd_model <- pldv(fmla, data = pUsage, model = "fd", objfun = "lsq",
#                          lower = 0, sample = "cens")

```

<!-- SEAN: Let's try a Panel model for Study 2 as well. We might need to use LASSO because we have so many variables (we also want to include interactions of weekdays with streak_dow). Panel is what we did for the previous analysis, but it was a bit more complicated with the habitization part. We can just do one mixed-effects LASSO and see if the results coincide with the analysis below and with the previous LASSO analysis. -->

-   Additional Analyses: Detailed list of supplementary analyses to be conducted.

## Supplementary Equations

## Supplementary Notes

<!-- (including notes clarifying statistical analyses, acknowledgements, grant or other numbers) -->
